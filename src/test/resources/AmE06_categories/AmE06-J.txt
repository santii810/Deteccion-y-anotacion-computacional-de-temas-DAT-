<file= AmE06_J01>

INTRODUCTION
This paper focuses on several recent examples in which: (i) laboratory studies provide much new data  -  i.e. rest frequencies typically better than 1 km s-1 in equivalent radial velocity  -  to guide future astronomical searches for rare isotopic species or additional transitions of known astronomical molecules, (ii) the close coordination of laboratory astrophysics and radio astronomy has led to the discovery of a new molecule in space, or (iii) laboratory studies provide unambiguous assignment for a previously unidentified series of astronomical lines. The molecules
highlighted here are: (1) silicon sulfide SiS, (2) the isomeric pair SiCN/SiNC, (3) the free radical 12CH, and its carbon-13 isotopic species 13CH, and (4) C6H in a low-lying vibrationally excited state, which we conclude on the basis of laboratory measurements is the carrier for a series of unidentified series of lines in the rich astronomical source IRC+10216.
The spectrometer that has been responsible for the detection of these and many other molecules in our laboratory is a Fourier transform microwave (FTM) device used in combination with a supersonic molecular beam, in which an electrical discharge source through suitable precursors gases is used to produce the desired species in high abundance. A detailed description of the spectrometer can be found elsewhere. It is equipped with a sensitive microwave receiver and is fully computer controlled so that nearly round-the-clock operation is routinely achieved. The spectrometer possesses both high sensitivity  -  about a million molecules per quantum state per gas pulse  - 
and high spectral resolution  -  approaching 1 part in 107. FTM spectroscopy has been used with good success in the past nine years by our group to detect a large number
of molecules of astronomical interest: more than 130 by late 2005. Nine of these species have already been detected in rich astronomical sources on the basis of precise laboratory rest frequencies, and many of the others are good candidates for detection because they are closely related in structure and composition to known astronomical molecules; many are also calculated to be quite polar which should assist radioastronomical searches.
A large subset of our new discoveries are carbon chains terminated at one or both ends with a heteroatom (e.g.Si, S, O, etc.) or functional group (e.g. -CH3, CN, etc), but stable and reactive rings, and molecular ions have been detected as well. Small atomic clusters which are weakly polar can also be detected by this technique.
The key point here is that many polar molecules, even light hydrides, often possess at least one transition  -  a pure rotational transition or a lambda-type doubling transition in the ground or low-lying vibrational state  -  in the centimeter-wave band. Because the production efficiency and detection sensitivity of our present spectrometer are usually both high, extensive isotopic spectroscopy is often undertaken, as have studies of vibrationally excited states.
Silicon Sulfide
Silicon sulfide SiS plays a very important role in the rich astronomical source IRC+10216 because chemical models predict it is the progenitor for most Si-bearing molecules in the outer molecular envelope. Although SiS is composed of two second row elements, a total of seven isotopic have now been detected in this source, the faintest of which is Si36S. On the basis of observations using the IRAM 30 m telescope of the 34S and 34S isotopic species of CS and SiS, Mauersberger et al.  conclude that the 36S/34S abundance in this source (107±15) is more
than two times smaller than the solar value (288), but close to the interstellar value (115), implying that IRC+10216 is fairly well processed. The astronomical detection of Si36S which is secure, however, was based on a predicted rather than a measured rotational constant, because this rare isotopic species and several others had not yet been detected in the laboratory.
One goal of recent laboratory studies  -  motivated in large part by this astronomical work  -  was to significantly extend the isotopic spectroscopy of this important diatomic molecule. Towards this end, rotational lines from all twelve isotopic species of SiS have now been detected using a dilute mixture of silane (SiH4) in combination with a sulfur-containing gas (e.g., H2S, OCS, or CS2, which all appear to work comparably well). As the spectrum in Fig. 1 shows, the least abundant species 30Si36S, which has a fractional abundance relative to 28Si32S of 6x10-6, can now be detected with good signal to noise in a modest integration time. Owing to the high spectral resolution, for species such as 29Si33S, it is possible to resolve widely-spaced hyperfine structure which arises from the I=3/2 spin of the 33S nucleus, and much smaller nuclear spin-rotation splittings that arise from the interaction of the I=1/2 nuclear spin of 29Si with the small magnetic field induced by rotation [see Fig. 1(b)]. The ground state rotational constants for all eleven isotopic species of SiS are summarized in Table 1. We estimate by comparison of line intensities in the beam with those of the stable molecule carbonyl sulfide (OCS), at a known fractional abundance (i.e. 1% OCS in Ne), that a remarkably high percentage of the precursor  - 
nearly 15%  -  is converted to SiS. By comparison, the mole fraction of most reactive molecules produced in our discharge source is typically in the range of 10-4 -10-6. The high abundance of SiS may not be so surprising, however, if one considers that this molecule is the second row analog to CO, and therefore may be unusually stable. For the more abundant isotopic species of SiS, in addition to rotational lines that originate from the ground vibrational state, strong satellites from high-lying vibrational states have also be observed, up to high as v=51,
or about 60% of the dissociation limit, for the normal species. Although of less astronomical interest, these measurements in combination with tetrahertz and infrared data enable the potential energy surface of SiS to be probed at quite high energy. Such measurements also enable one to study how vibration excitation affects hyperfine interactions in the molecule, e.g. the vibrational dependence of the quadrupole coupling constant of 33S has been measured up to v=17. High vibrational temperatures are possible in our discharge expansion because of collisional
excitation with electrons, which typically have a kinetic temperature of 10,000 K. Because V-V cross-sections for diatomic molecules are normally large compared to those for V-T relaxation, redistribution of the rotational population to high v levels occurs very rapidly. Similar excitation of CO into high v levels has long been observed in discharge sources similar to ours.
The Isomeric Pair: SiCN and SiNC
Because silicon-bearing molecules are so conspicuous in IRC+10216, considerable effort has been devoted to the laboratory detection of new Si species of plausible astronomical interest. Considerable success has now been achieved by using silane gas as a source for free Si, which, owing to its refractory nature, is otherwise difficult to produce in high quantities in the gas-phase. Nearly 30 Si-bearing molecules have now been discovered by this method (see Fig. 2 of Ref. 8), including the isomeric pair SiCN and SiCN, which are of particular astronomical interest because other refractory molecules containing nitrile groups have been detected in IRC+10216. Theoretical calculations conclude that the two isomers are comparable in energy, differing by less than 5 kcal/mol, with SiCN being both slightly lower in energy. Both are predicted to possess linear structures, 2Π electronic ground states, and to be fairly polar, with SiCN possessing a somewhat large dipole moment. 
We have now detected SiCN and then SiNC in the laboratory, first in the centimeter band, and then in the millimeter band with a long path free space absorption spectrometer. Detection in both bands allows the radio spectra of each to be calculated to very high precision (better than 1 km sec-1 in equivalent radial velocity) over the entire range of interest to radio astronomers. The millimeter-wave measurements provide information on the centrifugal distortion of the molecules at high rotational excitation, which is crucial in the 2-3 mm band where these radicals are expected to possess their most intense rotational transitions in IRC+10216, a source characterized by an
effective rotational temperature of roughly 20 K. On the basis of the laboratory measurements, radio emission lines of SiCN were detected soon afterwards in IRC+10216, and more recently so too were those of the slightly less stable and less polar isomer SiNC.
The observed lines are faint for both radicals, but the astronomical identifications are secure because multiple lines have been detected at precisely the expected frequencies with similar intensity. All of the observed lines appear to display a characteristic double cusped line shape indicating that they are concentrated in the thin outer shell where
molecules like SiC2 have also been found. Derived column densities for the isomeric pair are quite similar, in contrast to the cyanopolyyne/isocyanopolyynes where the decrement is nearly 100. Both SiCN and SiNC are present at a fractional abundance of order 10-9 relative to H2.
The Free Radicals 12CH and 13CH
Another prominent astronomical molecule which has been the subject of recent studies in our laboratory is the CH radical. CH was one of the first molecules detected in space by its optical transitions in the blue; it has a modest dipole moment of 1.46 D, and is comprised of two cosmically abundant elements. Astronomical detection at radio frequencies has proven far more challenging, but after considerable effort, lambda-doublet transitions in the lowest X2Π1/2 fine structure ladder were detected by Rydbeck et al. in 1973, and then those in the X2Π3/2 ladder were subsequently found by Ziurys and Turner in 1985. Astronomical data on the rare isotopic
species 13CH is scarce, but some of its lines have now been identified in the solar spectrum.
Although much laboratory spectroscopy has already been done on CH and 13CH, including detection at terahertz frequencies of pure rotational transitions, the low-J lambda-doublet transitions have never been observed directly, and as a consequence, rest frequencies are still not known to high precision. This result is somewhat surprisingly because many of the same transitions of astronomically important OH were reported many years ago in response to astronomical observations. At present, the most accurate measurements of the CH lambda-doubling frequencies are those derived from the astronomical data. For 13CH, owing to the lack of such data, these transitions are only known to a few MHz.
By means of FTM spectroscopy, lambda-doubling transitions of 12CH and 13CH in the lowest rotational levels of the X2Π1/2 ground state have been directly detected for the first time. Frequencies of the hyperfine-split lines have been measured to better than 1 ppm, an improvement over previous laboratory data of order 100 for CH and 1000 for 13CH. When these measurements were included in a least-squares fit of all the available spectroscopic data, a set of spectroscopic constants is obtained which yields significantly better predictions of the CH and 13CH lambdadoubling transitions.
As the rotational energy level diagram in Fig. 2 illustrates, a total of nine hyperfine lines in the X2Π1/2 state of CH, three from J=1/2, four from J=3/2, and two from J=5/2, have now been measured. Most are observed with high signal-to-noise as the spectrum in Fig. 2 (insert) shows. Sixteen lines of 13CH, including 14 from J=3/2, have also been detected, many again with good signal-to-noise. A refined set of spectroscopic constants has been derived for CH and 13CH by a least-squares fit of the new measurements with all of the other available spectroscopic data, using
an effective Hamiltonian for a diatomic molecule in an isolated X2Π1/2 electronic state. Because both data sets now contain more accurate measurements of the lambda-doubling intervals, the hyperfine constants are significantly better determined, c and d for CH and c for 13CH by as much as a factor of 17. In addition, for 13CH, we are now able to determine a(H), d(H), and the dD(H) for the first time; all are now determined with comparable precision to those for CH. A complete account of the laboratory observations will be published elsewhere.
B1395: An unidentified series in IRC+10216
The last example highlighted here is the identification of the carrier of a series of unidentified lines in IRC+10216.

</file>

<file= AmE06_J02>

Overview. 
Plate tectonics on Earth, at present, consists of about a dozen large semi-coherent entities -- called plates -- of irregular shape and size that move over the surface, separated by boundaries that meet at triple junctions. There are also many broad zones of deformation. Plate tectonics is often regarded as simply the surface, or the most important, manifestation of thermal convection in the mantle [this phrase, and phrases in the same typeface, is a Googlet; see Preface or type it into a search engine]. In this view the plates are driven by thermal and density variations in the mantle. Cooling plates and sinking slabs can also be regarded as driving themselves, and driving convection in the underlying mantle; they create chemical, thermal and density anomalies in the mantle. Plate tectonics qualifies as a branch of complexity theory. Plate tectonics may be a far-from-equilibrium self-organized system powered by heat and gravity from the mantle and organized by dissipation in and between the plates. Mantle convection, below the plates, may not drive or organize the plates; it may be the other way around. Plate buoyancy and dissipation control plate motions, stresses, and locations of plate boundaries, intraplate extensional zones and volcanic chains. The cold stiff outer shell of Earth is the active element and the template; the underlying convective mantle is passive. The outer shell of the Earth is not just a thermal boundary layer or a cold strong layer. It is, in part, the accumulated buoyant residue of mantle differentiation including the on-going process of seafloor spreading and building of island arcs. It is composed of fertile melts, dikes, sills and cumulates, and infertile refractory residues. It is, in part, isolated from the low-viscosity fertile interior. Earthquakes and volcanoes not only mark plate boundaries but they antici- pate new plate boundaries, changes in boundary conditions and dying plate boundaries. If the shallow mantle is close to the melting point or partially molten, volcanoes have a simple cause, stress. The mode of convection in the Earth depends on the distribution of radioactive elements and physical properties and how these properties depend on temperature and pressure and melt- ing point. An Earth with most of the radioactive elements in the crust and upper mantle, and with strongly pressure-dependent thermal properties will not behave as a uniform fluid being heated on a stove. These effects, plus continents and sphericity, break the symmetry between the top and bottom thermal boundary layers. The main plate tectonic cycle is the ridge- trench-slab system, primarily playing out in the ocean basins. There is a secondary cycle involving underplating, freezing at depth, delamination and asthenospheric upwelling. A mafic lower crust, if it thickens and cools sufficiently, will convert to a high density mineral assemblage, leading to a gravitationally unstable configuration in which the lower crust can sink into the underlying lower-density mantle, cooling it and fertilizing it. The solid Earth can rotate rapidly underneath its spin axis through a process known as true polar wander (TPW). The spinning Earth continuously aligns its maximum moment of inertia with the spin axis. Melting ice caps, plate motions, continental uplift and drift and ridge-trench annihilations can all cause TPW. The magnetic and rotational poles are good terrestrial reference systems; the hotspot frame is not. But fertile patches in the asthenosphere move more slowly than the plates and plate boundaries and therefore melting anomalies appear to be relatively fixed.
The outer shells of Earth.
Plate tectonics involves the concepts of plates, lithospheres, cratonic keels and thermal bound- ary layers; these are not equivalent concepts. Lithosphere means rocky shell or strong layer. It can support significant geologic loads, such as mountains, and can bend, as at trenches, without significant time-dependent deformation; it behaves elastically. Plates are not necessarily strong or elastic. Thermal boundary layers conduct heat out of the underlying regions; they are defined by a particular thermal gradient. 
Plates. 
A plate is a region of the Earth's surface that translates coherently. The word plate as in plate tectonics implies strength, brittleness and permanence and often the adjectives rigid and elastic are appended to it. But plates are collages, held together by stress and adjacent portions rather than by intrinsic strength. Plates break at suture zones (former plate boundaries), fracture zones and subplate boundaries, often generating volcanic chains in the process. A plate can be rigid in the sense that relative plate motions can be described by rotations about Euler poles on a sphere but can still have meter-wide cracks, which is all that is needed to create volcanic chains from the hot underlying mantle and its low-viscosity magmas. What is meant by rigidity is relative coherence in motions, not absolute strength. Because rocks are weak under tension, the conditions for the existence of a plate probably involve the existence of lateral compressive forces. Plates have been described as rigid but this implies long-term and long-range strength. They are better described as coherent entities organized by stress fields and rheology. The corollary is that volcanic chains and plate boundaries are regions of extension. Plates possibly organize themselves so as to minimize dissipation. The term plate itself has no agreed-upon formal definition. If plate is defined operationally as that part of the outer shell that moves coherently then several interpretations are possible. 
(1) Plates are strong and rigid (the conventional interpretation). 
(2) Plates are those regions defined by lateral compression since plate boundaries are formed by lateral extension. 
(3) Plates move coherently because the parts experience similar forces or constraints. 
With the first definition, if there is to be spread- ing and volcanism, local heating or stretching
must overcome the local strength; this reason- ing spawned the plume hypothesis . With the second definition the global stress field, dictated by plate boundary and subplate conditions, and cooling plates, controls the locations of stress conditions appropriate for the formation of dikes and vol- canic chains, and incipient plate boundaries; the underlying mantle is already at or near the melting point. This is the plate hypothesis. Plates are not permanent; they are temporary alliances of subplates. Global plate reorganization processes episodically change the orientations of spreading centers, the directions and speeds of plates, and redefine the plates. Plates annex and lose territory to adjacent plates and they break up or coalesce. New plate boundaries do not form all at once but evolve as age-progressive chains of volcanoes. Volcanic chains can also be extinguished if lateral compression takes over from local extension. Volcanism can be turned on and off by changing stress but it is not so easy to turn off plume volcanism, or to suddenly reduce the temperature of the mantle. Important aspects of plate tectonics are the necessity for ridges and trenches to migrate, for triple junctions and boundary conditions to evolve, and for plates to interact and to reconfigure when boundary conditions change. Second order features of plates and plate boundaries (e.g. fracture zones, accreted terranes , transform faults, broad diffuse zones, swells, sutures, lithospheric architecture and microplates) and boundary reorganizations are actually intrinsic and provide the key for a more general view of plate tectonics than contained in the rigid plate-fixed hotspot scheme . 
The many lithospheres.
The lithosphere is that part of the cold outer shell of the Earth that can support stresses elas- tically. The lithosphere is defined by its rheolog- ical behavior. There are other elements of the outer shell that involve lateral motions, buoy- ancy, chemistry, mineralogy or conductivity and these may or may not be part of the lithosphere. Lithosphere is not the same as thermal boundary layer or plate . Since mantle silicates flow readily at high temperatures and flow more rapidly at high stress, the lithosphere appears to be thicker at low stress levels and short times than it does for high stress levels and long times. Thus, the elastic lithosphere is thick when measured by seismic or postglacial-rebound techniques. At longer times the lower part of the instantaneous elastic litho- sphere relaxes and the effective elastic thickness decreases. Thus, the elastic litho- sphere is relatively thin for long-lived loads such as seamounts and topography. Estimates of the flexural thickness of the lithosphere range from 10-35 km for loads having durations of millions of years. A more complete definition of the lithosphere is that part of the crust and upper mantle that deforms elastically for the load and time scale in question. The viscosity and strength of the mantle depend on composition -- including water content -- mineralogy and crystal orientation as well as on temperature and stress. If the upper mantle is compositionally layered, then the lithosphere--asthenosphere boundary may be controlled by factors other than tempera- ture. For example, if the subcrustal layer is dry olivine-rich harzburgite, it may be stronger at a given temperature than a damp peridotite, or a clinopyroxene--garnet-rich layer. If the latter is weak enough, the lithosphere--asthenosphere boundary may represent a chemical boundary rather than an isotherm. Likewise, a change in the preferred orientation of the dominant crystalline species may also markedly affect the creep resistance. The boundary may represent a dehydration boundary -- wet minerals are weak. The effective elastic thickness of the lithosphere depends on many parameters but these do not necessarily include the parameters that define plates, thermal boundary layers and cratonic keels. The layer that translates coherently, the plate of plate tectonics , is often taken to be identical with the elastic lithosphere . This is probably a valid approximation if the stresses and time scales of the experiment that is used to define the 
flexural thickness are similar to the stresses and time scales of plate tectonics. It must be kept in mind, however, that mantle silicates are anisotropic in their flow and thermal characteristics, and that the stresses involved in plate tectonics may have different orientations and magnitudes than the stresses involved in surface loading experiments. We do not know the thickness of the plate or how well it is coupled to the underlying mantle. We do not even know the sign of the basal drag force . In a convecting or cooling mantle there is a surface thermal boundary layer (TBL) through which heat must pass by conduction. The thickness of the thermal boundary layer is controlled by such parameters as conductivity and heat flow and is not related in a simple way to the thickness of the elastic layer or the plate. Since temperature increases rapidly with depth in the conduction layer, and viscosity decreases rapidly with temperature, the lower part of the boundary layer probably lies below the elastic lithosphere; that is, only the upper part of the thermal boundary layer can support large and long-lived elastic stresses. Unfortunately, the con- duction layer too is often referred to as the lithosphere. In a chemically layered Earth there can be TBLs between internal layers. These TBLs act as thermal bottle-necks and slow down the cooling of an otherwise convective mantle. Continental cratons have high seismic velocity roots, or keels, extending to 200--300 km depth. These are referred to as archons . They persist because of low density and high viscosity, and because they are protected from high stress. They are more than just part of the strong outer shell. These keels can last for billions of years. Most models of the Earth&#8217;s mantle have an upper-mantle low-velocity zone , LVZ, overlain by a layer of higher velocities, referred to as the LID. The LID is also often referred to as the litho- sphere. Seismic stresses and periods are much smaller than stresses and periods of geological interest. If seismic waves measure the relaxed modulus in the LVZ and the high-frequency or unrelaxed modulus in the LID, then, in a chemically homogenous mantle, the LID should be much thicker than the elastic lithosphere. If the LID is chemically distinct from the LVZ, then one might also expect a change in the long-term rheological behavior at the interface. If the LID and the elastic lithosphere turn out to have the same thickness, then this would be an argument for chemical, water or crystallographic control, rather than thermal control, of the mechanical properties of the upper mantle.

</file>

<file= AmE06_J03>

INTRODUCTION
Studies on suspensions of noncolloidal particles have primarily concentrated on the
response when exposed to steady shear flow [Gadala-Maria and Acrivos (1980); Chan
and Powell (1984); Tsai et al. (1992), thus the rheology of these suspensions is fairly
well-defined as recently reviewed by Stickell and Powell (2005). In steady flow, noncolloidal
suspensions at vanishing Reynolds numbers undergo a diffusive-like motion which
is due solely to hydrodynamic interactions. Whereas the trajectories of particles undergoing
simple pairwise interactions are symmetric and reversible [Batchelor and Green
(1972)], inhomogeneities in the particle surface can cause permanent displacements of
particles from their original streamlines, resulting in irreversible interactions [Davis
(1992); Rampall et al. (1997)].
Shear-induced diffusion of particles in steady shear flow has been observed in both
experiments [Eckstein et al. (1977); Leighton and Acrivos (1987a); Breedveld et al.
(2001) and simulations [Dratler and Schowalter (1996); Drazer et al. (2002)]. The shear-induced diffusion explains the apparently random trajectories that particles undergo in
concentrated suspensions exposed to steady shear and has been used to explain such
phenomena as shear-induced particle migration [Leighton and Acrivos (1987b); Phillips
et al. (1992)]. This chaotic motion is intimately related to the time and spatial configuration
of the particles, closely linking the rheology to the microstructure.
Whereas the rheology of suspensions in steady shear is well-understood, the behavior
of noncolloidal suspensions in oscillatory flow is less understood. This is due in part to
few experiments investigating unsteady flows. Gadala-Maria (1979) studied suspensions
exposed to oscillatory shear, finding that above a certain strain amplitude, the stress
response became nonlinear. The rheology was found to be independent of the frequency
and the amplitude of the stress response drifted lower in time until a steady state was
achieved after approximately 100 cycles [Gadala-Maria and Acrivos (1980)]. Gondret
and Petit (1996) found a similar drift for a strain amplitude of 0.1, however, a steady state
was achieved after a much larger number of cycles. Gondret and Petit (1996) explained
the observed decrease in viscosity with time as being caused by microstructural changes
due to a regular ordering of particles into layers parallel to the flow direction, as evidenced
by optical microscopy experiments. Breedveld et al. (2001) performed a comprehensive
set of oscillatory shear experiments on noncolloidal suspensions in a Couette
geometry and found that the rheology was both frequency and amplitude dependent and
that plateaus existed at both small and large amplitudes which were separated by a
transient period in which the response signals were nonlinear. In contrast to Gondret and
Petit (1996), Breedveld et al. (2001) found that at small strain amplitudes, the particle
positions remained unchanged with no evidence of a shear-induced microstructure. Similar
to Breedveld et al. (2001), Pine et al. (2005) reported results on hydrodynamic
diffusivities from simulations and experiments indicating that at small strain amplitudes
the particle diffusivity became increasingly small such that particles returned to their
initial positions after each oscillation. Pine et al. (2005) also found a transition at high
strain amplitudes above which the flow becomes irreversible. Narumi et al. (2005) report
on the rheology of suspensions undergoing high amplitude oscillatory shear and explain
the unusual output wave forms in terms of the transient responses observed in shear
reversal experiments [Kolli et al. (2002); Narumi et al. (2002)].
Significant gaps therefore exist in understanding the characteristic behavior of noncolloidal
suspensions in oscillatory shear flow. In this study, suspensions of noncolloidal
particles are exposed to oscillatory shear to investigate the response as a function of the
applied strain amplitude. The goal is to gain some understanding of the dynamics within
unsteady flows. In the following sections, we present results from experiments of noncolloidal
suspensions exposed to both steady and oscillatory shear flow. The results are
presented for five different systems in both Couette and parallel-plate geometries using
the same instrument. In Sec. II, a detailed description of the experiment is provided along
with details concerning the characterization of each suspension system. Results are presented
in Sec. III for the cases of steady shear flow and oscillatory flow. The results are
discussed in Sec. IV and conclusions of the work are presented in the last section.
EXPERIMENT
Three sets of particles were used to determine the effect of size distribution and
particle type on the suspension rheology. Figure 1 shows scanning electron microscopy
(SEM) images of two sets of the particles with size distributions for all three sets. The
distributions were measured using a Coulter counter and were verified using scanning
electron microscopy. The polystyrene and poly (methylmethacrylate) (PMMA) particles 
obtained from Sapidyne Industries were found to be monodisperse with similarly shaped
size distributions and mean diameters of 99±6 m and 100±8 m, respectively. The
PMMA particles obtained from Bangs Laboratories had a much broader size distribution
and a mean diameter of 61±16 m as measured in the laboratory, though the manufacturer
reported a mean diameter of 83 m. The surface roughness was evaluated using
scanning electron microscopy, and for all of the spheres used, the size of the surface
roughness features never exceeded 1 m. The sphericity was also evaluated using scanning
electron microscopy. A form factor was evaluated according to the equation F
=4Ap /P2, where Ap is the particle area and P is the perimeter. For each set of particles,
a similar form factor of F 0.91 was measured, whereas a factor of 1 corresponds to a
perfect sphere.
Three different suspending liquids were investigated and are listed in Table I. For the
polystyrene spheres, a polyalkylene glycol oil (Aldrich) was used and found to be Newtonian
with a viscosity of 1.87 Pa s and a measured density of 1.052 g/cm3. Two different
suspending liquids were prepared for the PMMA spheres. The first consisted of a
mixture of equal amounts by volume water and 75-H-90000 UCON oil (Dow Chemical).
To effectively match the densities of the PMMA and suspending liquid, sodium iodide
(25% by weight) was added to the water prior to mixing. The composition of the suspending
liquid is comparable to previous experiments [Sinton and Chow (1991); Butler
and Bonnecaze (1999); Butler et al. (1999)]. Although the UCON oil/water suspending
liquid was found to have a constant short-time viscosity of 0.97 Pa s, a significant drift in
the viscosity with time was observed in the parallel-plate geometry due to a slow evaporation of the suspending liquid. This drift was found to be minimized by using the
Couette geometry since the fluid in a Couette gap is not directly exposed to the environment.
Due to the presence of evaporation in this system, an additional suspending liquid
identical to the one used in the experiments of Han et al. (1999) was used. In this system,
a mixture of approximately equal amounts by mass of ethylene glycol and glycerol were
mixed based on the measured density of the particles. The short-time viscosity of the
suspending liquid was found to be 0.0924 Pa s and although the suspending liquid has
been reported to be Newtonian in previous studies [Han et al. (1999)], a 30% decrease in
the viscosity was observed over 24 h when sheared in the parallel-plate geometry at a
constant shear rate of 24 s−1. A smaller decrease of 5% was observed in the Couette
geometry over the same time period and shear rate. The decrease probably results from
absorption of moisture from the surrounding air, which is minimized in the Couette
geometry since the exposed surface area to volume ratio is smaller.
A summary of the suspension systems studied is provided in Table I. To confirm close
matching of densities between the suspending liquid and particulate phases, suspensions
at a volume fraction of 0.10 were set out for a period of 24 h. In all systems, no
apparent sedimentation or buoyancy was observed during this period. The volume fraction
of particles henceforth was set at 0.40 for all experiments. All suspensions were
prepared by gently hand mixing the particles in small increments until a homogeneous
state was reached. The suspensions were placed under vacuum prior to testing to eliminate
any air bubbles entrained within the suspension.
The maximum particle-based Reynolds number for the systems used is 10−7, so the
effects of inertia are expected to be minimal. Likewise, thermal diffusion is negligible for
the relatively large particle sizes since the minimum particle-based Peclet number is 107.
Other colloidal interactions, such as electrostatic interactions, should also be of no importance.
The rheometer used in all experiments was an ARES LS-1 strain controlled rheometer
(TA Instruments). Experiments were performed using both parallel plate and Couette
geometries. The Couette geometry consisted of a rotating 34 mm diameter cup and a
stationary 32 mm diameter bob, resulting in a gap of 1 mm. The bob height was 33 mm
and to ensure consistent measurement, the reservoir gap was set to 10 mm in all experiments.
A 50 mm diameter parallel plate was also used and the plate separation was set at
1.0 mm in all cases reported in Sec. III. Note, however, some experiments were performed
for different plate separations of 0.5, 1.5, and 2.0 mm. Results for plate separations of 1.0 mm and larger matched within experimental error, indicating that boundary
layer effects near the walls negligibly impact the reported results despite the relatively
small ratio of gap to particle diameter.
Since the rheology of noncolloidal suspensions is sensitive to the microstructural
arrangement of particles and thus to the initial configuration, the suspensions were presheared
in each geometry before starting the oscillatory experiments. A steady preshear of
24 s−1 was employed for a period of 120 s to reduce any effects caused by loading the
sample and to ensure that the oscillatory experiments began from a fairly consistent
configuration. This total strain of 2880 was sufficient to reach a short-time steady state in
the Couette geometry (Fig. 2 inset), but not long enough to result in any significant
shear-induced migration in the vertical direction [Leighton and Acrivos (1987b)]. In the
case of the Couette geometry, a nonuniform concentration profile develops within the gap
during sample loading. To achieve a homogeneous state, a preshear is required to induce
a redistribution of particles in the radial direction [Leighton and Acrivos (1987b)]. Following
the preshear, the oscillatory shear experiments were performed. In each oscillatory
shear experiment, the shear rate was set by specifying the strain amplitude A and
frequency f. Based on the applied strain amplitude, a total strain can be calculated by
4An, where n is the number of cycles. Unless noted otherwise, all experiments were
performed at the same frequency of f =1.59 cycles/s. The temperature was maintained at
25°C using an external temperature bath. During a typical experiment the temperature
fluctuated less than 0.05°C.
RESULTS
In this section, steady shear results are first given as a reference to the oscillatory shear
results. The oscillatory shear results are then provided in detail for a monodisperse suspension
system. Comparison of these results are made with other suspension systems to
evaluate the effects of particle size distribution and choice of particle type and suspending
fluid.
Steady shear
The time evolution of the relative viscosity r (defined in this work as the suspension
viscosity normalized by the short-time viscosity of the suspending liquid) of both monodisperse
and polydisperse suspension systems sheared in the Couette geometry at a steady
rate of 24 s−1 is shown in Fig. 2. At short times the systems show a small decrease in
the viscosity followed by a short-time steady state (shown as an inset in Fig. 2). This
short-time steady state corresponds to a microstructural rearrangement of particles across
the Couette gap [Leighton and Acrivos (1987b)]. From this short-time steady state, the
viscosity of the suspensions decreases continuously in time. For the suspension of monodisperse
polystyrene spheres in polyalkylene glycol, a long-time steady state occurs after
15 h and was found to be sustainable for at least 10 h more. The transient long-time
decrease in the viscosity of suspensions of spheres in the Couette geometry has been
previously observed in experiments [Gadala-Maria and Acrivos (1980); Phillips et al.
(1992); Chow et al. (1994)] and is the result of shear-induced migration of particles out
of the Couette gap into the stagnant reservoir [Leighton and Acrivos (1987b)].

</file>

<file= AmE06_J04>

INTRODUCTION
A few inorganic glasses exhibit extraordinary properties:
remarkable mechanical stability, densities of order 90% of
their crystalline counterparts, nearly reversible glass transitions,
and almost no bonding defects that would create electronic
traps. These glasses are composed of stress-free (ideal)networks, and these networks can persist metastably at temperatures below their glass transitions for very long times. In
the last 25 years our knowledge of these networks has grown
rapidly, both experimentally and theoretically, especially for
inorganic chalocogenide and oxide glasses (such as window
glass). At the same time, the idea that H-bonded protein
networks exhibit many (almost equilibrated, nearly reversible) properties similar to those of inorganic network glasses has become popular.
Deductive multiparameter approaches to such ideal
glasses, employing standard polynomial Newtonian methods (such as molecular dynamics), encounter severe difficulties even in supercooled liquids (where the viscosity grows exponentially
and diverges as T→Tg), and these difficulties
increase in the glass, where relaxation follows the even
slower path described by stretched exponentials. In practice
these difficulties often limit simulation times to ns or even
10 ps. Some simulations of inorganic glasses have circumvented
these difficulties by guessing that a good approximation
to the optimally dense glassy network can be obtained
by ring disordering an amorphous solid; this guess can be
tested semiquantitatively (but with only polynomial, not exponential,
accuracy) by comparison with radial distributions
observed by diffraction, but this method, like molecular dynamics,
does not predict phase diagrams or predict chemical
trends in general.
An abstract, parameter-free axiomatic method, based on
the variational concepts of Lagrangian mechanics, has provided
an excellent guide for experiments on inorganic network
glasses, and it was indeed helpful in identifying the
intermediate compositional window where nonreversing and
aging effects are small, as well as the internal network
stress. This method is hierarchical in nature, and its application
involves the general principles of chemical bonding as
utilized by Huckel and Pauling (resonating valence bonds in mixed ionic-covalent molecules). Interatomic forces are supposed to decrease from
bond stretching to bond bending to dihedral interactions by
roughly a factor of 3 - 4 between stages. (There are always
uncertainties in classical interatomic force fields weaker
forces tend to be overestimated, but in good glass formers
these hierarchies are unambiguous). The accuracy of the
method in predicting optimized compositions (without using
any adjustable parameters) can be as good as 1%. It is summarized
in Sec. II, where examples of Lagrangian constraints
show how the method works in the simplest cases of
covalent-ionic chalcogenide and oxide network glasses.
The success of the abstract axiomatic method can be
tested best by showing that it can be used to identify ideal
glasses. For the inorganic network glasses the success has
been confirmed in many ways, but most of these have not
yet been tested for organic glasses. Some organic glasses,
such as polymers, are not well described by constraint
theory, and to broaden constraint theory to include some
cases of hydrogen bonding (Sec. III) requires careful choices
of test cases. It turns out that several simple carbohydrates
(polyalcohols and saccharides) are good candidates for ideal
glasses; the reasoning that led us to those choices, as opposed,
for example, to polymers, is discussed in Sec. III,
where the hierarchy of H-bonding interactions is inserted
into the covalent hierarchical framework. Some macroscopic
properties of polyalcohols and saccharides are collected in
Sec. IV. It is then straightforward to enumerate (Sec. V)
these interactions for polyalcohols and monosaccharides, but
when the procedure is extended to sucrose and trehalose, the
results are surprising. Molecular dynamics models in the
time range from 10 ps to 1 ns partially explain the remarkable
biopreservative properties of trehalose, but other factors
could be important on a time scale of years. The hierarchical
results for trehalose lead to the tandem array network model
described in Sec. VI; this model represents a substantial refinement
of the two-dimensional percolative model of trehalose
films proposed in recent molecular dynamics models of trehalose performed in the time range from 10 ps to 1 ns.
This model is used to contrast the structure and properties of
trehalose with a polyester biopreservative, cutin, in Sec. VI.
There we note analogies with extensible tandem arrays of the
elastic protein titin and other extracellular matrix and cell
adhesion molecules, such as fibronectin, which also contain
tandem arrays. It appears that the tandem nature of trehalose
networks corresponds well to novel tandem repeats in the
cell surface proteins of archaeal and bacterial genomes (Sec.
VII). In the Appendix there are some general remarks on the
H-bonding network of water, especially at interfaces with
proteins, polyalcohols, and saccharides.
COVALENT BOND HIERARCHIES.
The simplest case of a covalent glass is a binary one
A1−xBx in which the covalent radii are nearly equal because A
and B belong to the same row of the periodic table - for
example, A=Ge and B=As or Se. Then the B-A-B and A-B
-A bond angles are nearly equal and the bond-bending constraints
at the ideal composition are all intact, while there are
no constraints on dihedral conformations. This situation is
illustrated in Fig. 1(a). The ideal glass-forming condition is
(Nc=number of constraints) =Nd=number of degrees of
freedom=3 (number of atoms Na), all per formula unit according
to axiomatic mean-field theory. Simulations with
space-filling models, based on bond-deleted, ring-disordered
space-filling networks, confirmed this condition and showed
a crossover from floppy to stiff networks at the ideal composition,
with a density of soft modes nearly linearly decreasing
with increasing connectivity.
What happens when A and B radii are quite different?
This is the case in g-SiO2, where diffraction data show that
the width of the O-centered bond angle distribution is much
greater than that of the Si-centered distribution. Then the
former bond-bending constraints are broken, while the latter
bond-bending constraints are intact, as shown in Fig. 1(b). In (Na2O)x(SiO2)1−x alloys with increasing x the Na atoms cut
(or form "nonbridging") O - Si bonds and there is a crossover
from broken oxygen bending constraints to intact ones; this
crossover actually shows up in the phase diagram as a narrow
low-temperature immiscibility gap (Tc 1000 K), a
transition that was previously unexplained and which has so
far been inaccessible to molecular dynamics simulations
(MDS) confined to T3000 K. More generally, the effects
of space filling on constraints is subtle and is best determined
empirically from traditional structural data (diffraction, Raman,
infrared) or even from the phase diagram and the location
of the stiffness transition. An important point is that the
axiomatic rules are discrete (constraints are nearly always
broken or intact in glasses and are seldom in an intermediate
case between the two limits) and have been refined systematically
in ways that are transferable between situations that
are apparently very different and are conventionally described
by completely different (and often quite large) sets of
adjustable parameters.
The ways in which phase diagrams and structural data can
be interpreted in terms of intact and broken constraints are
illustrated in Fig. 2. In underconstrained glasses, Fig. 2(a),
there are not enough bond-stretching and bond-bending constraints
to exhaust the 3Na degrees of freedom. Thus some of
the dihedral angles can be constrained. In the case of g-Se,
there are one stretching and one bending constraints per
atom, leaving room for one dihedral angular constraint per
atom. This makes it possible for g-Se to form long chains
(300 atoms), yet remain glassy because of entanglement.
As cross-linking Ge or As atoms are added, the number of
allowed dihedral constraints decreases and the chain segments
rapidly shorten.
In overconstrained glasses (Fig. 2b), the number of
bond-stretching and bond-bending constraints is too large and some redundancies will occur. A simple way for this to
happen is for pyramidal or tetrahedral building blocks to
share edges, but there are other possibilities, such as replacement
of single bonds by double bonds.
Nanoscale phase separation is common in binary glasses.
Percolative backbones can consist of molecular units that
locally satisfy the ideal glass condition, also describable by
isostatic, a term borrowed from hydrodynamics to describe
parts of the network that are strain free. The fraction of the
network that is isostatic is variable, and this leads to the
formation of a narrow range of compositions with sharp
edges that have very favorable properties. The density
reaches a plateau, and the glass transition is nearly reversible
and shows little aging. In this reversibility window the network
is unstressed and Raman vibrational frequencies shift
linearly with hydrostatic pressure. Outside the window the
shifts are small until a threshold pressure is reached, which is
interpreted as an internal network pressure. For underconstrained
networks, this pressure stiffens soft matrices before
it affects isostatic backbones, and for overconstrained networks,
it stiffens the isostatic regions before it affects the
stiffer overconstrained regions.
H-BOND HIERARCHIES.
Hydrogen bonding DuHA energies are small
(3 kcal/mol) and problematic, partly due to the relative
weakness of the interaction. Their electronic components
have been studied for a few small molecules, while in
larger molecules these energies are subject to large dynamical
screening corrections, especially those due to the large
OH dipoles responsible for the dielectric constant e80 of
water. Thus recent models of H bond interactions in proteins
are empirical and are based on complex statistical analysis
designed to differentiate H-bonding interactions with both
peptide backbones and amino acid side groups. However,
one may still expect the usual hierarchy of interaction energies,
with E(OuH-A)E(NuH-A)E(CuH-A)and E (bond stretching)E(bond bending). For the polyalcohols
considered below, where the complexities produced
by N lone pairs are avoided, this leads to the simple hierarchy
shown in Fig. 3(b). It is far from obvious that the
CuH-A bending energies are larger than the covalent dihedral
energies; these two could be grouped together without
changing some of our results.
Polymers may avoid crystallization because of entanglement,
and similarly molecular glasses may form because of
steric hindrance. The polar nature of H bonds suggests that
dynamical interactions with local electrical fields to form H
bond networks can compete with steric hindrance in promoting
the glass-forming tendency and be more easily quantified.
Thermal expansion at constant pressure [P=V−1(V/T)P] and at constant dielectric relaxation time
[a=V−1(V/T)T] near the glass transition temperature [T=1s] provides an easy way to gauge quantitatively whether or not H bonding is critically enhancing the glassforming
tendency. (The results are quantitatively similar for
T=1000s or when T is replaced by the viscosity n). The
measured values of the ratio aT/aP are near unity (they range from 0.6 to 2.8) for 15 molecular and polymer glass formers, but are 6 and 17 for the strongly hydrogen-bonded polyalcohols
sorbitol and glycerol, respectively. H bonding is thus
less sensitive to pressure, and more sensitive to temperature,
than covalent bonding and is more dominant, and therefore
probably more easily quantified, in polyalcohols than in polymers.
POLYALCOHOLS AND SACCHARIDES.
Polyalcohols and saccharides have strikingly simple
bonding patterns: all the covalent bonds are single and all the
carbons are tetrahedrally coordinated. Many polyalcohols are
good glass formers, so they are a good starting point for
broadening constraint theory to include hydrogen bonding.
The melting points Tm and glass transition temperatures Tg of
a few polyalcohols and sugars are listed in Table I, together
with the extrapolated slopes of reduced viscosity (fragilities
m on a reduced temperature scale - that is, m=d ln T/d ln(Tg/T)T=Tg. Saccharide data are also included,as these also have strong H-bonding interactions. The
ratios r=Tg/Tm are a simple measure of the extent of clustering
or medium range order in the glasses. This ratio is r=0.77 for sorbitol, 0.74 for orthoterphenyl, and 0.73 for the monosaccharide glucose; these ratios are typical for many
glasses. The ratio behavior of the disaccharides sucrose and
trehalose is unusual: for sucrose r=0.52, while in trehalose
r=0.81. The asymmetry of the rings in sucrose suppresses
clustering, but the bisymmetric structure of trehalose produces
one of the largest values of r known for good glass
formers. However, the anomalously large value of r in trehalsoe
does not tell us much about the nature of the cluster
ordering in the glass.Clustering is a common property of good glass formers;
another quantitative measure of clustering is the ratio of the
minimum in Raman scattering intensity before the boson
peak to the value at the peak, denoted by R1.

</file>

<file= AmE06_J05>

CHAPTER 1.
Perspective, Perspective,Perspective.
Introduction.
Perspective, perspective, perspective - chemical manufacturing industries are often the targets
of misperceptions. In this opening chapter, be prepared to see a more accurate representation
of the U.S. chemical industry, including its value to humanity, its history, and its
high degree of safety. The first section is a brief review of the countless benefits of the chemical industries that surround us, increase our life span, and enhance our enjoyment of life.
The second section is a glimpse of the history of the vital chemical manufacturing industry.
However, the concept of comparative risks is the main emphasis of this chapter. The
perceived risks of the chemical industry and its occupations are often misunderstood.
Working in the chemical industry is safer than most individuals realize. We shall provide
a perspective of the risks of working within this industry by comparing that risk with actual
statistical dangers encountered with other well-understood occupations, commonplace
activities, and life-styles. Later chapters will focus on costly errors in the chemical industry
along with practices and procedures to reduce the occurrence and severity of such incidents.
Viewed in isolation, case histories alone could easily lead to the inaccurate picture that the
chemical industry is dangerous. In fact, the chemical industry has an impressive safety record
that is considerably better than most occupations. The news media does not often speak of
the safety of the chemical plants because these passive truths lack news-selling sizzle.
The Media Rarely Focuses on the Benefits of the Chemical Industry.
Chemical manufacturing and petroleum refining have enriched our lives. Few individuals
in the developed world stop to realize how the chemical industry has improved every
minute of their day. The benefits of the industries are apparent from the time our plastic
alarm clock tells us to wake up from a pleasant sleep on our polyester sheets and our
polyurethane foam mattresses. As our feet touch the nylon carpet, we walk a few steps to
turn on a phenolic light switch that allows electrical current to safely pass through
polyvinyl chloride insulated wires. At the bathroom sink, we wash our face in chemically
sanitized water using a chemically produced soap.
We enter the kitchen and open the plastic-lined refrigerator cooled by fluorochlorohydrocarbon
chemicals and reach for the orange juice, which came from chemically fertilized
orange groves. Many of us bring in the morning newspaper and take a quick look at the
news without thinking that the printing inks and the paper itself are chemical products.
Likewise, other individuals choose to turn on the morning news and do not think twice
that practically every component within the television or radio was made of products produced
by the chemical industry. In short, we just do not think we are surrounded by the
benefits created from chemicals and fail to recognize how the industries have enriched our
lives.
A recent publication distributed by the American Chemical Society states:
The chemical industry is more diverse than virtually any other U.S. industry. Its products
are omnipresent. Chemicals are the building blocks for products that meet our most fundamental
needs for food, shelter, and health, as well as products vital to the high technology
world of computing, telecommunications, and biotechnology. Chemicals are a keystone
of U.S. manufacturing, essential to the entire range of industries, such as pharmaceuticals,
automobiles, textiles, furniture, paint, paper, electronics, agriculture, construction, appliances
and services. It is difficult to fully enumerate the uses of chemical products and
processes. . . . A world without the chemical industry would lack modern medicine, transportation,
communications, and consumer products. 
A Glance at the History of Chemical Manufacturing before the Industrial Revolution.
Humanity has always been devising ways of trying to make life a little better or easier. In
the broad sense, prehistoric people practiced chemistry beginning with the use of fire to
produce chemical changes like burning wood, cooking food, and firing pottery and bricks.
Clay was shaped into useful utensils and baked to form water-resistive hard forms as crude
jars, pitchers, and pots at least as far back as 5000 B.C. 
The oldest of the major industrial chemicals in use today is soda ash. It seems to date
back to 3000 to 4000 B.C. because beads and other ornaments of glass, presumably made
with soda ash, were found in Egyptian tombs. It seems a natural soda ash was used as an
article of trade in ancient Lower Egypt. 
From what we know today, even the earliest civilized man was aware of the practical use
of alcoholic fermentation. The Egyptians and Sumerians made a type of ale before 3000
B.C., and the practice may have originated much earlier. Wine was also made in ancient
Egypt before 3000 B.C. by treading the grapes, squeezing the juice of the crushed grapes,
and allowing the juice to ferment in jars. In addition to the ale and grape-wine, the ancients
drank date-wine, palm-wine, and cider. 
The Romans and Greeks before the Christian era seem to have been without soap as we
know it, and to some of us today their cleaning methods seem unrefined. The Greeks used
oil for cleansing the skin, and supplemented it with abrasives such as bran, sand, ashes, and
pumice-stone. Clothes and woolen textiles were cleaned by treading the material or beating
the fabric with stones or a wooden mallet in the presence of fuller's earth together with
alkali, lye, or more usually ammonia in the form of stale urine. Roman fullers put out
pitchers at the street corners to collect urine. As repugnant as it seems to many, it should be noted that stale urine was used for cleaning clothes from Roman times up to the nineteenth
century, when it was still in use on sailing ships. 
During the 900s, Europeans only lived for about 30 years, and life was a matter of much
toil for very little rewards. Food was scarce, monotonous, often stale or spoiled. Homes
offered minimal protection from the elements and clothing was coarse and rough. War, disease,
famine, and a low birth rate were ever present. Fewer than 20 percent of the
Europeans during the Middle Ages ever traveled more than 10 miles (16 km) from the
place they were born. The age that followed these bleak years brought forth a burst of
inventiveness as mankind began to understand how science could take over some of their
burdens. 
In Europe, the harvesting and burning of various seaweeds and vegetation along the
seashore to create a type of soda ash product is one of the earliest examples of recorded
industrial chemical manufacturing. No one is sure when this type of chemical processing
began, but it was fairly widespread before modern recorded history. In fact, the Arabic
name for soda, al kali, comes from the word kali, which is one of the types of plants harvested
for this early industrial chemical producing activity. The desired product of this
burned vegetation was extracted with hot water to form brown colored lye. The process
yielded primarily sodium carbonate (or by its common name, soda ash), which was used
to manufacture soap and glass. Soda ash is by far the oldest of the major industrial chemicals
used today. 
During the 1600s and 1700s, scientists laid the foundations for the modern chemical
industry. Germany, France, and England initially manufactured inorganic chemicals to preserve
meat and other foods, make gunpowder, dye fabrics, and produce soap. In 1635, the
first American chemical plant started up in Boston to make saltpeter for gunpowder and
for the tanning of hides. 
The chemical industry was being formed as the Industrial Revolution began, but as late
as 1700, only 14 elements had been identified. The early chemical manufacturing process
development can be accredited to Nicolas LeBlanc, a physician to the Duke of Orleans,
who outlined a method of making soda ash starting with common table salt. The Duke of
Orleans gave Dr. LeBlanc sufficient funds to build such a plant not far from Paris in the
1790s. Other soda plants sprang up in France, England, Scotland, Austria, and
Germany.
The LeBlanc Process was the first large-scale industrial chemical process. The process
produced large quantities of gaseous hydrochloric acid as a by-product that released into
the air and caused what was probably the first large-scale industrial pollution. It was later
found that this waste gas could be captured and reacted with manganese dioxide to produce
gaseous chlorine. The LeBlanc Process was used until about 1861, after which it
began to be replaced by the more efficient Solvay Process. 
The Modern Industrial Chemical Industry Modifies Our Way of Living.
During the 1800s, chemists discovered about half of the 100 known elements. After 1850,
organic chemicals, such as coal-tar dyes, drugs, nitroglycerin explosives, and celluloid
plastics were developed and manufactured. The two World Wars created needs for new and
improved chemical processes for munitions, fiber, light-weight metals, synthetic rubber, and fuels. The 1930s witnessed the production of neoprene (1930), polyethylene
(1933), nylon (1937), and fiberglass (1938), which signaled the beginning of an era that
would see plastics replace natural materials. These "plastics" would radically influence how
things were designed, constructed, and packaged. 
After the Second World War, the expansion of the petroleum refining and chemical
process industries far outstripped that of the rest of the manufacturing industries. The
chemical industry also was different than the older established industries due to the nature
of toxic and flammable liquids and gases. Naturally, the handling and storage of hazardous
materials presented a potential peril that was often far greater than those posed by
the traditional industries.
By the 1950s and 1960s chemical processing became more and more sophisticated, with
larger inventories of corrosive, toxic, and flammable chemicals, higher temperatures, and
higher pressures. It became no longer acceptable for a single well-meaning individual to
quickly change the design or operation of a chemical or petrochemical plant without
reviewing the side effects of these modifications. Many case histories of significant process
accidents vividly show examples of narrowly focused, resourceful individuals who cleverly
solved a specific, troubling problem without examining other possible undesired consequences.
This book will focus on a large number of near misses, damaging fires, explosions, leaks,
physical injuries, and bruised egos. A flawed "plant modification," improper maintenance,
poor operating practice, or failure to follow procedures was determined to be at least a contributory cause in many case histories cited in the chapters that follow. Strangers to the
chemical industry might be tempted to think that it is one of the most hazardous of industries;
the opposite is true. The U.S. Chemical Industries (and most European Chemical
Industries) are among the safest of all industries. The facts show that it requires a high
degree of discipline to handle large quantities of flammable, combustible, toxic, or otherwise
hazardous materials.
The chemical industry generally handles business so well that it is difficult to find large
numbers of recent incidents for examples. Many of the featured case histories in this book
occurred over 20 years ago; however, the lessons that can be learned will be appropriate into
the twenty-first century. Tanks can fail from the effects of overpressure and underpressure
in 2010 just as well as they failed in the 1980s. Incompatible chemicals are incompatible
in any decade and humans can be forgetful at any time. Before we review a single case history,
it is time to boast about the safety record of the chemical industry.
Risks Are Not Necessarily How They Are Perceived.
True risks are often different than perceived risks. Due to human curiosity, the desire to sell
news, 24-hour-a-day news blitz, and current trends, some folks have a distorted sense of
risks. Most often, people fear the lesser or trivial risks and fail to respect the significant dangers faced every day.
Two directors with the Harvard Center of Risk published (2002) a family reference to
help the reader understand worrisome risks, how to stay safe, and how to keep the risk in
perspective. This fascinating book filled with facts and figures is entitled Risk - A Practical
Guide for Deciding What's Really Safe and What's Really Dangerous in the World Around The Introduction to Risk - A Practical Guide . . . starts with these words.

</file>

<file= AmE06_J06>

Intrinsic differences between tunneling two-level systems (TLSs) in molecular versus polymeric
glasses are revealed by studying the effect of compression on TLS dynamics. Photon echo studies
under variable low-temperature (1.1 - 2.3 K) and high-pressure (0 - 30 kbar) conditions have been
performed to contrast the effect of compression on molecular [2-methyl-tetrahydrofuran (2MTHF)]
versus polymer [Polymethylmethacrylate (PMMA)] glasses. The pressure-induced reduction in the
magnitude of the optical dephasing rate of rhodamine 640 in a molecular glass (2MTHF is found
to be comparable to the volume decrease of the glass (e.g., 20% at 30 kbar), indicating that TLSs
in 2MTHF are associated with void space or low-density regions of the glass. In contrast, the
relative pressure insensitivity observed for organic polymer glasses (PMMA) supports the idea that
these TLSs are associated with side chain defects. The power-law exponent for the
temperature-dependent dephasing in 2MTHF also decreased significantly at high pressure,
suggesting a change in the form of the TLS density of states upon compression of the molecular
glass.
INTRODUCTION.
The thermal, acoustic, and optical dephasing characteristics of glasses at low temperature are well described by tunneling two-level-system (TLS) models that postulate a
distribution of phonon-assisted tunneling rates. Furthermore, the similar TLS dynamics observed for a variety of inorganic, organic, covalent, and molecular glasses has led
to a rather homogeneous view of TLS. In the present study, high pressure is used as a relevant variable for probing the nature of TLS in different glasses. The compression of materials under extreme pressure permits experimental perturbation of low frequency modes, including tunneling two-level systems in glasses. The ability to experimentally alter TLS of glasses provides a tool
to differentiate the microscopic origin of TLS in glasses. The standard approximation in TLS dynamics yields a temperature dependent pure dephasing rate (at (sufficiently low temperatures,
(4 K).
In this TLS model the magnitude of the pure dephasing rate a is proportional to the product of the TLS-chromophore coupling and the TLS density (i.e., number of TLSs per unit
volume), and the temperature exponent a is related to the TLS density of states (i.e., number of TLSs of energy E per unit volume), where E is the TLS energy splitting and a=1+m. At temperatures
above 4K the excited state of the chromophor could couple to thermally populated optical phonons15 and obscure the TLS dynamics. In the present study the temperature is low enough (1.1 - 2.3 K) that TLS dynamics is the dominant dephasing mechanism, and high pressure is utilized
as a probe of the physical nature of TLS. The microscopic nature of TLSs has been identified in only a few systems, such as the reorientation of the central phenyl ring of
p-terphenyl near boundaries between domains of different
central phenyl ring ordering or the large cooperative reorientations
of 30 SiO4 tetrahedra in SiO2. For polymer systems
and molecular glasses such as polymethylmethacrylate (PMMA) and 2-methyl-tetrarydrofuran (2MTHF), the microscopic
nature of the TLS is unknown. The present study utilizes
high pressure to differentiate the properties of TLSs in a
molecular versus a polymeric glass.
The compression of the glass matrix under high pressure
leads to corresponding changes in the TLS coordinates18,19
that can affect the TLS dynamics. Photon echo measurements
under variable low-temperature (1.1 - 2.3 K) and variable
high-pressure (0 - 30 kbar) conditions can be used to
characterize changes in the TLS dynamics. High-pressure
optical dephasing studies in various polymer glasses at low
temperature (4 K) indicate that pressure-induced
changes in TLS density, tunneling barrier heights, and TLS-phonon
interactions can lead to dephasing rates that are either
insensitive to pressure or that increase at higher
pressure. In contrast, high-pressure hole-burning measurements
in molecular glasses yield pressure-induced line
narrowing. The present study will contrast temperatureand
pressure-dependent photon echo measurements using a rhodamine 640 (Rh640) chromophore in a 2-methyltetrahydrofuran (2MTHF) molecular glass versus a polymethylmethacrylate
(PMMA) polymer glass. 
EXPERIMENT.
A methylene chloride solution of Rh640 and PMMA was evaporated to obtain a film of a few hundred microns thickness. The final concentration of Rh640 in the PMMA film
was 10−4M. For high-pressure studies a sample was placed in a gasketed Merill-Bassett diamond-anvil cell (DAC), which was then immersed in liquid helium. Samples
of Rh640 in 2MTHF were prepared by dissolving Rh640 in 2MTHF. At low pressures, solutions of Rh640 in 2MTHF were frozen by immersion into liquid helium while high-pressure
Rh640 in 2MTHF was solidified by compression in a DAC, which was subsequently immersed in the liquidhelium cryostat.
The sample pressure of the DAC in the helium bath was
determined by measuring the fluorescence shift of a ruby
chip placed in the DAC sample compartment. Temperatures
from 1.2 to 2.1 K were obtained by controlling the
vapor pressure in a liquid-helium immersion Dewar.
The photon echo measurements were performed using a
cavity-dumped, tunable dye laser (maximum pulse energies
of 10J), pulse width of 30 ps, at a repetition rate of 800 Hz
[pumped by the second-harmonic output of a mode-locked
Q-switched neodymium-doped yttrium aluminum garnet
(Nd:YAG) laser]. The laser wavelength was set slightly to
red of the zero-phonon absorption peak (e.g., 585 nm at 0
kbar and 605 nm at 30 kbars for Rh640 in PMMA). The laser
pulses were attenuated to about 0.1J, focused to 50m,
and spatially overlapped in the sample.
RESULTS.
Pressure-dependent photon echo measurements.
The "homogeneous" dephasing time T2 was determined
by fitting the two pulse photon echo measurements to an
exponential decay, where T is the delay time between the two laser pulses. The
temperature-dependent dephasing time T2 was determined at
several different pressures over the range of 0 - 30 kbar for
Rh640 in both 2MTHF and in PMMA. Independent transient
grating measurements of the pressure-dependent lifetime T1
were used to determine the temperature dependence of the
pure dephasing rate 1/T2=1/T2−1/2T1.
Effect of compression on TLS dynamics in 2MTHF versus PMMA.
The temperature-dependent pure dephasing rate of
Rh640 in 2MTHF and PMMA, at both ambient pressure (0 kbar) and at high pressure (30 kbar), is illustrated in Fig. 1. The data in Fig. 1 were fit to a power law 1/T2=aT to
obtain the parameters listed in Table I. A significant lowering
of the pure dephasing rate is observed for Rh640 in 2MTHF
when the pressure is increased from 0 to 30 kbars. The
power-law prefactor a decreased by 20% over the 30 kbar
pressure range a30 kbars (a30 kbar=131/162), which is comparable
to the 20% volume decrease of the 2MTHF matrix
over this pressure range. The correlation of the dephasing
rate decrease with the volume reduction suggests that compression
of void space may be reducing the number of
TLS.9,10 In addition, the power-law exponent (i.e., the
slope in Fig. 1) was also observed to decrease from T1.6 at 0
kbar to T0.9 at 30 kbar. If the TLS density of states p(E) Em correlates with a power law of the form 1/T2=aT1+m, then this change would also suggest a corresponding
pressure-induced change in the form of the TLS density of
states.
Optical dephasing of Rh640 in PMMA versus 2MTHF at ambient pressure.
The temperature-dependent optical dephasing rates for
Rh640 in 2MTHF and PMMA have nearly the same powerlaw
exponent at ambient pressure; however, the optical
dephasing rate was observed to be 1.5X faster in 2MTHF
versus PMMA. The faster dephasing rate in 2MTHF has been attributed to a stronger TLS-chromophore coupling in 2MTHF. Since both the PMMA polymer (Tg=378 K) and
the 2MTHF molecular glass (Tg=87 K) are polar, the coupling
mechanism of the TLS to the polar ionic Rh640 chromophore
is expected to be similar in both glasses. The stronger
TLS-chromophore coupling in 2MTHF versus PMMA
(Ref. 8) may be due to slightly different solute-solvent interactions
(e.g., ether versus ester oxygen or due to TLSs that
are intrinsically different (e.g., void space in 2MTHF versus
side group motion in PMMA).
Comparison of high-pressure photon echo and spectral hole-burning studies of 2MTHF glass.
The pressure-dependent photon echo results for Rh640
in 2MTHF are comparable with pressure-dependent fast (15 ms delay) spectral hole-burning results reported for bacteriochlorophyll-a (BChl-a) in 2MTHF (not shown).
Hole-burning and photon echo measurements do not necessarily probe the same subset of TLSs (due to different intrinsic timescales), and different probe molecules (e.g., ionic
Rh640 versus nonpolar BChl-a) may also couple to different
TLS subsets. Nevertheless, the pressure-induced spectral line
narrowing reported for BChl-a in 2MTHF glass is comparable
to the decreased dephasing rate observed for Rh640 in
2MTHF at high pressure, suggesting that similar TLSs are
being probed in these different measurements.
DISCUSSION.
In general, the microscopic nature of TLSs are not well
understood. Crest and Cohen have proposed that TLSs can
be associated with void spaces that form as the temperature
of the glass is lowered below its glass transition temperature.
Simulations on Lennard-Jones computer glasses have indicated
that polymeric side groups can act as defects that give
rise to TLSs. Therefore, two types of TLSs are considered:
TLSs associated with internal void space formed as the viscous
glass formed and TLSs primarily associated with the
restricted motion of polymeric side groups. High pressure
provides an experimental means of compressing void space
to probe the properties of TLS in different glasses. The surprisingly
similar optical dephasing characteristics observed
for a wide variety of inorganic, organic, covalent, and molecular
glasses at ambient pressure have led to a rather homogeneous
picture of TLS in different glasses. However, the
present study illustrates that high pressure can be used to
characterize intrinsically different types of TLS that otherwise
yield similar dynamics under ambient conditions.
Compression of a glass can change the TLS dynamics by
decreasing the spatial separations, increasing the coupling of
TLS to both the chromophore and/or phonons, increasing
barrier heights, and collapsing TLS associated with voids.
It has been proposed that the free volume near the liquidglass
transition temperature can give rise to tunneling centers,
and Tg has also been used as a qualitative measure of
such tunneling barrier heights. The glass transition temperature
for PMMA at ambient pressure is Tg=378 K, and it
increases by 23 K/kbar from 0 to 3 kbar. A conservative
extrapolation yields a twofold increase in Tg by 30 kbar. A corresponding twofold increase in the average tunneling barrier
height (and corresponding increase in the distribution of
asymmetry parameters) should make a significant perturbation
of the TLS dynamics.
The volume change of PMMA under pressure (at room
temperature is given by P=6.4(kbar)[V0/VP)−1], where
P is the applied pressure and V0 /VP is the associated volume
change. Assuming the bulk modulus is not a strong function
of temperature, this corresponds to a volume reduction of
20% at a pressure of 30 kbar. However, this compression is
not isotropic on a molecular scale. For example, compression
is greatest along coordinates with the weakest intermolecular
interactions (e.g., void spaces followed by intermolecular
contacts and followed by covalent bonds). The significant
decrease in the compressibility of PMMA at pressures 10 kbar indicates a significant decrease in the available free volume over the pressure range of this study. Thus,
TLSs closely associated with free volume within the glass
are expected to be strongly affected by the 20% volume
compression, whereas TLS associated with tightly packed
polymer side groups may be relatively insensitive to void
space compression.
CONCLUSIONS.
Temperature-dependent optical dephasing measurements
under variable high pressure have been used to identify intrinsically
different TLSs in molecular versus polymer
glasses. The pressure-induced decrease of the pure dephasing
rate in the molecular glass 2MTHF is attributed to a reduction
in the number of TLSs associated with the collapse of
void space. The 20% reduction in the magnitude of the
dephasing rate of Rh640 in 2MTHF that occurs when the
pressure is increased from ambient to 30 kbar is comparable
to the corresponding 20% volume reduction of the glass.
This correspondence supports the idea that TLS in 2MTHF
may be associated with void space or low-density regions of
the glass. The pressure-induced change in the temperaturedependent
power-law exponent also suggests that the form of
the density of states of the remaining TLS is altered at high
pressure.
The relative pressure insensitivity of the lowtemperature
dephasing rates in polymer glasses indicates that
the nature of TLS in organic polymers  (e.g., PMMA and
polyisobutylene) is intrinsically different than those in molecular
glasses.

</file>

<file= AmE06_J07>

The Equations of Fluid Motion.
We live in an incredibly dynamical universe. Any chance observation bears
witness to the enormous diversity of motions and interactions that dominate
the structure of matter in astrophysical environments. It is the aimof this book
to put some of this in context. We see that gravitation, the ultimate structuring
force on the scales typical in astrophysical problems, plays a key role in
understanding how matter evolves. But we also must reckon with magnetic
fields, turbulence, and many of the same processes that dominate laboratory
studies.
Introductory Remarks.
When you think of a fluid, the idea of a structureless deformable continuous
medium comes to mind. Other properties, ingrained in childhood, are also
likely to occur to you; things like incompressibility, viscosity (if you were really
precocious), and perhaps even the statement that a fluid will not support
shear and will yield freely in the presence of an applied force. So what have
these to do with an astrophysical context? More than these definitions and
properties might lead you to suspect. Of course, you will likely think, since
stars are gaseous (something we have known since the 1860s), that we should
be dealing with kinetic theory and gas dynamics. The processes of rarefied
media are likely to dominate our understanding of cosmic bodies. This is not
quite true. Since a star is composed of gas which is homogeneous (in most
cases) and which acts collectively to create its own gravitational field, it mimics
rather well the behavior of a fluid moving (or sitting) under gravity. The
collision times are so short (or, put another way, the mean free paths are so short
compared with any scale lengths in the medium) in the interior of the star that any
disturbances can be washed out and the structure can be described as continuous. Naturally, this is, for the moment, only an assertion. We shall prove it
in due time.
Themost crucial point is that stars and all other cosmicmatter can be treated
as an ensemble object or system when we have carefully chosen some scales of
length and time. In a nutshell, the reason for this book is that we can very
often, at some magnification of scale or some rate of clock ticking, apply a fluid
approximation to the problems at hand. This book is meant to provide the
machinery, both computational and conceptual, with which to begin treating
dynamical and static problems posed by fluids in a nonterrestrial environment.
1.2
Equations of Motion
1.2.1
Distribution Functions
We start with a homogeneous medium of identical particles, forgetting for
the moment that this may be too restrictive an assumption. Imagine that this
group of particles is characterized by a global velocity distribution. Also, assume
that we can know this distribution function and that the positional information
can eventually be derived for the particles as well. Let us start with
a gas that consists of a collection of myriads of particles, all of identical mass.
If we assume that these particles execute collective motions, we will be able to
take ensemble averages and treat them as if they were a continuous medium.
This is what we mean by a fluid in an astrophysical context. But before we
can reach the stage of describing the matter as a classical substance, we need
to consider the microscale phenomena and how to incorporate them into a
macroscopic description of the motion and thermal properties. To do this, we
begin with a statistical mechanics treatment and then generalize from there.
Let us say that there exists a possibly time-dependent distribution function
f , which is a function of x, the particle positions, and v, their velocities, and
which provides a complete description of the chance of any single particle
having a specific position and velocity at any time. We assume that the particle
motions are individually governed by any forces imposed externally by the
medium and also by any mutual interactions (i.e., collisions). By this we mean
that the particles "see" each another through short-range interactions and also
collectively through bulk or ensemble interactions. A good example of the
former is the electrostatic interaction between charged particles in a plasma,
while the latter is exemplified by the integrated gravitational field resulting
from the distribution of the whole mass of the material. Both feed back into
the distribution function, both alter the microscale properties, and therefore both internal and external forces must be considered if we are to calculate the
physical attributes of the medium in the large.
Some constraints can be placed on the formof the distribution function right
from the start. For one thing, it should depend only on the magnitude of the
velocity (momentum), not on its direction. Another way of saying this is that
it should be symmetric with respect to the spatial and velocity components,
that is, f (x, v) = f (−x,−v). Since the distribution function is assumed to be
a measure of the probability that a particle will have a specific position and
velocity, f must be integrable and normalizable. It need not be algebraic; for
example, a delta function, δ(x)δ(v), is allowable. Now the hyperspace we are
dealingwith is well known. It is the phase space of the ensemble, that collection
of individual momenta and positions familiar from classical mechanics. We
can picture this collection as a group of free particles all passing through a
box in which we have placed an observer. This observer has no idea where
these came from or where they are headed, but can at least describe them in
the vicinity. They will arrive in this corner of the world, interact (perhaps),
and then exit. The overall result is that a complete distribution function can
be specified and, if this observer is not too egocentric, this function can even
be generalized to describe all of spacetime containing these particles.
A few things are then clear. The distribution function must be scalar and
depend only on scalar quantities. That is, it cannot depend on the placement
of the observerwithin the ensemble of particles. It must depend only on scalar
quantities, although these may themselves be combinations of many physical
properties. If the distribution function is to be global, it must be characterized
by some global parameter which is a constant for the system under study.
The assertion that the distribution does not change an inversion suggests that
it cannot be a pseudoscalar. So f must be positive everywhere. Since the fact
that the distribution function is defined in terms of a probability, we would not
know how to interpret negative values for f . But this property of probabilities
is very important for our considerations that follow.
If every particle in a gas has a position and a velocity, we might ask what the
mean value is of any quantity connected with this distribution. For instance,
wemaywish to knowthe average velocity, orwhat the average distance is that
a particle may be away from the statistical center of the distribution. These
are moments of the ensemble. Even though we cannot observe the motion of
every constituent component of a body, and cannot distinguish the histories of
the individual particle trajectories, we can still say something about the most
probable values that any of the measurable quantities will have.
Let us examine this physical picture in terms of simple probabilities, reducing
the distribution function to only one independent quantity. We must
be careful to choose a physically meaningful attribute. For instance, position
means something in an extended body. But color probably does not. Even if the particles have different colors, masses, or whatever, we can always ignore
these attributes until some quantity thatwe happen to be interested in requires
including them. Take, for example, the position of a particle. If the probability
of being some distance from a fixed point in space, x0 = 0, is P(x), then
the mean value for the displacement is (x) = xP(x)dx. Now assume that
we have a one-dimensional distribution, but one that extends over the range
(−∞,∞). Since the probability of being on the negative side or the positive
side of the reference point is assumed to be the same, the mean value for the
position must vanish; that is, on average the particle will be at the reference
point. Another way of saying this is that the integrand consists of a symmetric
and an antisymmetric part and therefore vanishes over the whole space.
But this clearly does not make sense if the ensemble is extended. There must
be some other way of treating the fact that many of the particles, although
perhaps equally distributed on the two axes, may not be concentrated at the
nominal zero point. We require a quantity that does not vanish on integrating
over the whole ensemble, (x2). This is a measure of the dispersion of the
particles in space, and unlike (x), (x2) is finite. Now we have both a symmetric
function and a symmetric interval and the mean value therefore does not
vanish.This has been a rather long digression. It is, however, prompted by the need
to place the process of taking moments in context.
Moments of the Distribution Function.
Of all the quantities that you can think of as characterizing this gas, the most
obvious ones are functions of velocity and density. This is just a product of
our Newtonian bias. We will separate the equations for the velocity into two
components. One is the mean velocity, which we shall write as Vi, and the
other is the random motion, which is assumed to have a mean of zero. This
velocity we shall call ui. All of the moments will be taken assuming that the
distribution function is taken over the random velocities only. For instance,
there are quantities around which classical descriptions in physics revolve:
the number density, n(x), the momentum flux, n(x)V(x), and the energy density,
1/2n(x)V · V. You will notice that each of these is a function of some power of
the velocity although each depends only on space and time, not on the internal
velocity distribution of the particles.
It is then not hard to see how to generalize this process to create as large a
collection of moments as we would like. Now you see why that long digression
was necessary. The principal reason for taking the various moments is to
remove the individual velocity components from the picture, to average over
that portion of phase space, and therefore to obtain mean physical quantities that characterize the macroscopic spatial distribution of the matter. To do this
within the limits of the function f (x, v), we proceed as follows.
If we integrate over the entire volume of phase space, we must recover the
total number of particles in the system.
We now have the prescription for taking moments!
Assume that we have components vi(x, t) = Vi + ui and that we are
free to choose any such components for examination. The subscript is then a
dummy, so that we can multiply these together as f vivj . . . vn, which we can
then integrate over the normalizable distribution function in velocity space.
The various moments are averages over the random velocity distribution
function. Since we cannot measure this in detail, we get rid of it via integration
(which is equivalent to averaging). Statistically, all of the macroscopic
properties of themedium are the expectation values of the distribution and its
moments. Historically, it was an important step forward when it was realized
that the proper treatment of thermodynamics, namely the statistical approach
rather than the vaguer mean-value methods of the mid-nineteenth century,
could also be taken over into dynamics of media composed of individual randomly
moving particles. It is no accident that the evolving theory of statistics
closely paralleled  -  and spurred  -  the development of statistical mechanics. It
provided a natural arena in which to display the ideas.
It is now time to begin taking the moments of the distribution. For example,
the density is clearly (from the previous discussion and definition) the
0th moment.

</file>

<file= AmE06_J08>

Introduction: Wet Feet Marching.
Blame It on Rio?
Imagine you carefully save money your entire life to buy a beautiful piece of land in the country to farm. The land and equipment cost more than you expect, and you quickly become dependent on a narrow margin of profit to sustain yourself and your family. Soon after you move in, however, someone buys the property bordering your land and immediately opens a landfill, accepting trash and hazardous wastes from the entire region. A mountain of trash rapidly grows; the landfill stinks, the noise of the trucks and bulldozers is deafening, and the waste leaks into your groundwater. Since the land is in an unzoned, unincorporated township, you have no recourse to stop the dumping through zoning limits, and the landowner is best friends with the major political and economic players in the county, state, and even the federal government. The value of your property plummets, and you cannot afford to move. The owner of the dump lives elsewhere and grows rich on its income; you suffer all the costs of his operation and gain none of the benefits. You seek to make an agreement with the neighbor, asking for some limits on his behavior. He negotiates with you for years, but never agrees to any substantial changes in his dumping. Instead, he says it would be unfair to have to do so unless you also agreed to stop dumping your farm waste, which would prevent you from being able to farm effectively. You turn to your other neighbors, seeking partners who will force the dump owner to clean up or close. Some agree, but these are only the poorest and least powerful of your neighbors - the others are friends of the dump owner, or own businesses that they fear might be hurt by the restrictions you seek. The dump owner suggests that the impacts of his dumping require more study and promises enormous research projects by scientists of his choosing. Repeatedly and with great fanfare he promises to lend you money on good terms to build a wall as a visual screen, to clean your drinking water, and to help you deal with other effects. Desperate for any progress, you accept his offers, but his promises are quickly forgotten, and the improvements are never completed. He asks you again to sign an agreement that in a few years would make it impossible for you to increase production on your farm to the point where your family could live decently. You and the other less powerful neighbors resist the agreement. With only slight changes in the details, this is the story of global warming and all the years of discussion and action since the issue was identified in the late 1980s. Now picture this nonfictional scene: After three years of frustrating negotiations following the drafting of the world's first framework for a treaty on global warming, Atiq Rahman, of the Bangladesh Centre for Advanced Studies, rose to his feet in a huge Berlin conference room. Looking out across a sea of scientists, negotiators, and lobbyists from around the world, Rahman struggled to express the urgency of the injustice of global warming in as plain words as he could find.  In the decade leading up to the 1995 conference, Bangladesh had been struck by two devastating floods and two typhoons that left over a hundred thousand people dead and tens of millions of people homeless.  With climate change, scientists predicted a rise in sea level and more severe tropical storms. 'If climate change makes our country uninhabitable,' Rahman warned, 'we will march with our wet feet into your living rooms' Looking back a decade later, Rahman's warning remains as painfully absurd now as it was then. The globe's wealthy and poor nations live in worlds so distant and disparate, and the wealthy are so sealed off from the poor, that Rahman's words might sound farfetched. Yet the plight of the world's poor cannot be ignored. The issue of reconciling social justice with environmental protection has surfaced at every major international meeting since the first environment and development conference at Stockholm in 1972, Nairobi in 1982, Rio 
in 1992, Rio+5 in New York, and Johannesburg in 2002. At the Rio Earth Summit, poor na- tions feared limits on their efforts to grow economically and care for the basic needs of their people, but several powerful industrialized nations refused to curtail their own excesses unless poor nations did the same. President George H. W. Bush's famous statement that 'the American lifestyle is not open to negotiation' remains a colorful reminder of this key sticking point. The most controversial issue at Rio was global climate change. Under intense pressure to do something, 187 nations eventually signed the United Nations Framework Convention on Climate Change (UNFCCC). However, the treaty avoided tough details. It called on nations to 'protect the climate system . . . on the basis of equity and in accordance with their common but differentiated responsibilities and respective capabilities' but consensus on these 'first principles' masked profound disagreement on the issue of actual obligations. Developing countries interpreted the 'common but differentiated' language with great precision: industrialized nations would need to take the lead by cutting their own emissions and transferring large sums of environmental assistance to the South.  However, developed countries saw more room for selective interpretation. Before the ink had even dried on the UNFCCC agreement, rich nations began to backpedal on their promise of massive technology transfer and technical assistance to the developing world.  The estimated price tag for sustainable development in the Third World was $625 billion a year, with the North supplying about 20 percent of the total cost in grants or below-market rate loans.  However, the rich nations delivered less than one-fifth of that promise.  Three years later, the 'Berlin Mandate' called for the rich nations to first reduce their emissions, with the poorer nations joining on the second or third round. More rounds of negotiations foundered on the rocks of equity and justice at Kyoto, Buenos Aires, Bonn, The Hague, and Marrakech.  President Bill Clinton signed on to the Kyoto Protocol to limit carbon dioxide emissions in 1997, but even before he did, the U.S. Senate voted 95 to 0 to support the Byrd-Hagel Resolution, which would block any 'unfair' treaty that did not require the poor nations to also address the problem.  This move by the United States bred great animosity in the developing world because of what was widely perceived to be Americans co-opting and thus undermining the Southern position of 'climate injustice.' Third World policy makers and activists were quick to point out that the average U.S. citizen dumps as much greenhouse gas into the atmosphere as nine Chinese and eighteen Indians, and that developing countries are immeasurably more vulnerable to rising tides, tropical storms, droughts, and flooding than rich nations. However, as we will argue in this book, social understandings of fairness are highly elastic and subject to political manipulation. The ominous 95 to 0 vote on the Byrd-Hagel Resolution specifically tried to discredit the protocol on the basis of the 'disparity of treatment between Annex I Parties [essentially the wealthy OECD] and Developing Countries.' Eventually, U.S. Secretary of State Madeline Albright declared a 'diplomatic full court press to encourage meaningful developing country participation' but poor nations continued to hold out.  Yet interestingly, almost all developing countries refused to accept scheduled commitments for future reductions of emissions in the name of fairness. In fact, the very suggestion that poorer nations should restrict their economic growth by reducing emissions led to an openly hostile negotiating environment. China's lead negotiator said 'In the developed world only two people ride in a car, and yet you want us to give up riding on a bus' Facing pressure from President Clinton, Chancellor Luiz Felipe Lampreia of Brazil flatly stated, 'We cannot accept limitations that interfere with our economic development.' President Clinton and Vice President Gore never dared to bring Kyoto to the Senate for ratification. Their successors, President George W. Bush and Vice President Cheney, then pulled the United States out of the Kyoto treaty entirely in March 2001 and in February 2002 offered a much weaker policy on reducing U.S. contributions to global warming. The Bush administration continues to oppose Kyoto because it is 'an unfair and ineffective means of addressing global climate change concerns' and 'would cause serious harm to the U.S. economy.' Diametrically opposed perceptions of 'climate justice' among rich and poor nations, we argue, pose a serious threat to political resolution and pollute a diplomatic atmosphere already teetering on the edge of disaster. Scientists and environmentalists in the world's wealthier nations are mystified as to why this life-threatening issue has elicited such an anemic policy response, but many of them miss the point: Responses to
climate change are wound up with other social and economic issues facing nations and are fundamentally about inequality and injustice. The Kyoto Protocol suffers from a similar short-sightedness. While Russia's ratification of Kyoto has put the treaty into effect, and public concern about climate change seems to be increasing, the foot dragging of the world's largest emitter and the skittishness of developing countries cast a long shadow of uncertainty over the future viability of any post-2012 North-South climate pact. A better understanding of the current policy impasse is therefore urgently needed.
Our Argument in Brief.
Over the past twenty years, the theoretical literature in international environmental relations has blossomed. Scholars have argued that international environmental policy outcomes are the result of material self-interest,  bargaining power, and the ability to strong-arm weaker states through more coercive forms of power. Others have emphasized the importance of exogenous shocks and crises,  salient solutions,  a scientific 'burden of proof'  environmental nongovernmental orga- nizations (NGOs),  postmaterialist values,  epistemic communities,  transnational activist networks,  corporate nonstate actors,  intergovernmental organizations,  and political leadership.  Yet curiously, few scholars speak of the one variable singled out repeatedly by policy makers: global inequality. The small body of theoretical work that does exist on the topic rarely provides clear causal explanations of how inequality matters and under what conditions it affects outcomes in international environmental politics. Most analysts rely selectively on anecdotal evidence and particularize explanations without explicitly addressing the generalizability of their claims.  And rather than explaining the origins of global inequality and the forces leading to its persistence, scholars often take it as given. Inequality as it relates to climate change is also rarely measured systematically in its several dimensions, and its roots are poorly understood. We take a different approach. We develop scientific measures systematically in its several dimensions, and its roots are poorly understood. We take a different approach. We develop scientific measures of climate inequality, utilize statistical methods to evaluate its proximate and deeper social and historical determinants, and examine the causal channels through which inequality influences the form, frequency, timing, substance, and depth of international cooperation. Our account of the North-South stalemate on climate policy relies on the integration of three types of arguments: general theories about the behavior of states, intermediate explanations about international environmental politics and North-South politics, and issue-specific insights concerning the 'problem structure' of climate change. In the first group are issues of trust, worldviews, causal beliefs, and principled beliefs - issues we believe are largely attributable to the position of countries in the global division of labor. Inequality, we argue, dampens utility-enhancing cooperative efforts by reinforcing structuralist worldviews and causal beliefs, creating incentives for zero-sum and negative-sum behavior, polarizing preferences, generating divergent and unstable expectations about future behavior, eroding trust and civic norms among different social groups, destabilizing policy coalitions, and making it difficult to coalesce around a socially shared understanding of what is 'fair' At the intermediate level are explanations of the ongoing development crisis and those arising in environmental debates over the definition of sustainable development, foreign assistance for the environment, and global versus local environmental concerns. Climate negotiations do not take place in a vacuum. They are taking place at a time when concerns about Northern callousness and opportunism in matters of international political economy are rising, levels of generalized trust are declining, and calls for fair processes and fair outcomes are being marginalized.

</file>

<file= AmE06_J09>

Problem.
Uterine infections often develop in some livestock species during the
first luteal phase postpartum. Exogenous prostaglandin F2a (PGF2a)
induces luteolysis, reduces progesterone, and enables the uterus to
resolve infections. However, the effects of PGF2a on luteal function and
on immune functions are confounded. These effects must be disentangled
to determine whether alternatives to antibiotic treatments can
be successfully developed.
Method of study.
Treatments were in a 2 X 2 X 2 factorial arrangement. Main effects were
ovariectomy or sham on day 0 (i.e. estrus), exogenous progesterone or
sesame oil from day 0 to 11, and exogenous PGF2a or saline on day 9.
Intrauterine inoculations with Arcanobacterium pyogenes and Escherichia
coli were administered on day 6.
Results.
Ewes treated with exogenous PGF2a either did not have uterine infections,
infections were less severe, or infections were resolving when
uteri were examined on day 12, despite increased progesterone.
Conclusions.
Exogenous PGF2a has effects on the resolution of uterine infections that
are independent of its effects on luteal progesterone production.
Introduction.
Cattle, sheep, and pigs are resistant to uterine infections
when progesterone concentrations are basal,
and they are susceptible to uterine infections when
progesterone concentrations are increased. In
dairy cattle, this transition, and thus the development
of a non-specific uterine infection termed pyometra,
is coincident with the development of the
first postpartum corpus luteum. In livestock production
systems, non-specific uterine infections are
inevitable and typically not life threatening; however,
they reduce fertility and increase costs. The incidence of non-specific uterine infections in herds
and flocks ranges from 10% to 50% of the dairy cattle,
20% to 75% of the dairy buffaloes, and 5% to
10% of the dairy sheep. These infections are
considered non-specific because the initial colonizing
bacteria are not known, and the specific bacteria
causing the signs of infection are not known.
Nevertheless, Arcanobacterium pyogenes and Escherichia
coli are usually associated with non-specific uterine
infections in cattle and sheep.
Antibiotics are used commonly to treat uterine
infections in livestock, but many antibiotic-treatment
protocols have not been effective. There are general concerns about the relationship between
antibiotic use in livestock and the potential for accelerating
the evolution of antibiotic-resistant strains of
bacteria, although resistance to ceftiofur, an antibiotic
commonly used to treat uterine infections in
dairy cattle, does not yet seem to be extensive.
Despite that alternative treatments, such as exogenous
prostaglandin F2a (PGF2a), have been evaluated
and found to be effective for some conditions, especially
pyometra.
Because exogenous PGF2a is luteolytic, its mechanism
of action has been presumed to be related to
reductions in circulating progesterone concentrations
and the resultant upregulation of immune functions.
 But, the actual mechanism by which exogenous
PGF2a enhances the resolution of uterine
infections is not known. In fact, PGF2a has direct
and positive effects on immune functions. Thus,
we conducted this study with sheep to determine
whether exogenous PGF2a enhances immune functions
and improves the ability of the uterus to
resolve an infection, independent of the effects of
PGF2a on luteal progesterone production.
Materials and methods.
All procedures with animals were in compliance
with U.S. Department of Agriculture, Agricultural
Research Service guidelines and U.S. Sheep Experiment
Station (USSES), Institutional Animal Care
and Use Committee guidelines. All sheep were maintained
at the USSES, fed diets that met their nutrient
requirements, and given free access to water before
and during the experiment. Except that feed and
water were withheld for approximately 36 hr before
laparotomy to minimize the chances that ingesta
would escape from the rumen and be aspirated into
the lungs.
Estrous Cycle Regulation
Mature white-faced ewes (combinations of Rambouillet,
Columbia, Targhee, and Polypay genetics)
in synchronized estrus were assigned to eight randomized
treatment groups. Estrus was synchronized
during the autumn breeding season: pessaries containing
60 mg of medroxyprogesterone acetate (Veramix;
Pfizer Animal Health, Orangeville, ON,
Canada) for 12 d; 15 mg of PGF2a (Lutalyse; Pfizer
Animal Health, New York, NY, USA) i.m. on day 6
of the 12-day period; and 400 IU of eCG (Sioux Biochemical Sioux City, IA, USA) i.m. at pessary
removal. After pessary removal, vasectomized rams
were used to check ewes for signs of estrus. Ewes
standing firmly to be mounted were considered to
be in estrus. If ewes were not observed in estrus
after pessary removal, they were not used for this
experiment.
Experimental Protocol
The treatment groups were in a 2X2X2 factorial
arrangement, with eight ewes per group. The main
effects were ovariectomy, exogenous progesterone,
and exogenous PGF2a. Ewes were ovariectomized or
a sham procedure was performed on the day estrus
was detected (i.e. day 0). Sodium pentobarbital
(65 mg/mL) was infused i.v. to achieve and maintain
a surgical plane of anesthesia. The uterus and ovaries
were exposed through a midventral laparotomy,
the ovaries were removed, and the uterus was
returned to the abdominal cavity. Or, the uterus
and ovaries were exposed through a midventral
laparotomy and returned to the abdominal cavity.
The laparotomy was then sutured closed. Progesterone
(5 mg/2.5 mL of sesame oil; Sigma Chemical, St
Louis, MO, USA) or sesame oil (2.5 mL) was injected
i.m. at 06.30 and 18.30 hours daily from day 0
through day 11. PGF2a (15 mg in 3 mL) or saline
(3 mL) was injected i.m. at 06.30 hours on day 9.
On day 5, a catheter was positioned in the vena
cava, via a saphenous vein, at a point just cranial to
the site of entry of uteroovarian blood. On day
6, all ewes received intrauterine inoculations of
75X107 cfu of A. pyogenes and 35X107 cfu of
E. coli. Vena caval blood was collected on days
9, 10, and 11 to measure lymphocyte proliferation in
vitro and for progesterone and PGF2a immunoassays.
On day 12, ewes were anesthetized with sodium
pentobarbital (65 mg/mL) and exsanguinated. Catheter
placement was confirmed postmortem.
The uterus was collected postmortem from each
ewe. Uterine contents were flushed out with 0.9%
NaCl solution (wt/vol), samples were cultured for
bacteria, flushings were centrifuged, and the appearance
of the endometrium was evaluated to determine
whether a ewe developed a uterine infection
in response to the bacteria and whether a uterine
infection appeared to be resolving. Clear uterine
flushings, small amounts of sediment (<5% by
volume), which contains bacteria and leukocytes, no
signs of endometrial inflammation, and the inability to culture dense colonies of A. pyogenes and E. coli from the flushings were signs that a uterus was not
infected. Cloudy uterine flushings, large amounts of
sediment (>5%, but usually >20%, by volume),
inflamed endometrium, and the ability to culture
dense colonies of A. pyogenes and E. coli from the
flushings were signs of infection. This method for
determining whether a sheep uterus contains an
infection and then classifying the severity of the
infection is consistent with a method used with cervical
mucus from cattle. Because all ewes received
intrauterine inoculations of bacteria, A. pyogenes and
E. coli, which are typically found in livestock environments,
were cultured from all uterine flushing.
However, sediment volume, density of bacterial colonies,
and appearance of the endometrium always
seemed to be positively related.
Vena caval blood was collected with syringes via
the catheters, transferred to heparinized tubes, and
held in an ice bath. Within approximately 1 hr after
blood was collected, a portion of each sample was
centrifuged, and plasma was harvested and stored at -20°C. A solid phase RIA kit (progesterone; Diagnostic Products, Los Angeles, CA, USA) was
used to quantify progesterone.7,28 An EIA kit (Cayman
Chemical Co., Ann Arbor, MI, USA) was used
to quantify PGF2a.
Another portion of each blood sample was used
for lymphocyte isolation and culture to evaluate unstimulated
and mitogen-stimulated lymphocyte proliferation.
The isolation and culture procedures were
the same as those we have used previously, except
that a non-radioactive method, instead of [3H]thymidine
incorporation, was used according to the manufacturer's
instructions to quantify lymphocyte
proliferation (CellTiter 96 AQueous Non-Radioactive
Cell Proliferation Assay; Promega, Madison, WI,
USA). Concanavalin A (Con A; stimulates T
cells; 1.0 lg/well; Sigma) and lipopolysaccharides
(LPS; stimulates B cells; 0.5 lg/well; Sigma) were
the mitogens used.  The data were expressed
as optical density units (odu).
Statistical Analyses.
The General Linear Model (GLM) procedures of SAS
(SAS Inst. Inc., Cary, NC, USA) were used to analyze
the data. The anova model included terms for ovariectomy,
progesterone, PGF2a, ovariectomy X progesterone,
ovariectomy X PGF2a, progesterone X PGF2a,
ovariectomy X progesterone X PGF2a, ewe nested
within ovariectomy · progesterone · PGF2a, day, and
all interactions with day. Ewe nested within ovariectomy
X progesterone X PGF2a was the main plot error
term, and residual was the subplot error term. Unstimulated
lymphocyte proliferation was used as a linear
covariant in the anova models to analyze the data
for mitogen-stimulated proliferation. For volume
of sediment in uterine flushings, the model included
terms for ovariectomy, progesterone, PGF2a, ovariectomy
X progesterone, ovariectomy X PGF2a, progesterone
X PGF2a, and ovariectomy X progesterone X
PGF2a. The Duncan's option was used as needed, after
a prior significant F-test, to compare means.
Results.
Treatment groups will be abbreviated as, for example,
sham-oil-saline, which refers to ewes that were
sham ovariectomized on day 0, given sesame oil
injections from day 0 through day 11, and given an
injection of saline on day 9. Ovariectomy - progesterone - 
PGF2a refers to ewes that were ovariectomized
on day 0, given progesterone injections from day 0
through day 11, and given an injection of PGF2a on
day 9.
Vena caval progesterone concentrations were
consistent with the conditions of each treatment
group (Table I). Exogenous progesterone increased
(P < 0.05) vena caval progesterone concentrations,
and exogenous PGF2a decreased (P < 0.05) vena
caval progesterone concentrations in Sham ewes.
Progesterone concentrations on days 9, 10, and 11
in sham-oil-saline ewes were indicative of functional
corpora lutea. In sham - oil - PGF2a, progesterone concentration
decreased after PGF2a treatment on day
9; this indicates that exogenous PGF2a was luteolytic.
In sham - progesterone - saline and sham - progesterone - 
PGF2a, progesterone concentrations on
days 9, 10, and 11 were greater than they were in
all other groups. Progesterone concentrations in
ovariectomy - progesterone - saline and ovariectomy - 
progesterone - PGF2a were similar to those in
sham - oil - saline on days 9, 10, and 11 and in sham - 
oil - PGF2a on day 9, but they were greater than
progesterone concentrations in ovariectomy - oil - 
saline and ovariectomy - oil - PGF2a, which were
0.3 ng/mL or less on days 9, 10, and 11. The data
indicated that exogenous progesterone enhanced
vena caval progesterone concentrations in ovaryintact
ewes and maintained progesterone concentrations
in ovariectomized ewes. Ovariectomy produced  basal progesterone concentrations, and exogenous
PGF2a removed luteal progesterone. Thus, the treatment
groups produced the desired progesterone concentrations.
Ovariectomy decreased (P < 0.05), exogenous progesterone
increased (P < 0.05), and exogenous PGF2a
decreased (P < 0.05) sediment volume, but none of
the interactions were significant (Table I). On the
basis of the criteria used, sham - progesterone - saline,
sham - progesterone - PGF2a, and ovariectomy - progesterone - 
saline ewes had uterine infections at the
time uteri were collected on day 12; infections
seemed less severe in sham - progesterone - PGF2a and
ovariectomy - progesterone - saline than in sham - 
progesterone - saline ewes. Ovariectomy - oil - saline,
ovariectomy - oil - PGF2a, and ovariectomy - progesterone - 
PGF2a ewes did not have uterine infections on
day 12. Sham - oil - saline and sham - oil - PGF2a ewes
seemed to be resolving uterine infections on day 12,
although determining whether a ewe was resolving
an infection or whether the infection was less
severe throughout the experiment is somewhat
subjective.
Ovariectomy decreased (P < 0.05) and exogenous
PGF2a increased (P < 0.05) vena caval PGF2a concentrations
(Table I). PGF2a concentrations varied
(P < 0.05) with day (Table I). The concentrations of
PGF2a in sham - oil - PGF2a, sham - progesterone - 
PGF2a, and ovariectomy - progesterone - PGF2a ewes
were greater (P < 0.05) on day 10, which was 1 day
after exogenous PGF2a, than they were on days 9
and 11. This increase seemed to account for the day
effect (Table I).
Exogenous progesterone reduced (P < 0.01)
unstimulated lymphocyte proliferation, and the
PGF2a · day interaction was significant (P < 0.01;
Table II). Unstimulated lymphocyte proliferation
on days 10 and 11 was greater in PGF2a than in
Saline ewes (PGF2a: day 10, 1.95 and day 11,
1.95 odu; saline: day 10, 1.52 and day 11,
1.28 odu); this seemed to be the source of the
interaction. 
Prostaglandin F2a treatment increased (P < 0.05)
Con A-stimulated lymphocyte proliferation (Table II).

</file>

<file= AmE06_J10>

PRELIMINARIES.
INTRODUCTION.
The best time to contemplate the quality of evidence from a clinical trial is before it begins. High-quality evidence about the effects of a new treatment is a consequence of good study design and execution, which themselves are the results of careful planning. This book attempts to acquaint investigators with ideas of design methodology that are helpful in planning, conducting, analyzing, and assessing clinical trials. The discussion covers a number of subjects relevant to early planning and design; some that find general agreement among methodologists and others that are contentious. It is unlikely that a reader experienced with clinical trials will agree with all that I say or what I choose to emphasize, but my perspective should be mainstream, internally consistent, and useful for learning. This book is not intended to be an introduction to clinical trials. It should be part of a one- or two-quarter second structured postgraduate course for an audience with quantitative skills and a biological focus. The first edition evolved over a dozen years from the merging of two courses: one in experimental design and one in clinical trials. This second edition is the result of seven additional years of teaching and concomitant changes in the field. The book assumes a working knowledge of basic biostatistics and some familiarity with clinical trials, either didactic or practical. It is also helpful if the reader understands some more advanced statistical concepts, especially lifetables, survival models, and likelihoods. I recognize that clinicians often lack this knowledge. However, many contemporary medical researchers are seeking the required quantitative background through formal training in clinical investigation methods or experimental therapeutics. No clinical knowledge is needed to understand the concepts in this book, although it will be helpful throughout. Many readers of this book will find the discussion uneven, ranging from basic to technically complex. This is partly a consequence of the very nature of clinical trials and partly the result of trying to address a heterogeneous population of students. My classes typically contain an equal mixture of biostatistics graduate students, medical doctors in specialty or subspecialty training (especially working toward a degree in clinical investigation), and other health professionals training to be sophisticated managers or consumers of clinical trials. For such an audience the goal is to provide breadth and to write so as not to be misunderstood. This book should be supplemented with lecture and discussion, and possibly a computer lab. The reader who does not have an opportunity for formal classroom dialogue will need to explore the references more extensively. Exercises and discussion questions are provided at the end of each chapter. Most are intentionally made open-ended, with a suggestion that the student answer them in the form of a one- or two-page memorandum, as though providing an expert opinion to less-experienced investigators.
AUDIENCE AND SCOPE.
The audience for this book is clinical trialists. It is not a simple matter to define a clinical trialist, but operationally it is someone who is immersed in the science of trials. Being a truly interdisciplinary field, trialists can be derived from a number of sources: (1) quantitative or biostatistical, (2) administrative or managerial, (3) clinical, or (4) ethical. Therefore students can approach the subject primarily from any of these perspectives.
It is common today for rigorous trialists to be strongly statistical. This is because of the fairly rapid recent pace of methods for clinical trials coming from that field, and also because statistics pertains to all of the disciplines in which trials are conducted. However, the discussion in this book does not neglect the other viewpoints that are also essential to understanding trials. Many examples will relate to cancer because that is the primary field in which I work, but the concepts will generalize to other areas. Scientists who specialize in clinical trials are frequently dubbed 'statisticians' I will sometimes use that term with the following warning regarding rigor: statistics is an old and broad profession. There is not a one-to-one correspondence  between statisticians or biostatisticians and knowledge of clinical trials. However, trial methodologists, whether statisticians or not, are likely to know a lot about biostatistics and will be accustomed to working with statistical experts. Many trial methodologists are not statisticians at all, but evolve from epidemiologists or clinicians with a strongly quantitative orientation, as indicated above. 
I have made an effort to delineate and emphasize principles common to all types of trials: translational, developmental, safety, comparative, and large-scale studies. This follows from a belief that it is more helpful to learn about the similarities among trials rather than differences. However, it is unavoidable that distinctions must be made and the discussion tailored to specific types of studies. I have tried to keep such distinctions, which are often artificial, to a minimum. Various clinical contexts also treat trials differently, a topic discussed briefly in Chapter 4. There are many important aspects of clinical trials not covered here in any detail. These include administration, funding, conduct, quality control, and the considerable infrastructure necessary to conduct trials. These topics might be described as the technology of trials, whereas my intent is to focus on the science of trials. Technology is vitally important, but falls outside of the scope of this book. Fortunately there are excellent sources for this material.
No book can be a substitute for regular interaction with a trial methodologist during both the planning stages of a clinical investigation and its analysis. I do not suggest passive reliance on such consultations, but intend to facilitate disseminating knowledge from which true collaborations between clinicians and trialists will result. Although many clinicians think of bringing their final data to a statistician, a collaboration will be most valuable during the design phase of a study when an experienced trialist may prevent serious methodologic errors, help streamline a study, or suggest ways to avoid costly mistakes. The wide availability of computers is a strong benefit for clinical researchers, but presents some dangers. Although computers facilitate efficient, accurate, and timely keeping of data, modern software also permits or encourages researchers to produce 'statistical' reports without much attention to study design and without fully understanding assumptions, methods, limitations, and pitfalls of the procedures being employed. Sometimes a person who knows how to run procedure-oriented packages on computerized data is called the 'statistician', even though he or she might be a novice at the basic theory underlying the analyses. It then becomes possible to produce a final report of a trial without the clinical investigator understanding the limitations of analysis and without 
the analyst being conversant with the data. What a weak chain this is. The ideas in this book are intended to counteract these tendencies, not by being old-fashioned but by being rigorous. Good design inhibits errors by involving a statistical expert in the study as a collaborator from the beginning. Most aspects of the study will improve as a result, including reliability, resource utilization, quality assurance, precision, and the scope of inference. Good design can also simplify analyses by reducing bias and variability and removing the influence of complicating factors. In this way number crunching becomes less important than sound statistical reasoning. The student of clinical trials should also understand that the field is growing and changing in response to both biological and statistical developments. A picture of good methodology today may be inadequate in the near future. This is probably more true of analytic methods than design, where the fundamentals will change more slowly. Analy- sis methods often will depend on new statistical developments or theory. These in turn depend on (1) computing hardware, (2) reliable and accessible software, (3) training and 
re-training of trialists in the use of new methods, (4) acceptance of the procedure by the statistical and biological communities, and (5) sufficient time for the innovations to diffuse into practice. It is equally important to understand what changes or new concepts do not improve methodology but are put forward in response to non-science issues or because of creeping regulation. The best recent example of this is the increasing sacrifice of expertise in favor of objectivity in the structure and function of clinical trial monitoring (discussed in Chapter 14). Such practices are sometimes as ill considered as they are well meaning, and may be promulgated by sponsors without peer review or national consensus. Good trial design requires a willingness to examine many alternatives within the confines of reliably answering the basic biological question. The most common errors related to trial design are devoting insufficient resources or time to the study, rigidly using standard types of designs when better (e.g., more efficient) designs are available, or undoing the benefits of a good design with a poorly planned (or executed) analysis. I hope that the reader of this book will come to understand where there is much flexibility in the design and analysis of trials and where there is not.
OTHER SOURCES OF KNOWLEDGE. 
The periodical literature related to clinical trials is large. I have attempted to provide current useful references for accessing it in this book. Aside from individual study reports in many clinical journals, there are some periodicals strongly related to trials. One is Controlled Clinical Trials , which has been the official journal of the Society for Clinical Trials (SCT) (mostly a U.S. organization). The journal was begun in 1980 and is devoted to trial methodology. The SCT was founded in 1981 and its 1500 members meet yearly. In 2003 the SCT changed its official journal to Clinical Trials , the first issue of which appeared in January 2004. This reincarnated journal should be an excel- lent resource for trialists. A second helpful periodical source is Statistics in Medicine , which frequently has articles of interest to the trialist. It began publication in 1982 and is the official publication of the International Society for Clinical Biostatistics (mostly a European organization). These two societies have begun joint meetings every few years. Many papers of importance to clinical trials and related statistical methods appear in various other applied statistical and clinical journals. Reviews of many methods are published in Statistical Methods in Medical Research . One journal of particular interest to drug development researchers is the Journal of Biopharmaceutical Statistics . A useful general reference source is the journal Biostatistica , which contains abstracts from diverse periodicals. Statistical methodology for clinical trials appears in several journals. The topic was reviewed with an extensive bibliography by Simon (1991). A more extensive bibliography covering trials broadly has been given by Hawkins (1991). In addition to journals there are a number of books and monographs dealing with clinical trials. The text by Meinert (1986) is a practical view of the infrastructure and administrative supports necessary to perform quality trials, especially randomized controlled trials. Freidman, Furberg, and DeMets (1982) and Pocock (1996) also discuss many conceptual and practical issues in their excellent books, which do not require extensive statistical background. A nice encyclopedic reference regarding statistical methods in clinical trials is provided by Redmond and Colton (2001). There is a relatively short and highly readable methodology book by Silverman (1985), and a second more issue oriented one (Silverman, 1998) with many examples. Every trialist should read the extensive work on placebos by Shapiro and Shapiro (1997). In recent years many Web-based sources of information regarding clinicaltrials have been developed. The quality, content, and usefulness are highly variable and the user must consider the source when browsing. Resources of generally high quality that I personally find useful are listed in Table 1.1. My list is probably not complete with regard to specialized needs, but it provides a good starting point.
In the field of cancer trials, Buyse, Staquet, and Sylvester (1984) is an excellent source, although now becoming slightly dated. A contemporary view of cancer trials is given by Girling et al. (2003). The book by Leventhal and Wittes (1988) is useful for its discussion of issues from a strong clinical orientation. A very readable book with many good examples is that by Green, Benedetti, and Crowley (2002). In the field of AIDS, a useful source is Finkelstein and Schoenfeld (1995).

</file>

<file= AmE06_J11>

In the last edition of this chapter, Don Brenner
(Brenner, 1991) wrote:
"They are, nonetheless, important for a variety
of reasons. They have innate importance as members
of the family, to be compared and contrasted
with their relatives, in attempts to define
the phenotypic, phylogenetic, and ecological
boundaries of the family."
No words better define the current edition of
this chapter as the number of genera of Enterobacteriaceae
has risen 60%, from 15 to 24
(excluding Raoultella). This current list of 24 genera
does not even include such organisms as
Alterococcus agarolyticus and Saccharobacter
fermentatus, taxa that are not formally discussed
in this chapter, although they possess some characteristics
associated with the family Enterobacteriaceae
(Yaping et al., 1990; Shieh and Jean,
1998).
The exponential increase in new genera within
the family Enterobacteriaceae has been fueled
by molecular biology, including polymerase
chain reaction (PCR) technology and 16S (small
subunit) rDNA sequencing. This list of genera
will no doubt continue to increase. What has
resulted from this molecular onslaught is the
basic fundamental knowledge that the family
Enterobacteriaceae plays a critical role not only
in enteric disease of humans and animals, but
also as phytopathogens, as insect pathogens, and
in industrial processes. Members of the family
Enterobacteriaceae can be broadly classified
into four main categories for convenience,
although a number of these genera clearly overlap
into multiple categories (Table 1). Probably
the greatest explosion in our knowledge concerning
enterobacteria stems from new insight
into their role as endosymbionts of insects or of
the parasites that feed on insect larvae (Moran
and Baumann, 2000). Some genera, such as Photorhabdus
and Xenorhabdus, may serve as new
delivery vehicles of important bacterial toxins
with insecticidal activity, replacing such classic
agents as Bacillus thuringiensis (ffrench-
Constant and Bowen, 1999). Knowledge of the
association that genera, such as Brenneria and
Pectobacterium, have with plant diseases continues
to expand as our understanding of their correct
taxonomic position becomes apparent.
Phlyogenetic analysis has also revealed that
some agents, such as Calymmatobacterium granulomatis,
the causative agent of the sexually
transmitted disease, donovanosis (granuloma
inguinale), are actually members of the Enterobacteriaceae.
Thus, our knowledge regarding the
Enterobacteriaceae and factors regulating
molecular evolution of endosymbionts, disease
associations in humans, animals, and plants, biological
applications, and ecologic niches will continue
to expand at a rapid rate for the forseeable
future.
Arsenophonus (Androcidium)
In 1986, Werren et al. isolated the causative
agent (Arsenophonus nasoniae) of the son-killer
(sk) trait from infected tissues of the parasitic
wasp, Nasonia vitropennis. The sk trait, a sexratio
distorter, is carried by approximately 5% of
female wasps from natural populations (Gherna
et al., 1991). Infected wasps transmit the bacterium
to fly pupae (wasp host) during oviposition
and offspring then become infected perorally
(Werren et al., 1986). The bacterium causes the
death of only male wasp eggs (unfertilized) by
arresting their maturation. Werren et al. (1986)
recovered the slow-growing bacterium from
tryptic soy and brain heart infusion broth cultures
containing either infected sk-wasp tissues
or hemolymph of fly pupae parasitized by sk
females. Arsenophonus nasoniae was subsequently
proven to be the causative agent of the
sk trait by fulfilling Koch's postulates.
Arsenophonus nasoniae DNA was 0 - 5%
related to DNA from 54 representative species
of Enterobacteriaceae (Brenner, 1991). However,
total DNA from strain A. nasoniae SKI4
hybridized to an rRNA Escherichia coli
sequence enriched for, but not exclusive to,
members of the family Enterobacteriaceae
(Gherna et al., 1991). SKI4 was 62% related at
60°C and 42% related at 70°C to the Escherichia
coli probe. These values are similar to those exhibited by other members of the Enterobacteriaceae
including Providencia, Proteus mirabilis
and Xenorhabdus nematophilus. A phylogenetic
tree based upon rRNA sequences of selected
members of the family Enterobacteriaceae was
constructed and indicates that the genus Proteus
is the closest neighbor to strain SKI4 (Gherna
et al., 1991). This relationship is further supported
by the fact that the G+C content (39.5mol%) and fatty acid composition (large
amounts of C16:0 and 16:1 cis 9, smaller amounts
of 14:0) of strain SKI4 are very similar to that of
the genera Proteus and Providencia, respectively.
A second putative species in the genus Arsenophonus
has been detected as a "secondary (S)- endosymbiont" in the reduviid bug, Triatoma
infestans (Hyp a and Dale, 1997). A bacterial
culture TI1 was isolated from the hemolymph of
T. infestans. The TI1 bacteria could be propagated
in vitro by cocultivation with a mosquito
cell line (Aedes albopictus) but could not be cultured
in vitro. PCR amplification of a 16S rDNA
product (1361 bp) from TI1-A. albopictus cultures
and comparison to other 16S rDNA
sequences from eubacteria indicated that the
endosymbiont was a member of the γ-3 subdivision
of the class Proteobacteria (Hyp a and
Dale, 1997). The TI1 rDNA sequences are 96.2%
related to those of A. nasoniae and a phylogenetic
tree indicates that A. nasoniae is the most
closely related taxa to TI1. Because TI1 cannot
be cultured in vitro and valid biochemical tests
cannot be performed, Candidatus status (provisional
assignment) has been applied prior to the
vernacular epithet (Murray and Schleifer, 1994;
Hyp a and Dale, 1997). Whether Arsenophonus
contains the enterobacterial common antigen
(ECA) is unknown.
Arsenophonus nasoniae is a fastidious Gramnegative,
oxidase-negative, catalase-positive,
non-sporeforming, nonmotile rod that produces acid from D-glucose, fructose, and sucrose
(Brenner, 1991; Gherna et al., 1991). It is unusual
(with respect to the Enterobacteriaceae) in its
inability to grow on MacConkey agar and to
reduce nitrates. It grows optimally at 30°C, with
a range from 15 to 35°C. A description of biochemical
characteristics associated with A. nasoniae
can be found in Gherna et al. (1991). The
optimal pH range for growth is 7.2 - 7.8, with a
pH range from 6.2 to 8.7 (Brenner, 1991). Limited
antibiotic susceptibility studies indicate that
A. nasoniae is resistant to polymyxin, penicillin,
methicillin and erythromycin and is sensitive to
gentamicin, chloramphenicol, tetracycline and
sulfathiazole (Werren et al., 1986). Growth does
not occur in minimal salts plus glucose without
the addition of protein digests; amino acid supplements
do not support growth. Vitamins, trace
metals, and IsoVitalex also do not support
growth (Brenner, 1991). The addition of 1% proteose
peptone (Difco no. 0120) to conventional
media, however, has been shown to improve
growth (Gherna et al., 1991). "Candidatus Arsenophonus
triatominarum" is also Gram negative,
nonmotile, and non-sporeforming (Hyp a and
Dale, 1997). The rods are highly filamentous 
(>15μm in length, 1 - 1.5μm in diameter).
Both A. nasoniae and Candidatus Arsenophonus
triatominarum are maternally transmitted,
and both are found in the central nervous system
(brain and neural ganglia) of infected insects.
Candidatus Arsenophonus triatominarum is
additionally found in the hemolymph, heart, salivary
glands, visceral muscles, nephrocytes, ovaries,
testes and dorsal vessels of T. infestans
(Hyp a and Dale, 1997). A sex-ratio distorter
trait due to.
Candidatus Arsenophonus triatominarum has
not been found in T. infestans.
The type strain of A. nasoniae is SKI4(= ATCC 49151).
Brenneria.
The genus Brenneria is composed of several
necrogenic phytopathogenic species that formerly
resided within the "amylovora group" of
the genus Erwinia. Phylogenetic investigations
conducted on 16 Erwinia species utilizing 16S
rDNA sequence data identified four distinct
clusters, one of which (cluster IV) contained the
species E. salicis, E. nigrifluens and E. rubrifaciens
(Kwon et al., 1997). Hauben et al. (1998a)
analyzed the 16S rDNA sequences of 29 plantassociated
strains and also identified four clusters.
In this later study, cluster III contained the
three species previously found to reside in cluster
IV by Kwon et al. (1997), in addition to E.
alni, E. paradisiaca and E. quercina. These six
species shared sequence similarities ranging
from 94.7 - 97.4%. The mean sequence relatedness
of cluster IV members to the genera
Erwinia and Pectobacterium were 94.5 and
95.4%, respectively (Hauben et al., 1998a). Signature
sequences identified among cluster III
members were conserved in all six species; these
sequences differed from those of cluster I
(Erwinia), cluster II (Pectobacterium) and cluster
IV (Pantoea). Based upon these results,
Hauben et al. (1998a) proposed the transfer of
these six Erwinia species to a new genus, Brenneria,
named in honor of the American bacteriologist
Don J. Brenner. Subsequent studies
investigating sequence divergence in the glyceraldehyde-
3-phosphate dehydrogenase (gapDH)
gene of the glycolytic pathway support the
genetic differentiation of the genera Brenneria
and Erwinia (Brown et al., 2000). However,
other phylogenetic studies have not demonstrated
such a robust separation between Brenneria
and Pectobacterium species (Spröer et al.,
1999). At the DNA level, E. salicis, E. nigrifluens
and E. rubrifaciens are 27 - 32% related to E.
quercina EQ 102; E. rubifaciens, E. nigrifluens
and E. quercina are 33 - 60% related to E. salicis
ATCC 15712 at 60°C (Brenner et al., 1974). Similar
relatedness values have been reported using
E. nigrifluens (29 - 47%) and E. rubrifaciens (25 - 
54%) as labeled DNA. The G+C content of
members of the genus Brenneria range from 50.1
to 56.1 mol% (Hauben et al., 1998a).
The genus Brenneria consists of Gramnegative,
oxidase-negative, catalase-positive
fermentative rods, 1.3 - 3.0 μm in length by 0.5 - 
1.0 μm in width (Hauben et al., 1998a). Biochemically,
Brenneria species are similar to Pantoea,
Pectobacterium and Erwinia in that they fail to
produce arginine dihydrolase and to decarboxylate
amino acids such as ornithine and lysine
(Lelliott and Dickey, 1984). Acid is produced
from the fermentation of D-glucose, D-fructose,
salicin, mannose and sucrose. Amylases are not
produced. Brenneria rubrifaciens produces a
pink diffusible pigment on YDC medium (1%
yeast extract, 1% D-glucose, 2% precipitated
chalk, and 2% agar). Most species are sensitive
to carbenicillin, cephalothin, chloramphenicol,
naladixic acid and tetracycline, and resistant to
bacitracin, erythromycin and gentamicin.
Differential characteristics useful in separating
members of the genus Brenneria are listed in
Table 2.
As former members of the Erwinia "amylovora
group," Brenneria species cause dry necrosis
or wilt in the specific host plants they infect.
Brenneria alni causes necrotic cankers in the
bark of the trunk, branches and twigs of the Italian
alder (Alnus cordata) and black alder (Surico
et al., 1996). In the warmer months, a watery
liquid is expressed from small cracks in the canker.
A similar disease called "drippy nut disease"
is caused by B. quercina in oaks. Brenneria nigrifluens
causes bark canker in the Persian
(English) walnut, while B. rubrifaciens causes
necrosis in the Persian walnut (Juglans regia).
Brenneria salicis is the causative agent of
"watermark disease" in willows (Salix sp.). This
vascular wilting disease is characterized by the
appearance of wilted, dried, brown-colored
leaves (Hauben et al., 1998b). Infection primarily
resides within the xylem vessels of infected
trees.
The type strains of Brenneria species are listed
in Table 3.
Buchnera.
The genus Buchnera consists of a single species,
B. aphidicola, which is an obligate intracellular
symbiont of the greenbug aphid, Schizaphis
graminum (Munson et al., 1991). Buchera aphidicola
cannot be cultured in vitro. The bacterium
exists within 60 - 80 huge specialized aphid cells
called "bacteriocytes" or "mycetocytes" located
within host-derived membrane vesicles termed
"symbiosomes" (Baumann et al., 1995). Buchnera
is vertically transmitted to eggs and offspring
by a process that is not completely
understood, thereby perpetuating the association
between bacterium and insect. Buchnera or
Buchnera-like endosymbionts also have been
reportedly found in a number of aphid species
including Rhopalosiphum maides, Rhopalosiphum
padi, Myzus persicae, Uroleucon sonchi,
Acyrthosiphon pisum, Diuraphis noxia, Chaitophorus
viminalis, Mindarus victoria, Pemphigus
betae and Melaphis rhois (Munson et al., 1991);
some species of the tribe Cerataphidini are
devoid of Buchnera endosymbionts (Baumann
et al., 1995).
Based upon 16S RNA sequence data, Buchnera
belongs to the γ-3 subdivision of the class
Proteobacteria and is most closely related to
members of the family Enterobacteriaceae
(Munson et al., 1991; Baumann et al., 1995). The
G+C content of Buchnera DNA is 30 mol%.
Buchnera aphidicola appears to have developed
a symbiotic association with a progenitor aphid
species approximately 150 - 200 million years ago
(Buades et al., 1999). The genome (630 - 643 kb)
is only about one-seventh the size of E. coli
(Wernegreen et al., 2000). However, the size of
Buchnera genomes recovered from different
aphid species is highly conserved (<3% difference),
suggesting genome shrinkage early in the
evolutionary symbiotic process. Multiple copies
of the genome (>120) exist within bacteriocytes
(Komaki and Ishikawa, 1999).

</file>

<file= AmE06_J12>

Pneumocystis spp. pneumonia (PCP) in humans and
in surrogate animal species typically occurs in the
absence of CD4 T cells, as takes place during acquired
immune deficiency syndrome. However, patients
treated with highly active anti-retroviral therapy
sometimes exhibit an exacerbation of diseases such
as PCP that coincides with resurgent CD4 T cells, a
phenomenon known as immune reconstitution disease.
We used an animal model of PCP using the
B-cell-deficient μmMT mouse together with antibodymediated
depletion of various T-cell subsets to examine
the role of CD4 and CD8 T cells in the development
of pathology in PCP. Although overt pathology
occurs in the presence of CD4 T cells only, CD8 T cells
only, or both, pulmonary injury occurs via different
paths, depending on the complement of T cells
present. Surprisingly, profound damage occurred
when only CD4 T cells were present, and this pathology
coincided with enhanced recruitment and activation
of eosinophils and strong type 2 cytokine polarization
in the alveolar environment. In addition, CD8
T cells can act to moderate this CD4 T cell-mediated
pathology, possibly by increasing the ratio of putative
CD25+ suppressor CD4 T cells to CD25- effector
CD4 T cells. 
Because the respiratory system is constantly exposed to
potentially pathogenic microorganisms, it possesses the
capability of mounting vigorous innate and acquired immune
responses. However, it is these same vigorous
responses that are often the source of pulmonary impairments
and damage to the respiratory epithelium during
microbial infections. This type of inflammation-related
damage is evident in pneumonia caused by the opportunistic
pathogen Pneumocystis, which typically only occurs
in individuals with some form of immunosuppression.
In immunocompetent patients or experimental
animals, Pneumocystis infection is easily cleared without
significant ensuing pathology. However, in patients with
reduced CD4 lymphocyte numbers [such as acquired
immune deficiency syndrome (AIDS) patients], and in
mice in which CD4 lymphocytes have been experimentally
depleted, steady growth of Pneumocystis occurs in
the lung despite the accumulation of inflammatory cells,
including macrophages, CD8 lymphocytes, and, in later
stages, neutrophils. The considerable pathology that
ensues typically coincides with the accumulation of these
inflammatory cells; indeed SCID mice that are deficient in
lymphocytes demonstrate less respiratory impairment
than mice that are only deficient in CD4 cells, even
though they have comparable loads of Pneumocystis.
These observations have led to studies that examined
the type(s) of inflammatory cells potentially involved in the
host tissue damage seen in Pneumocystis pneumonia
(PCP). Although the influx of neutrophils into the alveolar
space coincides with the onset of severe pathology in
PCP, we have recently presented evidence that this is
only correlational and that the neutrophil defense mechanisms
may not be directly involved in the development
of lung pathology. In contrast, there is stronger evidence
that the presence of CD8 lymphocytes is required for the
full development of respiratory impairment in PCP, although
the mechanisms by which this damage occurs
are not yet clear. It is overly simplistic, however, to think
of pulmonary damage in a disease state occurring because
of the unmodulated actions of one cell type alone.
Not only are many types of inflammatory cell types capable
of producing host tissue damage, but there is also
abundant evidence of significant interactions between
inflammatory cell types that can potentially enhance or depress the potential for inflammatory damage by one the of cells in question. These types of interactions
may be relevant in immune reconstitution disease (IRD),
which occurs in some AIDS patients in whom highly
active anti-retroviral therapy results in markedly increased
levels of CD4 lymphocytes. In some of these
individuals, intense inflammatory responses can occur in
the lungs (involving CD4 cells and residual Pneumocystis
organisms) and requires vigorous therapeutic intervention.
This clinical syndrome highlights the potential
interaction of multiple inflammatory cell types and mechanisms
that may be involved in the pathology of PCP.
Furthermore, this scenario also points to the need for a
well-characterized animal model of PCP in which the
functions of CD4 lymphocytes can be investigated.
To address these questions we examined the onset of
Pneumocystis-related pathology in an animal model of PCP
using the B-cell-deficient μMT mouse. Because these mice
cannot produce functional antibodies, they develop fullblown
PCP despite the presence of CD4 lymphocytes.By
depletion of specific lymphocyte subsets using antibodymediated
depletions, we examined the type and onset of
pathology during PCP in μMT mice that were deficient in
CD4 lymphocytes, CD8 lymphocytes, or both CD4 and CD8
lymphocytes, as well as mice that were replete in both CD4
and CD8 lymphocytes. We report here that significant inflammatory
damage occurs in both CD4-depleted and
CD8-depleted μMT mice during Pneumocystis infection, although
it may occur via different mechanisms. We also
report evidence that there is interaction between CD4 and
CD8 cells when both are present during Pneumocystis infection,
which alters the subsets of lymphocytes present,
the cytokine profile, and the type of pathology present at the
inflammatory site.
Materials and Methods.
Animals.
Breeder mice (μMT; stock number 002249) were obtained
from Jackson Laboratories (Bar Harbor, ME) and
bred in our facility at Montana State University. Control
C57BL/6 mice were obtained from the National Cancer
Institute (Fredrick, MD). Mice were kept in ventilated
cages, so that incoming air was HEPA filtered, and given
autoclaved mouse chow and acidified water.
Depletion of T-Cell Subsets and Infection with Pneumocystis.
For depletion of CD4 T lymphocytes, mice were given intraperitoneal
injections of 300 mg of the anti-CD4 antibody
GK1.5 (American Type Culture Collection, Manassas, VA)
twice weekly. Depletion of CD8 T cells was done using
twice weekly injection of 300 μg of the anti-CD8 antibody
TIB-210 (American Type Culture Collection). Some animals
received double depletions with twice weekly injections of
300 μg each of both GK1.5 and TIB-210.
Respiratory Measurements.
Before tissue sampling, respiratory rates and tidal volumes
were measured on conscious mice using a wholebody
plethysmograph (Buxco Electronics, Sharon, CT).
Arterial blood gases were determined after warming the
mice for 5 minutes at 39°C, carefully nicking the ventral
tail artery, and then collecting 150 μl of blood into a
heparinized capillary tube. The blood was mixed using a
small steel flea and magnet, and the samples were analyzed
on a clinical blood gas analyzer (Omni AVL; Roche
Diagnostics, Indianapolis, IN) within 1 hour of sampling.
Bronchoalveolar Lavage.
Mice were sacrificed by pentobarbital anesthesia (100
μg/g body weight) followed by exsanguination. The trachea
of each mouse was nicked with fine scissors, and
then a tube was inserted. Five 1-ml aliquots of Hanks'
balanced salt solution with 3 mmol/L ethylenediaminetetraacetic
acid were then applied to lavage the alveolar
contents. Samples from each 5-ml pooled lavage were
spun onto a slide using a cytospin centrifuge and then
stained with Diff-Quick dye (Dade Behring, Newark, DE).
Proportions of each cell type were determined microscopically
using a X100 objective lens. The bronchoalveolar
lavage fluid (BALF) supernatant was collected by
centrifugation at 200 X g for 10 minutes, and aliquots
were used for the following assays. Albumin and lactate
dehydrogenase (LDH) concentrations were determined
with commercial kits (631-2; Sigma Diagnostics, St.
Louis, MO, and CytoTox 96: Promega, Madison, WI).
Eosinophil peroxidase (EPO) activity in the BALF was
measured using a modification of the method of Duguet
and colleagues; briefly, aliquots of BALF were incubated
either with O-phenylenediamine, H2O2, and the
specific inhibitor 3-amino-1,2,3-triazole or with O-phenylenediamine
and H2O2 alone. After 30 minutes of incubation
at room temperature, absorbance at 405 nm was
measured. EPO-specific activity was estimated as the
absorbance measured in wells without the inhibitor
3-amino-1,2,3-triazole minus the absorbance in wells
containing the same sample with 3-amino-1,2,3-triazole.
Flow Cytometry and Cytokine Analysis.
Bronchoalveolar lavage (BAL) cells were resuspended in
a minimal volume of phosphate-buffered saline with 2%
calf serum and an anti-mouse Fc receptor antibody
(Trudeau Institute, Saranac Lake, NY) to block nonspecific
binding. These cells were then stained with a cocktail
of fluorophore-conjugated antibodies against the
mouse CD antigens CD4, CD8, CD28, and either CD25 or
CD62L (PharMingen, San Diego, CA) and analyzed on a
FACSCalibur flow cytometer (Becton Dickinson, Mountain
View, CA). Lymphocytes obtained from spleens of
control uninfected mice were used as a basis of comparison.
Flow cytometry-based bead ELISA kits (mouse Th1-
Th2 and Mouse Inflammation; Becton Dickinson) were used to measure cytokine concentrations in undiluted samples of BALF.
Enumeration of Pneumocystis.
After lavage, the trachea was ligated and two-thirds of the
lung were removed, placed into 5 ml of sterile Hanks'
balanced salt solution, and homogenized by pushing
through a metal mesh screen. An aliquot of this material
was diluted 1:20 and applied to a glass slide using a
cytospin centrifuge. After drying, the slides were stained
for an extended period of time in Diff-Quick dye (40 to 60
minutes). Pneumocystis nuclei (both cysts and trophozoites)
were then enumerated in a minimum of five X60
fields, up to a maximum of 50 (if nuclei were not readily
apparent). The average counts were then converted to
log Pneumocystis nuclei per lung. With this technique in
this laboratory, the limit of detection was (log) 4.43 when
50 fields were counted. Although the previous lavage
removes some Pneumocystis organisms from the lung, it
is a small fraction of what is present in the lung (<10%),
and this was consistent among different groups of mice.
Histology.
One-third of the lung (the large left lobe) was fixed for 24
hours in phosphate-buffered formalin, embedded in paraffin,
sectioned at 5 μm, and stained with either hematoxylin
and eosin (H&E) or Alcian Blue with periodic acid-
Schiff using standard histological techniques.
Statistical Analysis.
The software program Graph Pad Prism (San Diego, CA)
was used for all statistical tests of significance (to P <
0.05). Where greater than two groups were being compared,
one-way analysis of variance with Tukey's post
hoc pairwise comparisons were made. When only two
groups were compared, we used a two-sided t-test, with
Welch's correction if the groups had unequal variances.
In cases in which a deviation from a normal distribution
was observed, a nonparametric test (Mann-Whitney test)
was also applied. In those cases, we found that both the
t-test and Mann-Whitney test indicated the same result
(ie, both indicated significance or nonsignificance); however,
typically one test gave a more conservative P value
(larger, but still <0.05). The P values we report are always
the conservative values.
Results.
Although the common mouse model of Pneumocystis pneumonia
(PCP) is that of wild-type mice depleted of CD4
lymphocytes, it has previously been shown that μMT mice,
which are deficient in B-cell and antibody production, also
develop serious and progressive infections with Pneumocystis,
even with normal numbers of CD4 lymphocytes.
We first compared these two models and found that at 28
days after inoculation μMT mice exhibited a burden of
Pneumocystis that is only slightly lower than that seen in
CD4+ depleted wild-type mice infected at the same time
(Figure 1). However, there were some subtle differences in
the Pneumocystis-related pathology found in these two
models. Although there were no statistical differences in the
indicators of BAL albumin, BAL LDH, or respiratory rates,
arterial blood oxygen was significantly lower in μMT mice
(Table 1). μMT mice also recruited considerably more cells
into the alveolar compartment than did CD4-depleted wildtype
mice. These included lymphocytes (mostly CD8 lymphocytes)
and most strikingly, large numbers of eosinophils
(Figure 1). The CD4-depleted wild-type mice had fewer
accumulated CD8 lymphocytes in their lungs and, as would
be expected, had no CD4 lymphocytes and few eosinophils
in the BAL during Pneumocystis infection (Figure 1). However,
if CD4 lymphocytes were depleted from μMT mice,
then the Pneumocystis disease took on characteristics very
similar to that of CD4-depleted wild-type mice. This was true
with respect to Pneumocystis burden (Figure 1), physiological
impairments (Table 1), and cellular influx into the lung
(Figure 1).
Previous research has implicated CD8 lymphocytes as a
cause of host tissue damage in Pneumocystis, and this initial
study would seem to agree, as both B-cell-deficient and Band
CD4-cell-deficient mice had extensive host tissue damage.
To examine further the role of the CD8 cells, we infected
μMT mice that were depleted of CD4 lymphocytes,
CD8 lymphocytes, or both CD4 and CD8 lymphocytes as
well as undepleted μMT mice with Pneumocystis.

</file>

<file= AmE06_J13>

CHAPTER ONE.
IT'S WHO YOU ARE.
RIGHT AROUND the time Washington crossed the Delaware River, the French chemist Antoine-Laurent Lavoisier wrote this in his notebook: "La vie est une fonction chimique."
Life is a chemical process.
Lavoisier was either lucky or prescient. (If he was lucky, it didn't last. French revolutionaries jailed and beheaded him in 1794.) But it was two centuries before scientists figured out the basic principles of heredity and came to widely accept that we inherit traits from our parents through a process that can only be called "chemical."
Heredity is carried in our genes - genes that are made of DNA. In the year 2000, scientists announced that they had launched what they said was a scientific revolution, that they had opened the book on human life. Three years later, in April 2003, they delivered the final version of that book.
They claimed they had figured out - chemical by chemical - what the DNA in human genes is made of.
"Essentially, we are now able to read our own instruction books," explains Francis Collins, director of the National Human Genome Research Institute in Bethesda, Maryland. And the term "instruction book," he says, hardly begins to define what the effort has uncovered. It is also a history book explaining how humans have evolved over time. It's a shop manual that describes with incredible precision how to build every cell in the human body. And most
important, Collins says, it's a medical textbook containing insights that will help doctors predict and, eventually, cure disease. 
It is humbling for me and awe inspiring to realize that we have caught the first glimpse of our own instruction book, previously known only to God - Dr. Francis Collins, Human Genome Project leader.
"We are the first generation in history to turn the pages of this book, an awesome and humbling experience for anyone to contemplate. In considering epic moments in human history, this has
to be very high on the list. History will decide," he adds, "but I would place the Human Genome Project alongside splitting the atom or going to the moon."
Introducing Your DNA.
As most everyone knows by now, DNA is short for deoxyribonucleic acid. But do you know where your DNA is? Can you tell a gene from a chromosome? Did you know that your genes are located on your chromosomes and not the other way around? Do you know how cloning might be used to fight diabetes, which companies are using worm DNA to figure out how to slow human aging, or how doctors are employing DNA knowledge to finally win the fight against cancer? Most people don't.
The DNA sciences will dominate in the twenty-first century, and you need to understand the terms and the concepts if you're going to stay on top of and benefit from the huge DNA-related advances
in medicine and other sciences. 
At first, the science seems intimidating, but once you get a grasp of a few terms and concepts, you'll see it is all actually quite simple. 
A View from the Top.
You hear a lot about DNA "carrying" information - and I'll get to that in a minute. But first, let's talk about DNA as an object, an actual molecule that takes up physical space. To give you some perspective, let's start big and get smaller. Take a human body, any body. It consists, you may know, of ten systems: nervous, muscular, skeletal, endocrine, digestive, respiratory, circulatory, immune, reproductive, excretory. 
I think we will view this period as a very historic time, a new starting point - Craig Venter, founder of Celera Genomics. Each of those systems has organs. For instance, the stomach is
an organ of the digestive system. 
Every organ - like every living thing - is made up of cells. The stomach is made of stomach cells.
Almost every cell, stomach or otherwise, has a nucleus at its center.
And this is, for me, where things get interesting.
Every nucleus includes chromosomes, rod-like structures that,
under a microscope, most resemble bundles of thread. Every cell's
nucleus contains exactly twenty-three pairs of these chromosomes.
(The exception to this is reproductive cells, which contain half the
normal amount of chromosomes. That makes sense considering
that reproduction is the result of the fusing of two cells - a sperm
and an egg.)
Look closer at any particular chromosome - let's choose chromosome
19 inside the particular nucleus of a stomach cell we're
examining - and you'll find that chromosome's DNA tightly coiled
up inside. If you unraveled that DNA and straightened it, you would
find that it is shaped very much like a ladder. Sugars and phosphates
form the sides of each ladder, and the four so-called "bases" pair up
to form the rungs.
This is the outstanding achievement not only of our lifetime,
but in terms of human history. I say this because the
Human Genome Project does have the potential to impact
the life of every person on this planet.
Dr. Michael Dexter, director of The Wellcome Trust
The bases are guanine, adenosine, thymine, and cytosine - G, A,
T, and C for short. You may also hear them called letters or
nucleotides. If you think of DNA as a language, and I do, then this is
the alphabet.
A given gene (made of DNA) is simply a given group of base
pairs on a DNA molecule. For instance, here on chromosome 19
you can find a long string of bases that together form the so-called
APOE gene. If you were unlucky, you may have inherited a dangerous
variety of this gene (there are three varieties) on your chromosome
19. This could affect your ability to break down cholesterol
and fat - leading to coronary heart disease, Alzheimer's, or other
fat-related ailments.
However, and this is where the gene sequence could come in
handy, if you were able to discover this risk factor early, through a genetic test, you might choose to go easy on the bacon double
cheeseburgers, a choice that could extend your life.
WHY YOU AREN'T A BLUE BLOOD.
For centuries, presumably all the way back to Aristotle, folklore
had it that heredity passed through our blood. Think
of the terms "bad blood," "mixed blood," "royal blood,"
"blue blood," or "bloodline" and you get the idea.
The irony is that there is no heredity coded in your red
blood whatsoever. The red blood cells are the only kind of
cells in your body that don't have DNA - because they're
the only cells in your body that don't have nuclei.
Go figure.
To summarize, you have about 30,000 genes located throughout your twenty-three pairs of chromosomes, which are found in the nucleus of almost every one of your cells. These genes describe, in the alphabet of Gs, As, Ts, and Cs, everything about you - from how tall you are, to how curly your hair is, to how likely you are to suffer from bad breath or cancer.
Your personal DNA sequence is the language in which everything about you is written. Interestingly, almost every cell in your body has all the information required to build an entirely new you. Visualizing Your DNA. A single DNA molecule is incredibly long and skinny. Uncoiled from a microscopic chromosome, a single strand would stretch about two inches. String out all the DNA from all twenty-three chromosomes from a human egg just about as big as the comma at
the end of this clause, and its length would add up to about six feet. Line up all the threads of DNA from every cell in your body end to end, and the entire length would be long enough to reach to the sun and back 500 times. But the same strand would be many thousands of times skinnier than a human hair. In the deepest sense, DNA's structure and function have become as much a part of our cultural heritage as Shakespeare, the sweep of history, or any of the things we expect an educated person to know - Microbiologist Ross L. Coppel, from his book with G.J.V. Nossal, Reshaping Life: Key Issues in Genetic Engineering (Melbourne, Australia: Melbourne University Publishing, 2002).
Now consider how compact your DNA is. Almost every cell in your body contains more than six feet worth of DNA coiled up inside. Even so, the standout feature of DNA is the way it stores information  -  information that precisely instructs the cell how to replicate
itself and what functions to perform.
But What Does DNA Do, Exactly?
DNA's job is simple. Its code tells your body how to build protein. Protein is at the foundation of all living things. All living cells depend upon proteins for virtually all their products and processes. Cells - whether they're bacteria, plant, or animal cells - use proteins for a variety of processes, from fighting infection, to sending and receiving messages, to rebuilding damaged parts.
You, as a human being, contain at least 50,000 different kinds of protein. And each kind of protein has a specific job to do. There are structural proteins for building your hair (collagen), your skin (keratin), and your ligaments (elastin). Hormonal proteins like insulin
carry messages and regulate body processes. The hemoglobin in your blood is one example of a transport protein. There are antibody proteins to protect your cells against invaders and protein
enzymes for digesting and otherwise breaking things down. The list goes on and on.
The primary job of DNA is to tell the body what proteins to build and how to build them. The order of the chemical bases A, T, C, and G on a gene gives the cell the recipe for that particular protein. Scientists used to think that one gene always directed the body to create one protein, but now they know that a single gene can potentially create more than one kind of protein.
We share 51 percent of our genes with yeast and 98 percent with chimpanzees - it is not genetics that makes us human - Ethicist Dr. Tom Shakespeare, University of Newcastle.
The idea of DNA creating proteins is a critical one. Proteins are the workhorses of the human body; they do all the work in a cell. They carry out chemical reactions, form new tissue, send signals between bodily systems, regulate body chemistry, you name it. The simplest way to think of DNA is that it is, at its most basic, just a huge, long file in which all the instructions for creating the proteins in your body are written down.
We have to focus on the possibilities, develop them, and then
face up to the hard ethical and moral questions that are
inevitably posed by such an extraordinary scientific discovery - United Kingdom Prime Minister Tony Blair.
Eric Lander of the Massachusetts Institute of Technology has
called this the fundamental secret of life. "The secret of life is this
huge diversity of components; fifty thousand [proteins] that are all
specified in the same simple description of the DNA language."2
When you hear that scientists "have mapped" the human
genome, this is what they are talking about. They have figured out
the exact order in which A, T, C, and G appear on human genes.
Quite obviously, there are areas of variation that explain why one
human has blue eyes, for instance, and another brown. But the
DNA of any two humans is well over 99 percent identical.
DON'T IT MAKE MY BROWN EYES BLUE (OR NOT).
People have been wondering for thousands of years why it
was that their baby had hazel eyes, when the parents had
blue and brown.
A gene, recall, is a given stretch of DNA located on one of
your twenty-three chromosomes. That gene codes for a specific
protein, which in turn performs a specific function or
helps build a certain structure.
The gene for brown eyes, for instance, codes for a protein (an
enzyme, actually) that selectively deposits pigment on the
irises of your eyes. If you have blue eyes, you lack that protein.
Now scientists are in the process of figuring out which proteins are
coded for by various sequences of bases (genomics), what those proteins
do (a field called proteomics), and what happens when the
sequence goes awry (functional genomics).

</file>

<file= AmE06_J14>

The prevention of disease has long been based implicitly on taking action on the assumption that a disease is caused by a factor that can be controlled. Early examples include the experimental evidence generated by Lind, showing that consumption of oranges and lemons prevented scurvy, and Snow's observations on cholera occurrence in London, showing a disease pattern consistent with water-borne transmission (Rosen, 1993). In these examples, preventive steps followed: After Lind's experiment, the diets of the British navy were supplemented with citrus fruits; and after Snow's observational study, steps were taken to ensure that the source of water was changed in the affected areas of London. Over the ensuing centuries, infectious agents were causally linked to specific diseases, and prevention was accomplished by interrupting transmission and by vaccines. During the twentieth century, public health was threatened by parallel epidemics of chronic diseases, including cancer; and as the causal agents were identified, a broad range of preventive strategies were implemented.
The concept of causation has long had a central role in the application of epidemiologic evidence for controlling cancer. The designation of a risk factor as "causal" has been the starting point for initiating cancer prevention programs based on reducing exposure to the risk factor. Although the concept of causation itself remains a matter of continuing discussion among philosophers and others, use of the term in public health implies that the evidence supporting causality of association has reached a critical threshold of certainty and that reduced exposure can be expected to be followed by reduced disease occurrence. Over the last 50 years, identification of the causes of cancer has been the primary focus of most epidemiologic research on cancer; only recently has attention shifted toward identifying genetic determinants of susceptibility and markers of the early stages of carcinogenesis.
There are numerous examples of how identifying a cause of cancer has led to intervention and reduction of cancer occurrence. Tobacco use and cancer of the lung is a notable example for its historical precedence and for the framework applied to the scientific evidence as the causality of the association was evaluated (US Department of Health Education and Welfare - DHEW, 1964; White, 1990). The range of causal risk factors for cancer is broad, including infectious agents (e.g., human papillomavirus and cervical cancer), physical agents (e.g., ionizing radiation and leukemia), inhaled agents (e.g., radon and lung cancer), pharmaceutical agents and hormones (e.g., diethylstilbestrol and adenocarcinoma of the vagina), food contaminants (e.g., aflatoxin and liver cancer), workplace exposures (e.g., asbestos), life style-related exposures (e.g., alcohol consumption), and genetic mutations (e.g., Li-Fraumeni syndrome). These and other factors considered to be causes of cancer have been given this label only after the accumulation of sufficient evidence, in most instances derived from both epidemiologic and laboratory research.
This chapter provides an overview of causal inference with a focus on the interpretation of epidemiologic data on cancer risk. It begins with an introduction to the centuries-old discussion on cause and causation and next considers the epidemiologic concept of causation, setting the discussion in the context of current understanding of carcinogenesis as a multistep process. The criteria for causation, often attributed to the British medical statistician Sir Austin Bradford Hill (Hill, 1965) or to the 1964 Report of the U.S. Surgeon General on tobacco (US Department of Health Education and Welfare - DHEW, 1964), have provided a framework for evaluating evidence to judge the causality of associations.
These criteria are addressed in depth, and their application is illustrated with the example of smoking, both active and passive, and lung cancer. The chapter concludes with a consideration of emerging issues concerned with causation, including the interpretation of data coming from the new technologies of contemporary "molecular epidemiology" and new approaches to evaluating causation.
CONCEPTS OF CAUSATION.
At its foundation, "cause" is not knowable with certainty. This fact underlies much of the methodologic and conceptual confusion that often swirls around claims of causation based on scientific data. The fundamental intuition underlying the causal concept is that event "A" somehow produces another event, "B." However, the "production" of "B" is not observable. The philosopher Bryan Magee summarized this conundrum eloquently.
It seems to be impossible for us to form any conception of an ordered world at all without the idea of there being causal connections between events. But when we pursue this idea seriously we find that causal connection is not anything we ever actually observe, nor ever can observe. We may say that Event A causes Event B, but when we examine the situation we find that what we actually observed is Event A followed by Event B. There is not some third entity between them, a casual link, which we also observe.... So we have this indispensable notion of cause at the very heart of our conception of the world, and of our understanding of our own experience, which we find ourselves quite unable to validate by observation or experience ... It actually purports to tell us how specific material events are related to each other in the real world, yet it is not derived from, nor can it be validated by, observation of that world. This is deeply mysterious. (Magee, 2001)
The fact that causation is not directly observable means that scientists and philosophers have had to develop a set of constructs and heuristics by which to define a "cause" operationally. These constructs typically have two components: a predictive or associational one, determined empirically, and an explanatory one, based on a proposed underlying mechanism. All causal claims rest on these twin pillars; an association with no plausible mechanistic basis is typically not accepted as causal, and a proposed mechanism, however well founded, cannot be accepted as the basis for a causal claim without empirical demonstration that the effect occurs more often in the presence of the purported cause than in its absence. However, these components need not contribute equally, and various causal claims may rest on quite different balances of contributions of empirical and mechanistic information.
Underlying any operational definition of causality must be an onto-logic one: that is, how a cause is defined in principle. A particularly useful, widely accepted definition in both philosophy and epidemiology is the "counterfactual" notion of causation. This concept had its origins at least as far back as the English philosopher David Hume (1711 - 1776) (Hume, 1739). During the twentieth century, this concept was further developed and applied by statisticians, philosophers, and epidemiologists.
The counterfactual definition holds that something is a cause of a given outcome if, when the same individual is observed with and without a purported cause and without changing any other characteristic of that individual, a different outcome would be observed. For example, the counterfactual state for a smoker is the same individual never having smoked. The state that cannot be observed is called the counterfactual state: literally, counter to the observed facts. The impossibility of observing the counterfactual state is what makes all causal claims subject to uncertainty.
The above definition is deterministic; that is, the outcome always occurs in the presence of the cause and never occurs without it. However, health research rarely deals with either a cause that inevitably produces certain outcomes, or outcomes that cannot occur absent specific causes. For example, smokers do not always get lung cancer, and never-smokers do develop this malignancy. Therefore, the counterfactual definition must be expanded to encompass the notion of a probabilistic outcome. That is, the formal definition of a cause in epidemiology requires that a factor X be associated with a difference in the probability of an outcome. 
For example, if X may take on two different values, y or z: 
Condition 1: observed association Pr(outcome | X = y) ð Pr(outcome | X = z)
Properly designed studies provide a scientific basis for inferring what the outcome of the counterfactual state would be and permit the related uncertainty to be quantified. In a laboratory, scientists are able to predict the outcome in this counterfactual state, generally with a high degree of confidence, by repeating an experimental procedure with every factor tightly controlled, varying only the factor of interest. In observational studies of humans, however, researchers must try to infer what the outcome would be in a counterfactual state by studying another group of persons who, at least on average, are substantively different from the exposed group in only one variable: the exposure under study. The outcome of this second group is used to represent what would have occurred in the original group if it were observed with an exposure different from that which actually existed (Greenland, 1990). In the case of smoking and disease, this comparison is between disease risk in smokers and nonsmokers.
Simply observing a difference in the probability of an outcome between two groups that differ on X is not sufficient condition for causation because it does not distinguish between causation and spurious or indirect association, produced by "confounders," or ancillary causes. The notion of "causation" requires that the cause somehow actively "produce" its effect, which is captured operationally by the requirement that active manipulation of the cause should produce a change in the probability of the outcome. For example, if one saw that students with poor visual acuity typically sat closer to the front of a classroom, one would not call the seating arrangement a "cause" of their poor eyesight unless it could be shown that seating them farther back improved it. The notation that captures this idea is one that introduces an operator, not part of traditional statistical notation "Set (X = x)," which corresponds to actively setting a risk factor X equal to some value x, rather than simply observing that the factor is equal to x. Thus the counterfactual notion of probabilistic causation for a risk factor X requires condition 2.
Condition 2: no confounding Pr(outcome | set[X = x)] = Pr(outcome | X = x)
If we put together condition 1 - that there is an observed association between cause and effect - with condition 2 - that there is no other indirect cause responsible for the observed effect - we have the counterfactual condition for probabilistic causation, expressed as follows.
Condition 1 + Condition 2 = Causality condition Pr[outcome | set(X = y)] ð Pr[outcome | set(X = z)]
This condition states that if the probability of an outcome changes when risk factor X is actively changed from z to y, then X is regarded as a cause of the outcome.
In the randomized controlled trial, a risk factor is actively manipulated. Understanding the role of randomization can deepen insights into the interpretation of nonrandomized designs used in epidemiology. Randomization has two critical consequences: (1) it makes exposure to a proposed causal factor independent of potentially confounding factors; and (2) it provides a known probability distribution for the potential outcomes in each group under a given mathematical hypothesis (i.e., the null) (Greenland, 1990). Randomization does not necessarily free the inference from an individual randomized study from unmeasured confounding (it does so only on average). Randomization does imply that measures of uncertainty about causal estimates from randomized studies have an experimental foundation. In the absence of randomization, uncertainty about causal effects depends in part on the confidence that all substantive confounding has been eliminated or controlled by either the study design or the analysis. The level of confidence is ultimately based on scientific judgment and consequently is subject to uncertainty and questioning.
One way to increase that confidence is to repeat the study. Similar results in a series of randomized studies make it increasingly unlikely that unmeasured confounding is accounting for the findings, as the process of randomization makes the mathematical probability of such confounding progressively smaller as the sample size or number of studies increase. In observational studies, however, increasing the number of studies may reduce the random component of uncertainty, but not necessarily the systematic component attributable to confounding. Without randomization, there is no mathematical basis for assuming that an imbalance of unknown confounders decreases with an increase in the number of studies. However, if observational studies are repeated in different settings with different persons, different eligibility criteria, and/or different exposure opportunities, each of which might eliminate another source of confounding from consideration, the confidence that unmeasured confounders are not producing the findings is increased.

</file>

<file= AmE06_J15>

The rate at which individuals are capable of processing information is considered a
sensitive indicator of brain dysfunction, particularly among clinical populations
whose neurocognitive impairments generally are considered relatively mild in
nature. Assessment of information processing speed often is confounded, however,
by comorbid impairments in other constructs of neurocognition, including
attentional capacity, visuospatial perception, language, immediate memory, and
motor speed/coordination. This investigation examined the effect of controlling
for various potential confounders on the strength of associations among several potential measures of information processing speed. Participants were 64 patients
diagnosed with clinically definite multiple sclerosis. Analysis indicated consistent
significant and positive associations among measures of processing speed, which
generally persisted despite simultaneous statistical control of potential confounding
factors. Results imply that examined confounding variables are similarly related to
the measures of processing speed. Therefore, any of the measures of information
processing speed considered in this study may be used as a proxy for the more direct
measure of this construct derived from the Sternberg Memory Scanning Test. 
The rate at which individuals are capable of processing information is
considered a sensitive indicator of brain dysfunction. Impairments on tasks that
require speeded performance have been observed repeatedly in persons with
otherwise mild cognitive deficits, including patients with multiple sclerosis
(MS, Rao et al., 1991), systemic lupus erythematosus (Shucard et al., 2004),
chronic fatigue syndrome (DeLuca et al., 2004), diseases of the basal ganglia
(Cummings, 1990), and mild to moderate traumatic brain injury (Levin, 1993).
Processing speed also declines early in the course of Alzheimer's disease
(Beatty et al., 2003) as well as in normal aging (Salthouse, 1991).
Assessment of processing speed is complicated because test scores on
any measure of speeded performance may be influenced, or confounded, by
the patient's capacity for attention, visuospatial perception, language, and
immediate memory as well as by motor speed/coordination and the integrity of
sensory systems.
Perhaps the purest measure of processing speed is the Sternberg Memory
Scanning Test (Sternberg, 1969). On this task, subjects first memorize a set
of 1, 2, or 4 digits. They are shown single digits on a computer screen and
must decide by pressing one of two keys whether the digit matches one of
those held in memory. Plots of reaction time against the size of the memory
set yield two measures: slope, which is taken as the measure of information
processing speed (usually expressed as ms/bit of information), and intercept,
which reflects overall response speed. Factors such as attention, visuospatial
perception, language, and sensory/motor ability either are irrelevant to the
task or exert influences unrelated to memory set size. These factors would be
expected to affect the size of intercept, but not the slope. Impairment in memory
might affect the slope, but if this was a serious problem for a particular patient,
an increase in the proportion of errors (normally nearly 0)would be expected. As
a practical assessment matter, one probably would not be especially concerned
about obtaining a precise estimate of information processing speed for a patient
who could not remember four single-digit numbers following rehearsal.
Rao et al. (1989) tested patients with MS and controls on the Sternberg
Memory Scanning Test. The patients and controls differed in mean slope (100
ms/digit vs. 68 ms/digit) and mean intercept (741 ms vs. 541 ms), but error
rates were low (1.9% vs. 0.9%) for both groups and not significantly different.
Thus, based on their steeper slopes, the patients exhibited slowed information
processing of central origin that could not be attributed to sensory or motor
deficits or to difficulties in attention or immediate memory. They also showed
overall slowing based on their higher values for the intercept.
As a relatively uncontaminated measure of processing speed, the Sternberg
Memory Scanning Test has many virtues, but the length of the task
(approximately 30 min) probably is too long for most clinical settings. A
number of shorter alternatives have been used to infer patients' abilities to
process information rapidly based on the timing element involved in their
completion. The present study examined the effect of controlling for various
possible confounders on the strength of associations among several potential
measures of information processing speed that are sufficiently brief and
therefore widely used in clinical practice. If the patterns of association among
the various measures were similar and statistically significant without and with
adjustment for the confounding variables, the implication would be that the
confounding variables have minimal to modest influences on the measures of
processing speed or, at least, that their influences are similar for all of the
processing speed measures.
METHOD
Participants
Participants were 64 patients who met inclusionary/exclusionary criteria
and volunteered to participate in a prospective investigation examining the
sensitivity of three screening measures to the cognitive impairments associated
with MS (Aupperle et al., 2002). Patients were recruited from the MS Clinic at
the Oklahoma City Department of Veterans Affairs Medical Center (DVAMC)
and from the practices of collaborating area neurologists. The sample consisted
of 27 males and 37 females. Four participants were African American,
1 participant was Hispanic, and 59 participants were Caucasian. Twenty (31%)
of the patients were employed full time; the remaining either were retired or
unemployed. All of the accepted volunteers met criteria for clinically definite
MS (Poser et al., 1983), were residing at home, and had adequate vision (e.g.,
ability to read newspapers, magazines, and the consent form without specialized aids) and hand function (e.g., legible signature of normal size on the consent
form) to complete all measures. Thirty-three patients had been diagnosed
with a relapsing-remitting disease course, and 31 had been diagnosed with a
progressive disease course (5 primary progressive, 26 secondary progressive).
Each potential participant initially was sent a letter describing the study
and subsequently was contacted by telephone to determine eligibility and
willingness to participate. Patients with any of the following conditions were
excluded from participation: history of head injury with loss of consciousness
for more than one hour or documented chronic cognitive impairment, history
of alcohol or drug abuse treatment, history of learning disability, myocardial
infarction during the prior twelve-month period, uncontrolled hypertension or
metabolic disease (e.g., diabetes, thyroid, or liver disease), chronic lung or
kidney disease, history of any central nervous system disease other than MS,
severe psychiatric disease (e.g., Schizophrenia or Bipolar Disorder), Major
Depressive Disorder or any anxiety disorder that clearly antedated the diagnosis
of MS. Based on these criteria, 8 volunteers were excluded (4 for poor vision,
1 for a learning disability, 1 who could not hold a pencil, 1 for a severe traumatic
brain injury, and 1 who was confined to bed). An additional 12 patients
(8%) who met the inclusionary/exclusionary criteria refused to participate.
Patients continued to take medications at optimal doses as prescribed by their
attending physicians (see Aupperle et al., 2002 for specific details). All other
patients provided written informed consent; the protocol was approved by the
University of Oklahoma Health Sciences Center Institutional Review Board
and the Research Committee of the Oklahoma City DVAMC.
Procedure
All testing of participants occurred at least one month following recovery
from an exacerbation of MS symptomatology. Measures were administered
and scored according to standardized procedures in the following order:
(1) Ambulation Index (AI; Hauser et al., 1983), (2) Mini-Mental State Exam
(Folstein et al., 1975), (3) Fatigue Severity Scale (FSS; Krupp et al., 1989), (4)
Chicago Multiscale Depression Inventory (CMDI; Nyenhuis et al., 1995), (5)
State-TraitAnxiety Inventory (STAI; Spielberger, 1983), (6) RepeatableBattery
for the Assessment of Neuropsychological Status (RBANS; Randolph, 1998),
Screening Examination for Cognitive Impairment (SEFCI; Beatty et al., 1995),
and Neuropsychological Screening Battery for Multiple Sclerosis (NPSBMS;
Rao et al., 1985; Rao et al., 1991) and, (7)Wisconsin Card Sorting Test (Heaton
et al., 1993). The order of administration for the three neuropsychological screening batteries listed in number six was controlled by between-subjects
counterbalancing. Additionally, a brief rest period was imposed between these
three batteries in order to minimize potential interference effects.
For the purposes of the present investigation, the following timed tests
from the neuropsychological batteries were selected as measures of information
processing speed: the Controlled OralWord Association Test with the letters F,
A, and S from the NPSBMS, the Coding and Semantic Fluency subtests from
the RBANS, the oral version of the Symbol DigitModalities Test (Smith, 1982)
from the SEFCI, and the Paced Auditory Serial Addition Test (PASAT) with
a 3-interstimulus interval (ISI) from the NPSBMS. The 3-second ISI version
of the PASAT was used because 11 patients were unable to complete the more
difficult 2-second ISI version. Control factors were the Digit Span Test from
the RBANS for auditory attention, the Figure Copy test from the RBANS for
visuospatial function, the Picture Naming test from the RBANS and the Shipley
Vocabulary Test (SILS-V; Zachary, 1986) from the SEFCI for language, the
ImmediateMemory Index from the RBANS for immediate memory, the AI for
motor ability, and the FSS for fatigue.
The number of patientswho refused to attempt various testswas as follows:
1-Coding and 3-Figure Copy from the RBANS; 1-oral version of the Symbol
Digit Modalities Test from the SEFCI; 3-PASAT from the NPSBMS. The
patient who refused to attempt the Coding subtest and the SDMT suffered from
ophthalmoplegia. Reasons for the other refusals are unknown. These patients
were excluded from statistical analyses, which were performed on the data of
the remaining 60 subjects who completed all measures.
RESULTS
Correlational analysis revealed no significant associations between the use
of any class of medications and performance on the cognitive tests. Likewise,
cognitive performance was not correlated with scores on any of the scales of the
CMDI or the STAI (see Aupperle et al., 2002). Table 1 shows the correlations
among the five measures of processing speed. Zero-order correlations are
shown in standard font and partial correlations in bold font. The partial
correlations were computed controlling for auditory attention, visuospatial
function, naming, vocabulary, immediate memory, motor ability, and fatigue
simultaneously. All of the zero-order correlations were statistically significant;
those between Coding or SDMT and other variables were at least moderate in
size (r ≥ .40). Associations between the verbal fluency and PASAT measures
were small in magnitude. After adjustment for the seven control factors, all but one of the correlations remained significant, although the strength of all
associations was reduced.
DISCUSSION
The present findings reveal consistent associations among probable measures
of processing speed, which generally persist despite simultaneous statistical
control of seven potentially confounding factors. The results imply, at
minimum, that the confounding variables are related to the measures of
processing speed in a similar way. Hence, any of the measures presumably
can be used as a proxy for the more direct measure derived from the Sternberg
Memory Scanning Test.
Of these, the oral version of the SDMT, which takes only about twominutes
to administer (with instructions), appears to be the test of choice for examination
of information processing speed. The SDMT has adequate norms adjusted for
various demographic characteristics, including age and gender (Richardson &
Marottoli, 1996; Uchiyama et al., 1994; Yeudall et al., 1986) and can be scored
easily with a template. The stimuli are large, so visual acuity rarely is a problem,
although patients with eye movement disorders may not produce valid scores.
Elsewhere, Beatty (2002) found that the SDMT, the FAS, and semantic
fluency are moderately correlated, aswas found in the present study, and each of
the tests has similar sensitivity and specificity for discriminating patients with
MS from demographic controls. Fluency tests are brief and adequate norms are
available (see Spreen & Strauss, 1998).
Performance on the coding test from the RBANS was highly correlated
with performance on the SDMT, which was expected because the tasks are the
same although the symbol keys and the response mode (oral vs. written) differ.
Because these tests are so highly correlated, they likely provide very similar
information about processing speed. Hence, the examiner probably can safely
select the test that uses the patient's least impaired response mode. Most often
this will be the oral administration of the SDMT.
Finally, the PASAT is longer than any of the other tests examined in
this investigation and often requires repetition of the instructions.

</file>

<file= AmE06_J16>

The vaccinia virus A35R gene is highly conserved among poxviruses and encodes a previously uncharacterized hydrophobic acidic protein. Western blotting with anti-A35R peptide antibodies indicated that the protein is expressed early in infection and resolved as a single sharp band of 23 kDa, slightly higher than the 20 kDa predicted from its sequence. The protein band appeared to be the same molecular weight on sodium dodecyl sulfate-polyacrylamide gel electrophoresis, whether expressed in an in vitro transcription/translation system without microsomes or expressed in infected cells, suggesting that it was not glycosylated. A mutant virus with the A35R gene deleted (vA35Δ) formed wild-type-sized plaques on all cell lines tested (human, monkey, mouse, and rabbit); thus, A35R is not required for replication and does not appear to be a host range gene. Although the A35R protein is hydrophobic, it is unlikely to be an integral membrane protein, as it partitioned to the aqueous phase during TX-114 partitioning. The protein could not be detected in virus-infected cell supernatants. 
A35R localized intracellularly to the virus factories, where the first stages of morphogenesis occur. The vA35Δ mutant formed near-normal levels of the various morphogenic stages of infectious virus particles and supported normal acid-induced fusion of virus-infected cells. Despite normal growth and morphogenesis in vitro, the vA35Δ mutant virus was attenuated in intranasal challenge of mice compared to wild-type and A35R rescue virus. Thus, the intracellular A35R protein plays a role in virulence. The A35R has little homology to any protein outside of poxviruses, suggesting a novel virulence mechanism.
Poxviruses are large double-stranded DNA viruses with genomes
that range from 130 to 379 kbp. Poxviruses have
worldwide distribution and infect a wide variety of animals,
including insects, birds, and mammals. While smallpox
was eradicated from nature, variola virus continues to present
a bioterrorism concern, and several other poxviruses infect
humans, causing morbidity and mortality: molluscum contagiosum
virus (MCV), monkeypox virus, Tanapox virus, Yaba-like
disease virus, cowpox virus, and Cantagalo virus (evolved from
a vaccinia virus [VV] vaccine strain). The prevalence of poxviruses in animals and humans and
their propensity for recombination and gene acquisition suggest
that it would be unwise to discount them as important
human pathogens. This is especially true, since most emerging
infectious diseases are zoonoses, crossing from animals to humans,
and poxviruses are known to acquire mutations and
become highly pathogenic in a new animal species.
The complex poxvirus replication cycle occurs in the cytoplasmic
areas of dense viroplasm called virus factories. Gene
expression is temporally regulated, with early, intermediate,
and late gene expression. Early genes include those encoding
intermediate transcription factors, components of the DNA
replication machinery, inhibitors of cellular apoptosis and proteins
that interfere with immune clearance of virus-infected
cells. Among the last group, there are inhibitors of complement;
alpha interferon (IFN-a), IFN-b, and IFN-y; major histocompatility
complex class I expression; antigen presentation;
interleukin 1-converting enzyme; Fas-induced killing; chemokines;
and interleukins. While a great many viral
pathogenic strategies have been described, my colleagues and
I have documented that of the 90 most widely conserved poxvirus
genes, 25 remain uncharacterized. This fact underscores
the large amount that is still unknown about poxviruses
and highlights the need for further study of these viruses and
genes.
Most poxvirus research has focused on VV, the prototypical
poxvirus that was used as the live smallpox vaccine. Poxvirus
morphogenesis is complex with multiple viral forms. The
spherical immature virus particle matures into the intracellular
mature virus (IMV) concomitant with proteolytic cleavage.
The IMV is infectious, but to escape the cell during infection,
IMV must undergo a second membrane-wrapping event, during
which the IMV acquires two additional membrane layers
derived from the trans-Golgi network or early endosomes,
forming the intracellular enveloped virus (IEV). The IEV fuses out, losing its outermost membrane and releasing the extracellular enveloped virus (EEV). Seven proteins have been identified that are
specifically incorporated into the IEV and/or EEV membranes:
an 89-kDa hemagglutinin (encoded by the VV A56R
gene), a 42 kDa protein (B5R), a 43- to
50-kDa protein (A36R), the 65-kDa (F12L) protein, the acylated 37-kDa (F13L) protein (3, 24), and two variably glycosylated 21- to 24-kDa proteins encoded by A33R and
A34R. The A33R, A34R, and A36R envelope
genes are contained within a small contiguous region of the
genome interrupted only by the A35R open reading frame
(ORF). The location of the A35R and its hydrophobic nature
suggested that it might also be an envelope-associated protein.
This report describes the first characterization of the A35R
protein and its role in virulence.
MATERIALS AND METHODS
Cells. HeLa cells were used for growth of virus stocks as previously described
(12). BS-C-1 cells were routinely used for plaque assays, immunostaining, immunoprecipitations,
and Western blotting. For titrations of VV and analysis of
plaque size, monolayers were fixed and stained with 0.1% crystal violet in 20%
ethanol.
RK13 cells were used to propagate virus for CsCl purification of virus particles.
Cells were grown in Eagle's minimum essential medium with 10% fetal bovine
serum, and infections were carried out in Eagle's minimum essential medium
with 2.5% fetal bovine serum. Cell lines used for host range studies are listed
with the American Type Culture Collection number in parenthesis: RK-13,
normal rabbit kidney (CCL-37); CV-1, normal African Green monkey kidney
(CCL-70); HeLa, human cervical adenocarcinoma (CCL-2); human, thymidine
kinase-negative (TK-) (CRL-8303); African green monkey normal kidney, BSC-
1 (CCL-26); human lung carcinoma, A549 (CCL-185); normal human lung
fibroblast, MRC-5 (CCL-171); and hamster kidney, BHK-21 (CRL 8544).
Antibodies. Polyclonal rabbit sera were raised to three A35R peptides (supplied
by J. Coligan, National Institute of Allergy and Infectious Diseases). Sequences
were derived from hydrophilic regions of the predicted amino acid
sequence of A35R. The peptide A35R-55 sequence was CQKCYFSYKGKIVP
QDSND (residues 55 to 72), the A35R-102 peptide sequence was CDIEDKH
QPFY (residues 102 to 111), and A35R-85 was YRSKNTIIIACDYDC (85 to
98). Cysteine residues (underlined) were included to aid in peptide coupling.
Peptides were coupled to keyhole limpet hemocyanin using the Imject Activated
Immunogen Conjugation kit (Pierce, Rockford, IL). A total of five injections
were given, and the rabbits were bled to prepare antiserum.
Cloning and in vitro expression of A35R. The Sal L opp plasmid (a gift from
Nelson Cole) was cut with NcoI and SpeI restriction enzymes to release a DNA
segment containing the A35R gene, from 1 nucleotide before the ATG translation
start site to 37 nucleotides after the stop codon. The A35R segment and
NcoI- and SpeI-cut pGem5Zf (Promega) and pTM1 plasmids were gel
purified using the Geneclean II kit, ligated, and transformed into Escherichia
coli. Ampicillin-resistant colonies were picked and screened for inserts. In vitro
expression was carried out using the TNT reticulolysate system (Promega) in the
absence of microsomal membranes.
vA35RΔ mutant virus construction. DNA segments containing the A35R
flanking regions were joined with the E. coli xanthine guanine phosphoribosyl
transferase (gpt) gene under the p7.5 promoter by recombinant PCR.
The flanking sequence that included part of the adjacent A34R gene was amplified
by using the primer pair CCAGATTGCCTAGACCGGATACTAG and
GCGGGTGGGTTTGGAATTAGTGTGGCGGCGTACGTTAACGACTTA.
Sequences of the gpt gene (underlined) were incorporated into the primers for
sequential amplification and joining of the gpt and A35R flanking regions. The
flanking sequence that included part of the adjacent A36R gene was amplified
using the primer pair CCTGGCACTGCCGGGCGTTCATAAAAGTTGTAA
AGTAAATAATAAAAC and TCATTCCTAGAAATATTATCTACG. Primers
used to amplify the gpt cassette were CACTAATTCCAAACCCACCCGCTT
and AACGCCCGGCAGTGCCAGGCGT. The three DNA segments were amplified
by PCR individually, purified by using Promega PCR Preps, and then
joined by recombinant PCR in a stepwise fashion. The final PCR product was
purified, TA cloned (Invitrogen), and sequenced using a Prism Dye Deoxy
Terminator Cycle Sequencing kit (Applied Biosystems, Foster City, Calif.) in
conjunction with a model 373 DNA sequencer (Applied Biosystems). The TA
plasmid containing the A35A construct was transfected with N-[1-(2,3-dioleoyloxy)
propyl]-N,N,N-trimethylammonium methyl-sulfate (DOTAP; Roche) into
cells infected with the wild-type VV strain Western Reserve (WR). Viruses were
grown under semisolid agarose (GIBCO/BRL, Grand Island, N.Y.) containing
mycophenolic acid (Sigma) for selection of recombinant virus. Viruses were
plaque purified three times and amplified. Recombinant viruses were PCR
screened for the presence of the gpt gene and the absence of the A35R gene
using primers flanking the A35R locus. Protein expression was analyzed by
Western blotting.
vA35-Res virus construction. The A35R gene and flanking sequences were
PCR amplified using primers CCAGATTGCCTAGACCGGATACTAG and
TCATTCCTAGAAATATTATCTACG. The PCR product was cloned into the
TA cloning vector (Invitrogen). Plasmids were sequenced to confirm the correct
sequence and transfected into vA35A-infected cells. vA35-Res recombinant viruses
were selected on STO cells in the presence of 6-thioguanine to select
against viruses containing the gpt gene. Expression of A35R protein was
confirmed by Western blotting.
Immunoprecipitation and Western blotting. Immunoprecipitation and Western
blots were performed essentially as described previously (46). Rabbit anti-
A35R sera were used at a 1:1,000 dilution in Western blots.
Triton X-114 partitioning of integral membrane proteins. BS-C-1 cells were
infected at a multiplicity of infection (MOI) of 10 for 24 h. The medium was
removed, and cells were extracted in cold buffer containing 10 mM Tris-HCl (pH
7.4), 150 mM NaCl, and 2% Triton X-114 (Pierce, Rockford, IL). Following a
15-min incubation, lysates were centrifuged for 7 min at 14,000 X g to remove
insoluble matter. Supernatants were wmed to 37°C, underlaid twice with 6%
sucrose, and centrifuged for 10 min at 1,000 X g to separate the upper aqueous
phase from the lower detergent phase. Samples were analyzed by immunoprecipitation
and sodium dodecyl sulfate-polyacrylamide gel electrophoresis.
Immunofluorescence microscopy. Fluorescence microscopy was performed as
previously described. Infected HeLa cell monolayers on coverslips were
washed with phosphate-buffered saline (PBS), fixed in 4% paraformaldehyde,
and permeabilized with 0.1% Triton X-100. Rabbit anti-A35R sera were used at
a 1:1,000 dilution. The secondary antibody was Alexa 568-labeled goat anti-rabbit
immunoglobulin G (Molecular Probes, Inc.) diluted 1:400, and DNA was stained
by 500-ng/ml Hoechst 33258 (Sigma). Samples were viewed and images were
collected with a Zeiss IIIRS immunofluorescent microscope.
One-step growth curve. Confluent BS-C-1 cell monolayers in six-well plates
were infected with VV at an MOI of 10 for 2 h. The inocula were removed, the
cells were washed, and fresh medium was added. At intervals, the medium from
an individual well was harvested and centrifuged at 1,800 X g for 5 min to pellet
detached cells. The resulting cell pellet was combined with infected cells that had
been scraped from the plate into 1 ml of fresh medium. Cells were frozen, thawed
three times, and sonicated. Viruses from the media and cells were individually
titrated in duplicate on BS-C-1 cell monolayers.
Virus purification. Virus particles were separated and purified based on differential
buoyant density centrifugation in CsCl, as described previously.
Briefly, RK-13 cells were infected at a multiplicity of infection of 10 for 48 h. For
EEV, supernatants were harvested and centrifuged twice at 3,000 rpm in a
Sorvall RT 6000B to remove cells. Then, supernatants were centrifuged at 22,000
rpm in a Beckman L8-70 M for 80 min to pellet virus. For IMV, cells were
scraped into 10 mM Tris (pH 9.0) and Dounce homogenized. After a low-speed
centrifugation, virus was pelleted through a 36% sucrose cushion. Afterwards,
both the EEV and IMV were purified on a CsCl gradient (layered densities of
1.3, 1.25, and 1.2 g/ml) and centrifuged at 32,000 rpm with slow acceleration for
60 min in a Beckman L8-70 M.
Syncytium formation. Confluent BS-C-1 cell monolayers were infected at an
MOI of 10 for 2 h, washed, and incubated in medium for an additional 10 h as
described previously. Cells were washed and treated with fusion buffer
[phosphate-buffered saline with 10 mM 2-(N-morpholino)ethanesulfonic acid
and 10 mM HEPES] at pH 5.5 or 7.4 for 2 min at 37°C. Afterwards, fusion buffer
was replaced by medium, and the cells were incubated at 37°C and then observed
by phase-contrast microscopy.
Determination of virulence in mice. Groups of six female BALB/c 6-week-old
mice were anesthetized and intranasally challenged with 104 to 106 PFU of
vA35A, WR, vA35-Res virus, or PBS in 20 ml 10 mM Tris-HCl (pH 9.0). Virus
titers were determined on the day of challenge to confirm virus dose.

</file>

<file= AmE06_J17>

ABSTRACT. 
This essay explores the various places inhabited by doctors and
patients, in order to lead doctors to a more complex understanding of their patients'
experiences of illness. Using Adam Haslett's "The Good Doctor" (2002), we examine
what happens when doctors enter the worlds of their patients, both the literal landscapes
of their patients' homes and the hidden landscapes of their minds.We illustrate
the impact place has on doctors' understanding of their patients and on the patients'
attitudes toward their illness. In addition, we examine how place informs readers' perceptions
of both the coherence and the divide between the worlds of doctor and
patient.
"THE HYPHEN," Ron Carson (2002) argues, "is a key to understanding the
relationship between patients and doctors. . . . It calls attention to the
distance between parties to the clinical encounter. And then, in the blink of an
eye, it is a bridge across the divide" (p. 171). This hyphen, he continues, represents
a liminal place between doctor and patient whereby "one is suspended,
straddling, or wavering between two worlds, neither here nor there, betwixt and
between settled states of the self " (p. 175).
This essay explores the various places inhabited by doctors and patients, an
exploration that might lead doctors to a richer, more complex understanding of
their patients' experiences of illness. Such inquiry involves the acquisition of narrative
knowledge, which one gains through written or spoken accounts of
patients' stories, some fictional, some not. Whatever its form, narrative knowledge
is always context-embedded and particular,
concerned with examining and understanding singular events within the specific
time and place of a unique life story. In order to find coherence in sequences of
events and to form meaningful wholes, physicians need to use not only the logicoscientific
knowledge that enables them to gather and evaluate generalizable,
replicable, and empirically verifiable data, but also narrative knowledge that
enables them to use the epistemology and interpretive abilities of a good reader.
(Montello 1997, p. 190)
Through our examination of place we achieve a better understanding of how it
figures into the relationship between doctors and patients - how one's location
colors the ways doctors and patients view each other, create expectations for
each other, place trust in each other, and form an ability to work with each other.
We show how place is more than the obvious physical or social dimensions of
one's life - how it is more than the "psychosocial" part of the biopsychosocial
model, the part which is, in crowded hospitals and other clinical settings,"not so
much opposed as treated with indifference or institutional cynicism" (Morris
1998, p. 73).As we develop it here, place is a kind of "geographical epistemology
which is founded on personal geographies composed of direct experiences,
memory, fantasy, present circumstances, and future purposes" (Relph 1976, p. 4).
When we speak of place, we are concerned with the full array of experiences
through which we come to both know places and to make places. Place is an essential
part of our existence that tells us, if only provisionally, who we are, where
we've been, and where we might be going. It is worthy of study and reflection,
particularly by doctors committed to understanding and respecting their
patients' lived experiences.
Using Adam Haslett's "The Good Doctor" (2002) as an organizer for our discussion,
we examine the specific place of life stories when doctors enter the
worlds of their patients, both the literal landscapes of the patients' homes and the
hidden landscapes of their minds.We illustrate the impact place has on doctors'
understanding of their patients and on the patients' attitudes toward their illness.
In addition, we show how place informs readers' perceptions of both the coherence
and divide between the life worlds of doctor and patient. Place contextualizes
a patient's story, inserting the reader or listener into that patient's experience
of illness, imparting a sense of what it is like to be that person.
To explore the relationship between medicine and place, we borrow heavily
from humanistic geography, a domain rarely used in medical discourse and one
that emphasizes the human construction and experience of place (Gregory and Walford 1989).To begin with, humanistic geographers (among other disciplines)
make a clear distinction between space and place. Space is an arena with no meaning
other than a mathematical one (Houston 1978); indeed, space precedes or
becomes place (Ley and Samuels 1978). Place, by contrast, has context,
historical associations where vows are made; encounters and obligations, met;
commitments, fulfilled; limits, recognized. Place implies belonging. It establishes
identity. It defines vocation. It envisions destiny. Place is filled with memories
of life that provide roots and drive direction.There is a verticality as well as a
horizontality to place that space does not have, for the former embodies the
verticality of human values and human needs. (Houston 1978, p. 226)
Place so conceived is not merely the physical backdrop of one's existence but is
created, elastic, and relational, mediated by culture (Ward 2003); it has the power
to "direct and stabilize us, to memorialize and identify us, to tell us who and
what we are in terms of where we are" (Casey 1993, p. xv). Moreover, place is
process, never static: "People constitute places even as they move within them;
people and their bodies are not self-contained but rather a merging occurs, place
flows" (Ward 2003, p. 85).
At first glance, readers may report that "The Good Doctor" is a story about a
young physician's home visit to a depressed woman in an isolated area of the
Midwest, having noted the descriptive "prairie" and an offhand reference to
"back East." Place, then, is circulating in the first page or two of the story, even
before Frank meets his patient, Mrs. Buckholdt. First, place refers to the literal
landscape, the "whereness" of the patient, the space she occupies, which has been
described as barren and run down, matching her depression. She is inside this
place; it is home and has been the site of both extraordinary and everyday events.
Second, we use place to refer to both Frank's and Mrs. Buckholdt's identities,
which include a fusion of personal history, actions, the meanings associated with
those actions, and the context in which they occur - in short, their lived experiences.
Thus, time is very much a part of place, which, in some cases, keeps the
idea of place fluid even as structures and natural features remain static. Frank's
place - his professional sphere - is rarely enacted in locations like Mrs. Buckholdt's
environment, but rather in hospitals and other clinical settings where he
and people like him are in charge because of their knowledge, skills, and social
position.There, he is inside his place; here, he is outside, literally and figuratively.
Mrs. Buckholdt's place is yet to be determined from the little information we
have at this time, but we assume that it is tied to this location.
Third, we use place to refer to the interaction that will take place between
Frank and Mrs. Buckholdt soon after Frank enters her house. Here we examine
place as mind, which includes all the values, beliefs, personal histories, and ways
of thinking and behaving they both bring to those moments, as well as how they
are played out during and after the characters' interaction. That is to say, "place
is not just the 'where' of something; it is the location plus everything that occupies that location seen as an integrated and meaningful phenomenon" (Relph
1976, p. 3).
Thus, the intense interaction between Frank and Mrs. Buckholdt, filled with
words, shifts, gestures, glances, and silences, each brimming with meaning, is tied
literally and figuratively to the ground on which they both sit. In the following
sections we examine this ground of meaning-making across the above three
dimensions as we watch and listen to Frank and Mrs. Buckholdt: place as landscape,
place as identity, and place as mind.
Place as Landscape
The character of a subject's own self-identity and self-conception, are tied to the places in which
the subject finds itself.
 - J. E. Malpas (1999)
We first meet Frank, the young doctor in "The Good Doctor," as he pulls into
the Buckholdts' driveway. The first thing he sees is a rusty, "battle-wasted tank"
of a Chevy Nova buried in grass as tall as its windows, and as he scans the rest of
the environment, the collective images are even more depressing: a brown yard
with plastic toys strewn everywhere, a sagging house, a dilapidated barn that has
been childishly spray painted "No Girls Allowed."We quickly sense that Frank is
not happy to be there. He is nursing a hangover after receiving word the day
before that his National Health Services Corps funding, which supposedly would
repay his medical school loans for three years' work in an underserved area, has
run dry. He has driven two and half hours to evaluate a patient, Mrs. Buckholdt,
who has refused to return to the clinic where he works but still wants refills on
her medication.The clouds of dust stirred up by his car mirror his mood.
To this point, Frank's impressions are tied to landscape, to the particularities
of this place. Indeed, place is often conceived as a physical, visual form that includes
human-made structures and natural features. Place is "substantial, capable
of being described. . . . The spirit of a place lies in its landscape" (Relph 1976, p.
30).The spirit of this place is tied to the beholder, for
the process of viewing a landscape is . . . one of careful construction, through
which the indifferent or unaccommodating space of a site or environment is
transformed into a place, which draws the viewer into its territory. Crucial to
this task of conversion is the viewer's location . . . from which to gain access
to the landscape. On encountering a particular site, the viewer must find her
bearings in relation to it. . . .The contemplation of landscape teaches us that
what we see is always and inevitably a question of how we see and from
where we see. (Whitehead 2003, p. 275)
"The Good Doctor" is written with a limited omniscient point of view; that is,
we know what Frank, but no one else, is thinking about the landscape he encounters. As he sees it, the Chevy is a skeleton; the 100-degree heat is oppressive;
the dirt is powder dry.The first signs of life - an eight- or nine-year-old girl
standing in the driveway, and a middle-aged man who answers the door - are
similarly flat and dismal: the girl's mouth is rigid, her eyes are narrowed; the man
holds a cigarette between thumb and forefinger as he scrutinizes Frank, his skin
blotched with spidery angiomas ("Hepatitis C, Frank thinks, or the end of a serious
drinking habit").To Frank's announcement that he is here to see Mrs. Buckholdt,
the man's only response is "Yeah . . . she's in there," spoken as he crosses
the porch and wanders into the yard. Left alone on the porch, Frank calls out
"Mrs. Buckholdt?" through the door the man has left open.
Other than weighty impressions gleaned through the surroundings of her
home and perusal of the girl and the man who must surely be part of her family,
what does Frank know about Mrs. Buckholdt? The psych notes tell him that
she is a 44-year-old woman with no family history of mental illness. She first
presented with depression after her oldest child died four years ago; she has two
younger children, a girl and a boy.At this point, these notes are the official story
of her illness; she is merely an entry here (Frank 1995). Her case, as Frank reads
it, has been miserably handled. He thought to himself, "A brief course of antidepressants,
probably never finished, and since then nothing but benzos - sedatives - 
written as needed. No therapy. . . . the shrink [I] had replaced wasn't about
to drive five hours round-trip for a meds consult, so he just kept calling in her
refills" (Haslett 2002, p. 29).
At this point, readers are very much "in" the setting: we can see it, smell it,
and hear it, pretty much as Frank does.

</file>

<file= AmE06_J18>

CHAPTER 1.
Matrix Algebra for Linear Models.
NOTATION.
Graybill (1969), Searle (1982), Harville (1996), and Schott (2005) provided
thorough introductions to matrix algebra for statistics. We summarize only key
results here. Substantial omissions include the deletion of nearly all proofs, as well
as consideration of more general forms not commonly used in statistics. Also,
many issues of numerical accuracy have been ignored. Some of the formulas
described here, although very useful for understanding concepts, prove
numerically unstable with typical computer precision.
Braces, { }, indicate sets and brackets, [ 1, indicate matrices or vectors (arrays).
In a distinct use of the same symbols, mathematical expressions will be grouped by
using the nesting sequence {[()I}, which may be iterated as { [( {[()I} )I}.
Definition 1.1 (a). A matrix is a rectangular, two-dimensional array of
elements. Writing A = {atJ} says A is the matrix with element azJ at row i
and column j. Here i is the row index, while j is the column index, which
are always written in row-column order. (b) A vector is any matrix with exactly one column, such as = [:I.(c). A scalar, such as s = 6, can be expressed as a vector with one row or as
a matrix with one row and one column, written s = s = S.
We restrict attention to real numbers and finite dimensions. With T rows, c
columns, A is T x c (T by c).
In turn, (A)t=J indicates element i ,j has been extracted from A.
Although not all authors do so, we are scrupulous about the distinction between
a matrix of one row.
The vector b can also be written in terms of a transpose (defined in the next section), b = A'.
Doing so not only avoids notational ambiguity, but also builds in many consistency
checks by requiring dimensions and symbol types (a or A) to align.
As an aid to working with matrices, we always use bold typeface in word
processing software, as in the present book. Most, but not all, statistical journals
require the convention. With the convention, a represents a scalar, a represents a
vector, and A represents a matrix. When handwriting expressions, we highly
recommend always putting a tilde or dash under any matrix or vector to indicate
boldface. It is extreme2y helpful to write the dimensions of each matrix in an
equation underneath the equation.
The practice saves a great deal of time (otherwise spent being confused). More
bluntly, if one does not know the dimensions, one cannot understand the equation.
We reserve superscripts for operators and use subscripts for descriptors, such as
in 5, x2, 5 2 . Often, functional notation, such as x ( c , a ) , provides a better
alternative than a long and elaborate subscript descriptor.
SOME OPERATORS AND SPECIAL TYPES OF MATRICES.
As mentioned earlier, we use the term "vector" only for n x 1 arrays and never
for 1 x n arrays. A 1 x n array will always be written as a matrix, such as
A = [ 1 2 3 1, or as a transposed vector, b' = [ 1 2 3 1. Here A = b' and A' = b.
Definition 1.6 An identity matrix, I or I,,,is a square matrix with all 1's on
the main diagonal, and all 0's off-diagonal. Equivalently, utj = 0 if i # j
and a,, = 1 if i = j .
Definition 1.7 A zero matrix, 0, has q7 = 0 and may be written
0, to indicate a vector or Orxc to indicate a matrix for clarity.
Definition 1.9 A partitioned matrix (supermatrix) has elements grouped
meaningfully by combinations of vertical and horizontal slicing, indicated
A = {Alk }. Necessarily A3k and Ap have the same number of rows, while
Ajk and Ajk Ih, ave the same number of columns.
Definition 1.10 A block diagonal matrix is a partitioned matrix with all
partitions zero except possibly {Ajj}.
However, complete uniformity of dimensions is not
required (the number of rows of B need not equal the number of rows of E). It
would be hard to overemphasize the value of partitioned matrices in deriving
algebraic and statistical properties for linear models. Expressions can often be
greatly simplified by taking advantage of special properties of partitioned matrices
for basic operations (matrix summation, multiplication, etc.) and more complicated
operations (determinants, inverses, etc.).
Definition 1.11 For T x c A, writing colk(A) = ak indicates extracting T x 1
column j from A. Writing Aj = rowj(A) indicates extracting a particular
1 x crow.
As an important example of partitioning, T x c A can be expressed in terms of
its c column vectors, {aj},w ith aj of dimension T x 1, or its T rows, {Ak}, with
Ak of dimension c x 1.
Definition 1.12 (a) Writing A = [ al a2 ... a,],w hich requires {aj} to be
T x 1, indicates {aj} have been horizontally concatenated.
Definition 1.13 Writing vec( ) indicates all elements of a matrix have been
stacked by column, as in bl = vec(A), because it creates an (TC) x 1 vector
from an T x c matrix. Equivalently, the columns have been vertically
concatenated.
Definition 1.15 Matrices conform for an operation if their sizes allow the
result of the operation to exist. Matrices do not conform for an operation ij
their dimensions do not allow the desired operation.
Definition 1.16 Matrix addition yields A + B = {az, + bzJ} while matrix
subtraction yields A - B = {azJ - bzJ}. Either result exists only if A and
B are the same size (and thereby conform for the operation).
FIVE KINDS OF MULTIPLICATION.
Definition 1.17 Scalar multiplication of a matrix gives Ab = bA = { baij}.
Definition 1.18 If A and B are both r x c, then elementwise
multiplication gives A # B = {a2Jb23=} C,w ith C also r x c.
Matrix multiplication can be expressed as a collection of cross products.
Multiplying row j' of A with column j' of B yields cjk = {rowj(A)colk(B)}.
Lemma 1.1 (a) Premultiplying by a (square) diagonal matrix scales the rows, and
postmultiplying by a (square) diagonal matrix scales the columns.
(b) The result generalizes to partitioned matrices with conforming partitions.
Definition 1.21 The horizontal direct product creates a new matrix by
elementwise multiplication of pairs of columns from two matrices with the same number of rows.
To operate on rows ( 1 ) transpose each operand, (2) compute the product, and
(3) transpose the result, as with (A' 0 B')'. The operator could be called the
vertical or column direct product.
With r x c A and s x d B, the result has dimension
( T S ) x ( c d ) = (rows x columns). Some authors choose to define {Abij} as the
direct product, which produces a different matrix.
THE DIRECT SUM.
Definition 1.23 The direct sum operator creates a block diagonal matrix
from any set of square matrices.
Direct products including an identity matrix, A @ I or I @ B, occur often in
expressions for covariance matrices of data in clusters of fixed size. A common
form occurs in describing the covariance matrix of data observed in N clusters of
constant size, with homogeneity of covariance between clusters.
RULES OF OPERATIONS.
Unless otherwise specified, we assume A and B conform for the operations in
question. Without additional knowledge of the matrices involved, the following
are true.
Theorem 1.1 Some operations obey commutative laws.
It is important to recognize that AB # BA and A @ B # B @ A, except in special cases.
Theorem 1.6 (a) For any matrix pair, tr(A @ B) = tr(A)tr(B)
(b) For conforming matrices, tr(AB) = tr(BA).
OTHER SPECIAL TYPES OF MATRICES.
Definition 1.24 A matrix of the form A'A is an inner product and AA' is an
outer product.
Both inner and outer products are always symmetric.
Using concepts introduced later in the chapter, inner and outer products are always
either positive definite (all eigenvalues real and positive) or positive semidefinite
(all real eigenvalues, with some positive and some zero). Inner and outer products
always have the same rank, which equals the rank of A. They also have the same
eigenvalues, except for some zeros if A is not square.
product) is diagonal, and a matrix is (rowwise) orthogonal if AA' (the outer
product) is diagonal. A matrix is (columnwise) orthonormal if A'A = I,
and a matrix is (rowwise) orthonormal if AA' = I. Two matrices are
biorthogonal if AB = 0.
In the preceding definition, neither A nor B need be square.
Definition 1.26 Any square matrix is described as idempotent if A = A2.
Lemma 1.3 If A is idempotent, then I - A is also idempotent and
A ( I - A ) = 0.
Idempotent matrices play important roles in discovering properties of quadratic
forms, especially independence.
QUADRATIC AND BILINEAR FORMS.
Definition 1.27 (a) For square A = A' and conforming x, the expression q = x'Ax is a quadratic form in x.
(b) The expression b = x',Bx,, for B not necessarily symmetric or square is a bilinear form in conforming vectors XI and x2.
If x ( 2 x 1) is free to vary and qo > 0 is constant, then qo = X'AX is the
equation of an ellipse. The result generalizes to higher dimensions. A quadratic
form lies at the heart of the density of a vector Gaussian and consequently leads to
ellipsoidal probability contours.
Lemma 1.4 If q = x'Az, then without loss of generality A may be assumed to be symmetric.
VECTOR SPACES AND RANK.
Definition 1.29 (a) Any finite set of n x 1 vectors generates a vector
space, namely the (usually infinite) collection of all possible vectors created
by any combination of multiplications of one vector by a constant, or the
addition of two vectors.
(b) Any set of vectors which generate the particular set of vectors spans the
vector space, and provides a basis for the space.
Definition 1.30 (a) The rank of the vector space equals the smallest possible
number of linearly independent vectors which span the space. The rank of a
set equals zero if and only if the only member of the set is xz = 0. The rank
of a set of p vectors, necessarily an integer, ranges from zero to p.
(b) A set with rank p isfull rank, while a set with rank strictly less than p is
less than full rank.
Any two distinct vectors XI and x2 are orthogonal if and only if xix2 = 0. An
orthogonal basis provides the most convenient form and has xix~ = 0 if j # j'.
Spectral (eigenvalue) decomposition provides an orthonormal basis for any square
and symmetric matrix, and some nonsymmetric square matrices. The singular
value decomposition provides a convenient way for any matrix, symmetric or not,
square or not. Both are discussed later in the chapter.
An T x c matrix, A, can be thought of as a collection of c vectors, the columns,
each T x 1. Alternately, considering the columns of A' allows describing the
matrix as a collection of T vectors, the transposed rows, each c x 1. The rank of a
matrix may be found by decomposing it into its columns and treating them as a set
of vectors.
Transposing the matrix allows applying the same process to the rows.
The resulting row rank always equals the column rank, which leads to the following.
Definition 1.31 The rank of a matrix equals the maximum number of linearly
independent rows or columns, indicated rank(A). An T x c matrix is full
rank if rank(A) = min(r, c) and less than f i l l rank otherwise. The only
matrix of rank zero is a matrix of all zeros, Onxm.
It would be hard to overemphasize the importance of the concepts of vector
space, span, basis, and orthogonal basis in the study of linear models.

</file>

<file= AmE06_J19>

In my methods classes, students are required to write short reaction papers based on the assigned readings before we discuss them in class. They are expected to explain how the articles they read resonate (or not) with their own experiences as students and as individuals, as well as with their emerging teaching orientations. Consider the following excerpt from a reading reaction paper written by Luisa, a Latina prospective teacher. Luisa was one of five Latinas in an elementary science methods class with 17 other female students and 1 male student (who were of Anglo ethnic backgrounds):
I also believe that it is important for educators and teachers, as well as everyone, to embrace diversity and multiculturalism. A few weeks ago, I heard some of my peers discussing multiculturalism, and they were saying that they thought it was so "annoying" that the education department promoted multiculturalism, and that it was stupid, and when they wrote papers they wrote what their professors wanted to hear, not what they really felt. I thought it was sad, and a little scary, that these people are our future teachers and they thought the "whole diversity and multicultural thing was annoying and stupid." It made me think that there are plenty of educators out there who also think like this, and there are even more children who will become the students of these teachers who will suffer because of their ignorance.
After having taught for more than a decade in postsecondary institutions in various cities in Canada and the United States, I have often heard and read comments like the ones Luisa mentions in the preceding quotation.
A few times, the comments are directed at me and meant to be personal because I am a Latino concerned with issues of social justice. One prospective teacher from the U.S. Midwest once wrote in his course evaluation, "Dr. Rodriguez is jamming multicultural education down our
throats because he has a chip on his shoulder." Comments of this nature reflect some of the prospective teachers' fears, frustrations, and anxieties as they attempt to make sense of what it means to be an effective teacher in a pluralist society. Whereas many prospective
teachers embrace the goals of the teacher education programs in which they are enrolled, others (like the ones just mentioned) often resist learning to teach for diversity (i.e., teaching in more culturally responsive, gender- inclusive, and socially relevant ways) and/or resist learning to teach for understanding (i.e., using constructivist, inquiry-based, and intellectually stimulating pedagogical approaches).
In this book, we (the authors in this volume including myself) recognize that the use of the term resistance has some limitations, because it is not always easily apparent what the intentions of the individual might be. Prospective teachers may purposely refuse to teach for diversity and
understanding for a variety of reasons, which are discussed later, or they may simply avoid using more inclusive, multicultural, and student-centered approaches because they feel that they lack confidence and/or skills. In any of these cases, we prefer to use the term resistance because it involves the individual's agency, or conscious choice to take action or not, and in doing so become authors of their own texts. We also prefer to use the term resistance because it provides a better construct for exploring the prospective teachers' thinking and professional growth, as they strive to align their belief systems and notions of good teaching with the expectations of their teacher education programs, programs that, in turn, are expected by teacher accreditation organizations, national and state boards of education, and national and state curriculum standards to prepare teachers as well-rounded professionals who are culturally inclusive and effective facilitators of knowledge in our pluralist society (Sleeter, 2001).
How then can teacher educators respond - and in essence positively counter - prospective teachers' resistance to teach for diversity and for understanding? What are some examples of promising pedagogical strategies that teacher educators could use in their courses to help prospective teachers meet the expectations of their teacher education programs, supervising teachers, and state and national standards? How can teacher educators better prepare prospective teachers to meet the challenges of helping increase the achievement and participation of all students in mathematics and science?
We tackle these questions head-on by providing rich narratives of our experiences in helping prospective teachers learn to teach for diversity and understanding in a variety of science and mathematics contexts.
Hence, the first goal of this chapter is to provide a broad description of multicultural education and social constructivism as the two theoretical frameworks that link all of the chapters together. The second goal is to provide an advanced organizer for the other chapters by listing a compendium of the various pedagogical strategies used by the authors in either their science or mathematics methods courses. We hope that this advanced organizer will provide a quick reference guide for readers and encourage them to find out more about how to implement these strategies in their own teaching contexts.
Also, it is important to note that the authors of this volume may use different ethnic terms to refer to individuals' cultural backgrounds. For example,
I prefer to use the term Anglo, because this relates to the individuals' ethnic and linguistic origins rather than their skin color. Although there is no perfect way to describe ethnic diversity, I believe these terms
are more inclusive than using colonial and skin color-based categories such as black, white, red, brown, and yellow. Using individuals' ethnic and cultural terms is a way to celebrate our rich cultural diversity instead of color coding it into colonial categories. Thus, readers may find the use of the following terms throughout this book, underrepresented or diverse students, First Nations (Native Americans), African or African American (Black), Latina/o (Hispanic), and Anglo (White).
In the next section, to facilitate discussion of the theoretical frameworks that guide the contributing authors' work, and how these relate to the construct of teacher resistance, two broad categories of teacher resistance are presented (Rodriguez, 1998a).
RESISTANCE TO LEARNING TO TEACH FOR DIVERSITY AND UNDERSTANDING
When the word resistance is used as a key term for conducting a bibliographical search in the Educational Resources Information Center (ERIC) database (http://askeric.org/plweb=cgi/fastweb?search) something really interesting happens. A list containing more than 6,900 documents is produced! This clearly indicates that at least in the last three decades the term resistance has been a key subject of study in education. This is also apparent when the word teacher is added to the same ERIC search. The result is a list of 2,574 documents dealing with teacher education and resistance in a variety of ways (e.g., teachers' resistance to teaching for diversity, students' resistance to learning the prescribed curriculum, resistance to dominant discourses, and so on).
It is odd, however, that the construct of resistance is not as predominant a topic in either the chapter titles or indices of important syntheses of educational research like those found in the Handbook of Research on Multicultural Education (Banks & Banks, 1995), the Handbook of Research on Teaching (Richardson, 2001),1 or the Handbook of Research on Science Teaching and Learning (Gabel, 1994). So, though teacher resistance to teaching in culturally and gender-inclusive ways (teaching for diversity), as well their resistance to teaching using student-centered and inquiry-based approaches (teaching for understanding), may be addressed in these handbooks or syntheses of educational research in one way or another, there is a need to make more predominant and more clear what educational researchers and educators mean by resistance in teacher education.
Most important, there is a need to provide a synthesis of how teacher educators are responding to teachers' resistance. This volume begins to address these issues by providing a compendium of promising pedagogical strategies used by teacher educators in a variety of contexts. The urgency to explore more effective ways to prepare teachers to teach for diversity and understanding is even more apparent when we consider the pervasive educational inequalities in our schools. These inequalities continue to prevent many traditionally underrepresented students from fully participating and finding academic success in science and mathematics classrooms (Rodriguez, 1998a, in press; Secada et al., 1998).
It is no wonder that in recent years, national standards and professional organizations in science and mathematics require that teachers be prepared to teach in more culturally sensitive and responsive ways (National Council of Teachers of Mathematics [NCTM], 1989, 2000; National Research Council [NRC], 1996). Similarly, the National Council for the Accreditation of Teacher Education (NCATE) requires teacher education programs to include courses on multicultural education and/or provide activities and experiences aimed to increase the prospective educators' teaching abilities and dispositions to work with culturally diverse students. In the United States, 25 out of 50 states and the District of Columbia require prospective teachers to complete multicultural education requirements (usually in the form of a university course) before certification (Sleeter, 2001). Even national teacher organizations, like the National Science
Teachers Association (NSTA), have put forward position statements that explicitly explain the importance of preparing teachers to teach for diversity.
For example, one of the NSTA's goals is to make certain that ethnically diverse children have "access to quality science education experiences that enhance success and provide the knowledge and opportunities required for them to become successful participants in our democratic society" (NSTA, 2000).
More recently, the No Child Left Behind (NCLB) act states that the gap in student achievement in the United States must be eliminated within 12 years, and that schools must make adequate yearly progress toward this goal (The White House, 2002). The NCLB act also mandates that schools failing to make adequate progress will be put in either the "identified for improvement category," or be reorganized under an improvement plan.
Whereas this new national policy evokes many questions regarding the significant financial and professional development support necessary to meet the prescribed goals within 12 years, we cannot avoid the obvious question: What role should (or must) teacher education programs play in helping prepare new generations of teachers to meet the challenges posed by the NCLB act? This takes us back to the core of this volume. If national policies and teacher education programs are expecting that teachers be
well prepared to teach effectively in diverse school contexts, how can teacher educators manage teachers' resistance to teaching for diversity and understanding? In this book, we provide a variety of suggestions for addressing this important question.
So, what is multicultural education, and why do some prospective teachers resist learning to teach for diversity?
RESISTANCE TO IDEOLOGICAL CHANGE The oppressor is solidary with the oppressed only when he stops regarding the oppressed as an abstract category and sees them as persons who have been unjustly dealt with, deprived of their voice, cheated in the sale of their labor ... To affirm that men are persons and as persons should be free, and yet do nothing tangible to make this affirmation a reality, is a farce. (Freire, 1989, p. 34)
This statement by Paulo Freire is at the crux of what it means to work for social justice in our everyday lives, and it also illustrates why perhaps the most difficult type of teacher resistance to manage is resistance to ideological change (resistance to teaching for diversity). This is defined as the resistance to changing one's beliefs and value system. Some prospective teachers who show resistance to ideological change might articulate a kind of "rugged individualism" by stating that what all students need to do is "to work hard enough" to be successful in science and /or mathematics or in life in general (Rodriguez, 1998b). They might also state that the students' language abilities, gender, ethnic backgrounds, or socioeconomic status do not matter, as long as they work hard enough. In addition, as mentioned previously, teachers may show resistance to ideological change not because they disagree with this orientation, but because they may lack the awareness, confidence, and/or knowledge and skills to implement more culturally responsive and socially relevant curriculum.
Since the early 1970s, research on multicultural education has been aimed at raising awareness about equity issues in all areas of education (Grant, 1999).

</file>

<file= AmE06_J20>

Introduction. Let K be a compact, 1-connected Lie group with complexification
G and Lie algebra k, and X a compact, connected Riemann surface. The
moduli spaceM(X) of isomorphism classes of flat K-bundles on X is homeomorphic
to the moduli space of grade-equivalence classes of semistable G-bundles,
by the theorems of Narasimhan-Seshadri and Ramanathan.
M(X) has two presentations as an infinite dimensional quotient which can
be used to study its cohomology. The first presentation was introduced by Atiyah
and Bott and is rather well understood. Let A(X) denote the affine space of
connections on the trivial K-bundle over X, with symplectic structure induced by
a choice of metric on k. The group K(X) of gauge transformations acts symplectically
on A(X) with moment map given by the curvature, and the symplectic
quotient is M(X).
Atiyah and Bott used the stratification of A(X) into Harder-Narasimhan types to compute the
Betti numbers of M(X); they conjectured that the stratification is identical to the
stratification into stable manifolds for the gradient flow of minus the Yang-Mills
functional. This was proved by Donaldson and Daskalopolous, who
also proved convergence of the gradient flow up to gauge transformation. R°ade proved that the gradient flow itself converges, and gave estimates for the
rate of convergence.
The second presentation has origins in Weil's double coset construction,
[3, p. 595]; here the related analysis has been less studied. Let S ⊂ X be an embedded
circle, and X the Riemann surface with boundary obtained by cutting X along S. The Yang-Mills heat flow on the space A(X) was studied by Donaldson, who obtained an analog of the Narasimhan-Seshadri theorem: The moduli
space M(X) of flat K-bundles with framings on the boundary is diffeomorphic
to G(∂X)/Ghol(X), where G(∂X) = Map (∂X,G) and Ghol(X) denotes the subgroup
of G(∂X) consisting of loops that extend holomorphically over the interior.
The loop group K(S) acts symplectically on M(X) with moment map given by
the difference of the restriction to the two boundary components, and M(X) is
homeomorphic to the symplectic quotient.
In recent years, this presentation has become more popular because of its connection
with conformal field theory and the Verlinde formulas. Here the circle S is assumed to bound a disk, so that X (in its algebraic
manifestation) becomes a punctured curve union a formal disk. Surfaces with
boundary do not fit into the algebraic framework.
In this paper we consider the analog of the Yang-Mills heat flow in the second
presentation, namely the gradient flow of minus the square of the moment map for
the loop group for an arbitrary embedded circle S in X. We show that the analog
of R°ade's result holds: The gradient flow exists for all times and converges to a
critical point. Although the evolution equation itself is not pseudo-differential, its
restriction to the boundary is a (nonlinear) heat equation involving the Dirichletto-
Neumann operator associated to the connection. Calder´on observed that this
is an elliptic pseudodifferential operator. Because pull-back to the boundary is
Fredholm on the space of harmonic forms, we are "up to finite dimensions" in
the same situation as for the first presentation, except that the moduli space of
framed bundles is not affine.
This analysis implies that M(X) admits a stratification into stable manifolds
for minus the gradient flow, which can be viewed as a generalization of the
Birkhoff decomposition to surfaces of higher genus. By definition, the stable
manifold for the zero locus of the moment map is the semistable locus. The other
strata are complex submanifolds of finite codimension, and the number of strata
of each codimension is finite. Using the stratification, we obtain several cohomological
applications which extend known results beyond the case that S bounds a
disk. The first, which was the motivation for the paper, is a K¨ahler "quantization
commutes with reduction" theorem, similar to that of Guillemin-Sternberg in
the finite dimensional case. This is an instance of Segal's composition axiom for
the Wess-Zumino-Witten conformal field theory. In the case S bounds
a disk, the algebraic version is due to Beauville-Laszlo, Kumar-Narasimhan-
Ramanathan, and Laszlo-Sorger; see also Teleman. The second
application is a surjectivity result for the equivariant cohomology with rational
coefficients, similar to that of Kirwan. In the case that S bounds a disk an
essentially equivalent result was proved by Bott, Tolman, and Weitsman. An
appendix contains a review of the relevant Sobolev spaces.
Background on connections on a circle. The following is contained in
Pressley-Segal in the context of smooth maps. Let S be a circle, that is, a
connected one-manifold.
For any s > 0, the group K(S)s+12:= Map (S,K)s+12 of free loops of Sobolev class s + 12
acts on the space A(S)s−12 of connections on the trivial bundle S ×K. Any connection differs from the trivial connection by a k-valued one-form; using the trivial connection as a base point we identify A(S)s−12 → Ω1(S; k)s−12.
For any s > r > 0 inclusion defines a bijection K(S)r−12\A(S)r+12→ K(S)s−12\A(S)s+12.
For s > 2, there is a smooth holonomy map Hol: A(S)s−12→ K depending on the choice of base point x0 in S; the assumption s > 2 implies that A is C1 which guarantees existence of a solution to the parallel transport equation.
For s > 0 and A ∈ A(S)s−12 the stabilizer K(S)s+12, A is a compact, connected Lie
group. For s > 2, K(S)s+12,A is isomorphic to the centralizer of the holonomy
Hol (A) via the map K(S)s+12 → K, k → k(x0).
Let Kx0 (S)s+12 denote the space of k ∈ K(S)s+12 such that k(x0) is the identity. For
s > 0 there are bijections Kx0 (S)s+12\A(S)s−12 → K, K(S)s+12\A(S)s−12 → Ad (K)\K,
which for s > 2 are given by taking the holonomy, resp. conjugacy class of the
holonomy of the connection.
The orbits of K(S)s+12 on A(S)s−12 can be parametrized by the Weyl alcove
as follows. Let Λ denote the coweight lattice of T and Waff := W ⋊ Λ
the affine Weyl group. The action of Waff on the Cartan subalgebra t has fundamental domain.
Background on connections on a surface. Let X be a compact, connected,
oriented surface. Since K is simply-connected, any principal K-bundle
is isomorphic to the trivial bundle X × K. Let A(X)s denote the affine space of
connections on X × K of Sobolev class s > 0. Using the trivial connection as
base point we may identify A(X)s → Ω1(X; k)s.
For any A ∈ A(X)s and K-representation V, we have by the Sobolev multiplication
theorem a covariant derivative dA(V): Ω0(X; V)s+1 → Ω1(X; V)s → Ω2(X; V)s−1.
Let dA := dA(k) denote the covariant derivative for the adjoint representation. A
is flat if and only if d2 A = 0. Choose a Riemannian metric on X and invariant
metric ( , ) on k and let ∗X: Ω•(X; k) → Ω2−•(X; k) denote the resulting Hodge star operator. The operator dA|Ω0(X, ∂X; k)s+1 has L2 adjoint d∗A: Ω1(X; k)−s → Ω0(X; k)−s−1, α → ∗X dA ∗ Xα
which restricts to a map Ω1(X; k)s → Ω0(X; k)s−1.
LEMMA 3.0.1. Suppose ∂X is nonempty. For A ∈ A(X)s, s > 0, (a) The generalized Laplacian
d∗A dA: Ω0(X, ∂X; k)s+1 → Ω0(X; k)s−1 is an isomorphism.
Proof. (a) Let A ∈ A(X) be smooth. By elliptic regularity Ker ( d∗
A dA)
consists of smooth solutions, see [16, Chapter 20], and we have a Hodge decomposition
Ω0(X; k)s−1 = Im(d∗A dA) ⊕ Ker ( d∗A dA). By the Aronszajn-Cordes
uniqueness theorem [1], Ker ( d∗A dA) = 0 and so d∗A dA is an isomorphism. Since
A(X) is dense in A(X)s and dA depends continuously on A ∈ A(X)s, d∗
A dA is an isomorphism for any A ∈ A(X)s. (b) By part (a), the subspaces are disjoint.
For a ∈ Ω0(X; k)s, we may find ξ ∈ Ω0(X, ∂X; k)s+1 such that d∗Aa = d∗
A dAξ. Then a − dAξ ∈ ker d∗A which shows the first splitting; the second is similar. (c)
follows immediately from d2A = (d∗A)2 = 0.
For s > 0 the gauge group K(X)s+1 := Map (X,K)s+1 is a Banach Lie group and acts on A(X)s by the formula k · A = Ad(k)A + k d(k−1) = Ad (k)A − dk k−1 in any faithful matrix representation of K. It has Lie algebra k(X)s+1 := Ω0(X; k)s+1.
The generating vector fields for the action of K(X)s+1 on A(X)s are
ξA(X)(A) := ddt ( exp ( − tξ) · A)|t=0 = dAξ, ξ ∈ k(X)s+1. 
In particular, the Lie algebra k(X)A of the stabilizer K(X)A of A is
k(X)A = ker ( dA | Ω0(X; k)).
Suppose X is equipped with a complex structure. The map d + ad(A) → ∂α := ∂ + ad(α) where α is the (0, 1)-form corresponding to a, defines a one-to-one correspondence
between covariant derivatives and holomorphic covariant derivatives
∂α: Ω0(X; g) → Ω0,1(X; g)
satisfying the holomorphic Leibniz rule ∂α( fs) = (∂f )s + f ∂αs. G(X) acts on the
space of holomorphic covariant derivatives by conjugation, and therefore on the
space of g-valued (0, 1)-forms by
g · α = Ad(g)α − (∂g)g−1.
This formula extends to a holomorphic action of G(X)s+1 on A(X)s. The invariant
metric on k defines a weakly symplectic form (that is, a closed 2-form that defines
an injection TA(X)s → T∗A(X)s) on A(X)s for s > 0 by
ωA(X): (a1, a2) → X (a1 ∧ a2), where (a1 ∧ a2) is the real-valued L1 two-form on X defined by the wedge and inner products. In the case that the boundary of X is empty, the action of K(X)s+1
is Hamiltonian with moment map given by the curvature [3]
M(X)s → Ω2(X; k)s−1, A → FA.
Let A (X)s denote the subspace of flat connections, A (X)s := {A ∈ A(X)s, FA = 0}.
The symplectic quotient M(X)s = K(X)s+1\\A(X)s := K(X)s+1\A (X)s is the moduli space of flat bundles on X. For s > 2, we have a holonomy map Hol: A (2)(X)s → Hom(π1(X, x0),K).
Evaluation at the base point x0 defines a homomorphism K(X)s+1 → K such that
k · A = k(x0) · Hol (A).
It follows that the stabilizer subgroup K(X)s,A is isomorphic to KHol (A) and so
K(X)s,A is compact. The holonomy map induces a homeomorphism M(X)s → Hom (π1(X, x0),K)/K.
In the case X has nonempty boundary, the moment map picks up an additional
term A(X)s → Ω2(X; k)s−1 ⊕ Ω1(∂X; k)(3) s−1/2, A → (FA,−r∂XA)where r∂X is restriction to the boundary. That is, for all ξ ∈ k(X)s+1(ξA(X))ωA(X) = −d X (FA ∧ ξ) + d ∂X (r∂XA ∧ ξ).
Let K∂(X)s+1 be the subgroup fixing a framing on the boundary, K∂(X)s+1 = {k ∈ K(X)s+1, k|∂X = 1}.
For s > 0 there is an exact sequence of Banach Lie groups 1 → K∂(X)s+1 → K(X)s+1 → K(∂X)s+12 → 1.
Surjectivity of the third map follows from triviality of π1(K) and the properties of the extension operator A.0.2 (e). The moment map for K∂(X)s+1 is the curvature and the symplectic quotient
M(X)s = K∂(X)s+1\\A(X)s := K∂(X)s+1\A (X)s is the moduli space of framed flat bundles on X. Note that the stabilizer K∂(X)s, A is trivial, since we can choose the base point to lie on the boundary. For s > 2 this gives another proof that the operator dA | Ω0(X, ∂X; k)s+1 is injective.
Charts for M(X)s, s > 0 are constructed from local slices for the gauge action
as follows. Using Lemma 3.0.1 and the implicit function theorem one sees that
for a ∈ Ω1(X; k)s sufficiently small there exists a unique gauge transformation
k ∈ K∂(X)s+1 in a neighborhood of the identity such that k · (A+ a) is in Coulomb
gauge with respect to A: d∗A(k · (A + a) − A) = 0.
Suppose that A is flat. By the implicit function theorem again, there exists a constant depending only on  d−1 A , open neighborhoods of A, resp. 0.

</file>

<file= AmE06_J21>

More than any other species, people are designed to be flexible learners and, from infancy, are active agents in acquiring knowledge and skills. People can invent, record, accumulate, and pass on organized bodies of knowledge that help them understand, shape, exploit, and ornament their environment. Much that each human being knows about the world is acquired informally, but mastery of the accumulated knowledge of generations requires intentional learning, often accomplished in a formal educational setting. Decades of work in the cognitive and developmental sciences has provided the foundation for an emerging science of learning. This foundation offers conceptions of learning processes and the development of competent performance that can help teachers support their students in the acquisition of knowledge that is the province of formal education. The research literature was synthesized in the National Research Council report How People
Learn: Brain, Mind, Experience, and School. In this volume, we focus on three fundamental and well-established principles of learning that are highlighted in How People Learn and are particularly important for teachers to understand and be able to incorporate in their teaching:
1. Students come to the classroom with preconceptions about how the world works. If their initial understanding is not engaged, they may fail to grasp the new concepts and information, or they may learn them for purposes of a test but revert to their preconceptions outside the classroom.
2. To develop competence in an area of inquiry, students must (a) have a deep foundation of factual knowledge, (b) understand facts and ideas in the context of a conceptual framework, and (c) organize knowledge in ways
that facilitate retrieval and application. take control of their own learning by defining learning goals and monitoring their progress in achieving them.
A FISH STORY
The images from a children's story, Fish Is Fish,  help convey the essence of the above principles. In the story, a young fish is very curious about the world outside the water. His good friend the frog, on returning from the
land, tells the fish about it excitedly:
"I have been about the world - hopping here and there," said the frog, "and I have seen extraordinary things."
"Like what?" asked the fish. "Birds," said the frog mysteriously. "Birds!" And he told the fish about the birds, who had wings, and two legs, and many, many colors. As the frog talked, his friend saw the
birds fly through his mind like large feathered fish. The frog continues with descriptions of cows, which the fish imagines as black-and-white spotted fish with horns and udders, and humans, which the fish imagines as fish walking upright and dressed in clothing. 
Principle 1: Engaging Prior Understandings
What Lionni's story captures so effectively is a fundamental insight about learning: new understandings are constructed on a foundation of existing understandings and experiences. With research techniques that permit the study of learning in infancy and tools that allow for observation of activity in the brain, we understand as never before how actively humans engage in learning from the earliest days of life. The understandings children carry with them into the classroom, even before the start of formal schooling, will shape significantly how they make sense of what they are taught. Just as the fish constructed an image of a human as a modified fish, children use what they know to shape their new understandings.
While prior learning is a powerful support for further learning, it can also lead to the development of conceptions that can act as barriers to learning. For example, when told that the earth is round, children may look to reconcile this information with their experience with balls. It seems obvious that one would fall off a round object. Researchers have found that some children solve the paradox by envisioning the earth as a pancake, a "round" shape with a surface on which people could walk without falling off. How People Learn summarizes a number of studies demonstrating the active, preconception-driven learning that is evident in humans from infancy through adulthood. Preconceptions developed from everyday experiences are often difficult for teachers to change because they generally work well enough in day-to-day contexts. But they can impose serious constraints on understanding formal disciplines. College physics students who do well on classroom exams on the laws of motion, for example, often revert to their untrained, erroneous models outside the classroom. When they are confronted with tasks that require putting their knowledge to use, they fail to take momentum into account, just as do elementary students who have had no physics training. If students' preconceptions are not addressed directly, they often memorize content (e.g., formulas in physics), yet still use their experience-based preconceptions to act in the world.
Principle 2: The Essential Role of Factual Knowledge
and Conceptual Frameworks in Understanding
The Fish Is Fish story also draws attention to the kinds of knowledge, factual and conceptual, needed to support learning with understanding. The frog in the story provides information to the fish about humans, birds, and cows that is accurate and relevant, yet clearly insufficient. Feathers, legs, udders, and sport coats are surface features that distinguish each species. But if the fish (endowed now with human thinking capacity) is to understand how the land species are different from fish and different from each other, these surface features will not be of much help. Some additional, critical concepts are needed  -  for example, the concept of adaptation. Species that move through the medium of air rather than water have a different mobility challenge. And species that are warm-blooded, unlike those that are cold-blooded, must maintain their body temperature. It will take more explaining of course, but if the fish is to see a bird as something other than a fish with feathers and wings and a human as something other than an upright fish with clothing, then feathers and clothing must be seen as adaptations that help solve the problem of maintaining body temperature, and upright posture and wings must be seen as different solutions to the problem of mobility outside water.
Conceptual information such as a theory of adaptation represents a kind of knowledge that is unlikely to be induced from everyday experiences. It typically takes generations of inquiry to develop this sort of knowledge, and people usually need some help (e.g., interactions with "knowledgeable others") to grasp such organizing concepts.8 Lionni's fish, not understanding the described features of the land animals as adaptations to a terrestrial environment, leaps from the water to experience life on land for himself. Since he can neither breathe nor maneuver on land, the fish must be saved by the amphibious frog. The point is well
illustrated: learning with understanding affects our ability to apply what is
learned.
This concept of learning with understanding has two parts: (1) factual knowledge (e.g., about characteristics of different species) must be placed in a conceptual framework (about adaptation) to be well understood; and (2) concepts are given meaning by multiple representations that are rich in factual detail. Competent performance is built on neither factual nor conceptual understanding alone; the concepts take on meaning in the knowledgerich
contexts in which they are applied. In the context of Lionni's story, the general concept of adaptation can be clarified when placed in the context of the specific features of humans, cows, and birds that make the abstract concept of adaptation meaningful.
This essential link between the factual knowledge base and a conceptual framework can help illuminate a persistent debate in education: whether we need to emphasize "big ideas" more and facts less, or are producing
graduates with a factual knowledge base that is unacceptably thin. While these concerns appear to be at odds, knowledge of facts and knowledge of important organizing ideas are mutually supportive. Studies of experts and novices - in chess, engineering, and many other domains - demonstrate that experts know considerably more relevant detail than novices in tasks within their domain and have better memory for these details (see Box 1-4). But the reason they remember more is that what novices see as separate pieces of information, experts see as organized sets of ideas.
Engineering experts, for example, can look briefly at a complex mass of circuitry and recognize it as an amplifier, and so can reproduce many of its circuits from memory using that one idea. Novices see each circuit separately, and thus remember far fewer in total. Important concepts, such as that of an amplifier, structure both what experts notice and what they are able to store in memory. Using concepts to organize information stored in memory allows for much more effective retrieval and application. Thus, the issue is not whether to emphasize facts or "big ideas" (conceptual knowledge); both are needed. Memory of factual knowledge is enhanced by conceptual knowledge, and conceptual knowledge is clarified as it is used to help organize constellations of important details. Teaching for understanding, then, requires that the core concepts such as adaptation that organize the knowledge of experts also organize instruction. This does not mean that that factual knowledge now typically taught, such as the characteristics of fish, birds, and mammals, must be replaced. Rather, that factual information is given new meaning and a new organization in memory because those features are seen as adaptive characteristics.
Principle 3: The Importance of Self-Monitoring
Hero though he is for saving the fish's life, the frog in Lionni's story gets poor marks as a teacher. But the burden of learning does not fall on the teacher alone. Even the best instructional efforts can be successful only if the student can make use of the opportunity to learn. Helping students become effective learners is at the heart of the third key principle: a "metacognitive" or self-monitoring approach can help students develop the ability to take control of their own learning, consciously define learning goals, and monitor their progress in achieving them. Some teachers introduce the idea of metacognition to their students by saying, "You are the owners and operators of your own brain, but it came without an instruction book. We need to learn how we learn." "Meta" is a prefix that can mean after, along with, or beyond. In the psychological literature, "metacognition" is used to refer to people's knowledge about themselves as information processors. This includes knowledge about what we need to do in order to learn and remember information (e.g., most adults know that they need to rehearse an unfamiliar phone number to keep it active in short-term memory while they walk across the room to dial the phone). And it includes the ability to monitor our current understanding to make sure we understand. Other examples include monitoring the degree to which we have been helpful to a group working on a project.
In Lionni's story, the fish accepted the information about life on land rather passively. Had he been monitoring his understanding and actively comparing it with what he already knew, he might have noted that putting on a hat and jacket would be rather uncomfortable for a fish and would slow his swimming in the worst way. Had he been more engaged in figuring out what the frog meant, he might have asked why humans would make themselves uncomfortable and compromise their mobility. A good answer to his questions might have set the stage for learning about differences between humans and fish, and ultimately about the notion of adaptation. The concept of metacognition includes an awareness of the need to ask how new knowledge relates to or challenges what one already knows - questions that stimulate additional inquiry that helps guide further learning.The early work on metacognition was conducted with young children in laboratory contexts. In studies of "metamemory," for example, young children might be shown a series of pictures (e.g., drum, tree, cup) and asked to remember them after 15 seconds of delay (with the pictures no longer visible). Adults who receive this task spontaneously rehearse duringn the 15-second interval. Many of the children did not. When they were explicitly told to rehearse, they would do so, and their memory was very good.

</file>

<file= AmE06_J22>

Chapter 1.
Philosophy as a way of life.
If I do not reveal my views on justice in words, I do so by my conduct - Socrates to Xenophon. Despite its claim to be novel and unprecedented, existentialism represents a long tradition in the history of philosophy in the West, extending back at least to Socrates (469-399 bc ). This is the practice of philosophy as 'care of the self' ( epimeleia heautou ). Its focus is on the proper way of acting rather than on an abstract set of theoretical truths. Thus the Athenian general Laches, in a Platonic dialogue by that name, admits that what impresses him about Socrates is not his teaching but the harmony between his teaching and his life. And Socrates himself warns the Athenian court at the trial for his life that they will not easily find another like him who will instruct them to care for their selves above all else. This concept of philosophy flourished among the Stoic and Epicurean philosophers of the Hellenistic period. Their attention was focused primarily on ethical questions and discerning the proper way to live one's life. As one Classical scholar put it, 'Philosophy among the Greeks was more formative than informative in nature'. The philosopher was a kind of doctor of the soul, prescribing the proper attitudes and practices to foster health and happiness.
Of course, philosophy as the pursuit of basic truths about human nature and the universe was also widespread among the Ancient Greeks and was an ingredient in the care of the self. It was this more theoretical approach that led to the rise of science and came to dominate the teaching of philosophy in the medieval and modern periods. Indeed, 'theory' today is commonly taken as synonymous with 'philosophy' in general, as in the expressions 'political theory' and 'literary theory', to such an extent that 'theoretical philosophy' is almost redundant. At issue in this distinction between two forms of philosophy (among other things) are two different uses of 'truth': the scientific and the moral. The former is more cognitive and theoretical, the latter more self-formative and practical, as in 'to thine own self be true'. Whereas the former made no demands on the kind of person one should become in order to know the truth (for the 17th-century philosopher Rene Descartes, a sinner could grasp a mathematical formula as fully as a saint), the latter kind of truth required a certain self-discipline, a set of practices on the self such as attention to diet, control of one's speech, and regular meditation, in order to be able to access it. It was a matter of becoming a certain kind of person, the way Socrates exhibited a particular way of life, rather than of achieving a certain clarity of argument or insight in the way Aristotle did. In the history of philosophy, care of the self was gradually marginalized and consigned to the domains of spiritual direction, political formation, and psychological counselling. There were important exceptions to this exiling of 'moral' truth from the academy. St Augustine's Confessions ( ad 397), Blaise Pascal's Pensees (1669), and the writings of the German Romantics in the early 19th century are examples of works that encouraged this understanding of philosophy as care of the self. It is in this larger tradition that existentialism as a philosophical movement can be located. The existentialists can be viewed as reviving this more personal notion of 'truth', a truth that is lived as distinct from and often in opposition to the more detached and scientific use of the term. It is not surprising that both Soren Kierkegaard (1813-55) and Friedrich Nietzsche (1844-1900), the 19th-century 'fathers of existentialism', had ambivalent attitudes towards the philosophy of Socrates. On the one hand, he was seen as the defender of a kind of rationality that moved beyond merely conventional and subjective values towards universal moral norms, for which Kierkegaard praised him and Nietzsche censured him. But they both respected his individuating 'leap' across the gap in rationality between the proofs of personal immortality and his choice to accept the sentence of death imposed by the Athenian court. (Socrates was tried and found guilty on charges of impiety and for corrupting the youth by his teaching.) In other words, each philosopher realized that life does not follow the continuous flow of logical argument and that one often has to risk moving beyond the limits of the rational in order to live life to the fullest. As Kierkegaard remarked, many people have offered proofs for the immortality of the soul, but Socrates, after hypothesizing that the soul might be immortal, risked his life with that possibility in mind. He drank the poison as commanded by the Athenian court, all the while discoursing with his followers on the possibility that another life may await him. Kierkegaard called this an example of 'truth as subjectivity'. By this he meant a personal conviction on which one is willing to risk one's life. In his Journals , Kierkegaard muses: 'the thing is to find a truth which is true for me , to find the idea for which I can live and die' (1 August 1835). 
Clarity is not enough. 
Galileo wrote that the book of nature was written in mathematical characters. Subsequent advances in modern science seemed to confirm this claim. It appeared that whatever could be weighed and measured (quantified) could give us reliable knowledge, whereas the non-measurable was left to the realm of mere opinion. This view became canonized by positivist philosophy in the 19th and
early 20th centuries. This positivist habit of mind insisted that the 'objective' was synonymous with the measurable and the 'value-free'. Its aim was to extract the subject from the experiment in order to obtain a purely impersonal 'view from nowhere'. This led to a number of significant discoveries, but it quickly became apparent that such an approach was inconsistent. The limiting of the knowable to the quantifiable was itself a value that was not quantifiable. That is, the choice of this procedure was itself a 'leap' of sorts, an act of faith in a certain set of values that were not themselves measurable. Moreover, the exclusion of the non-measurable from what counted as knowledge left some of our most important questions not only unanswered but unanswerable. Are our ethical rules and values merely the expression of our subjective preferences? To paraphrase the mathematician and philosopher Bertrand Russell, scarcely an existentialist: can anyone really believe that the revulsion they feel when they witness the gratuitous infliction of pain is simply an expression of the fact that they don't happen to like it? Such was the doctrine of the 'emotivists' in ethical theory, sometimes called the 'boo/hurrah' theory of moral judgements. They were forced in that direction by acceptance of the positivist limitation of knowledge to the measurable. But are we even capable of the kind of antiseptic knowledge that the positivists require of science? Perhaps the knowing subject can be reintroduced into these discussions without compromising their objectivity. Much will depend on us revising our definition of 'objectivity' as well as on discovering other uses of the word 'true' besides the positivists' 'agreement with sense experience'. The existentialists among others responded to this challenge. Jean-Paul Sartre (1905-80) exemplifies this response when he remarks that the only theory of knowledge that can be valid today is one which is founded on that truth of microphysics: the experimenter is part of the experimental system. What he has in mind is the so-called Heisenberg Uncertainty Principle from atomic physics which, in its popular interpretation at least, states that the instruments which enable us to observe the momentum and the position of an orbital electron interfere with the process such that we can determine the one or the other but never both at once. Analogously, one can object that the very act of intervening in the life of a 'primitive' tribe prevents the ethnologist from studying that people in their pristine condition. Such considerations served to undermine the positivists' concept of knowledge as measurability. But they also clouded the rationalists' view of reality as exhaustively available to a logic of either/or with no middle ground. To cite another example, light manifests qualities that indicate it is a wave and others that show it to be a particle. Yet these two characteristics seem to exclude each other, leaving the question 'Is light a wave or a particle?' unanswerable with the standard logic of either/or. Light seems to be both and yet neither exclusively. Another kind of logic seems called for to make sense of this phenomenon. Numerous other examples from physics and mathematics appeared early in the last century that offered counterexamples to the positivists' and the rationalists' claims about knowledge and the world. 
Lived experience. 
It is into this world of limited and relative observation and assessment that the existentialist enters with his/her drive to 'personalize' the most impersonal phenomena in our lives. What, for example, could be more impersonal and objective than space and time? Even the chastened view of space-time that the Relativity Theory offers us relies on an absolute or constant referent, namely the speed of light. We measure time by minutes and seconds and chart space by yards or metres. This too seems quantitative and hence objective in the positivists' sense. And yet the notion of what existentialists call 'ekstatic' temporality adds a qualitative and personal dimension to the phenomenon of time-consciousness. For the existentialist, the value and meaning of each temporal dimension of lived time is a function of our attitudes and choices. Some people, for example, are always pressed to meet obligations whereas others are at a loss to occupy their time. Time rushes by when you're having fun and hangs heavy on your hands when you are in pain. Even the quantitative advice to budget our time, from an existentialist point of view, is really a recommendation to examine and assess the life decisions that establish our temporal priorities in the first place. If 'time is of the essence', and the existentialist will insist that it is, then part of who we are is our manner of living the 'already' and the 'not yet' of our existence, made concrete by how we handle our immersion in the everyday. The existentialist often dramatizes such 'lived time'. Thus, Albert Camus (1913-60) in his allegory of the Nazi occupation of Paris, The Plague , describes the people in a plague-ridden, quarantined city: 'Hostile to the past, impatient of the present, and cheated of the future, we were much like those whom men's justice, or hatred, forces to live behind prison bars.' The notion of imprisonment as 'doing time' is clearly existential. And Sartre, in an insightful analysis of emotive consciousness, speaks of someone literally 'jumping for joy' as a way of using their bodily changes to conjure up, as if by magic, the possibility of possessing a desirable situation 'all at once' without having to await its necessary, temporal unfolding. Though Sartre stated this thesis in the 1930s, one immediately thinks of the photo of Hitler's little 'jig' under the Arc de Triomphe during the German occupation of Paris. Time has its own viscosity, as Michel Foucault remarked. Ekstatic temporality embodies its flow. But existential space is personalized as well. Sartre cites the social psychologist Kurt Lewin's notion of 'hodological' space (lived space) as the qualitative equivalent to the lived time of our quotidian existence. The story is told of two people, one who prefers to get as closely face-to-face in conversation as possible and the other a distant, stand-off kind of person, propelling and repelling each other around the room at a cocktail party in an attempt to carry on a conversation. Lived space is personal; it is the usual route I take to work, the seating arrangement that quickly establishes itself in a classroom, or the ordering of the objects on my desk. It is what psychologists call my 'comfort zone'. This too is a function of my life project.

</file>

<file= AmE06_J23>

The Johnson Presidency.
Yellow Light: Johnson and the Crisis of May - June 1967.
Lyndon Baines Johnson brought to the presidency
a remarkable array of political talents. An activist and a man of
strong passions, Johnson seemed to enjoy exerting his power. As majority
leader in the Senate, he had used the art of persuasion as few other
leaders had; building consensus through artfully constructed compromises
had been one of his strong suits. His political experience did not, however,
extend to foreign-policymaking, an area that demanded his attention,
especially as American involvement in Vietnam grew in late 1964 and
early 1965.
Fortunately for the new president, one part of the world that seemed
comparatively quiet in the early 1960s was the Middle East. Long-standing
disputes still simmered, but compared with the turbulent 1950s, the
situation appeared to be manageable. The U.S.-Israeli relationship had
been strengthened by President Kennedy, and Johnson obviously was prepared
to continue on this line, particularly with increases in military assistance.
His personal sentiments toward Israel were warm and admiring. To
all appearances he genuinely liked the Israelis he had dealt with, many of
his closest advisers were well-known friends of Israel, and his own contacts
with the American Jewish community had been close throughout his
political career.
Johnson's demonstrated fondness for Israel did not mean he was particularly
hostile to Arabs, but it is fair to say that Johnson showed little
sympathy for the radical brand of Arab nationalism expounded by Egypt's president Gamal Abdel Nasser. And he was sensitive to signs that the
Soviet Union was exploiting Arab nationalism to weaken the influence of
the West in the Middle East. Like other American policymakers before
him, Johnson seemed to waver between a desire to try to come to terms
with Nasser and a belief that Nasser's prestige and regional ambitions
had to be trimmed. More important for policy than these predispositions,
however, was the fact that Johnson, overwhelmingly preoccupied by Vietnam,
treated Middle East issues as deserving only secondary priority.
U.S.-Egyptian relations had deteriorated steadily between 1964 and
early 1967, in part because of the conflict in Yemen, in part because of
quarrels over aid. By 1967, with Vietnam becoming a divisive domestic
issue for Johnson, problems of the Middle East were left largely to the
State Department. There, the anxiety about increased tension between
Israel and the surrounding Arab states was growing after the Israeli raid
on the Jordanian town of Al-Samu' in November 1966, and especially
after an April 1967 Israeli-Syrian air battle that resulted in the downing
of six Syrian MiGs. Under Secretary of State Eugene Rostow was particularly
concerned about the drift of events, suspecting that the Soviets were
seeking to take advantage in the Middle East of Washington's preoccupation
with Vietnam.
If the tensions on the Syrian-Israeli border provided the fuel for the
early stages of the 1967 crisis, the spark that ignited the fuel came in the
form of erroneous Soviet reports to Egypt on May 13 that Israel had
mobilized some ten to thirteen brigades on the Syrian border. Against the
backdrop of Israeli threats to take action to stop the guerrilla raids from
Syria, this disinformation apparently helped to convince Nasser the time
had come for Egypt to take some action to deter any Israeli moves against
Syria and to restore his own somewhat tarnished image in the Arab
world. The Soviets, he seemed to calculate, would provide firm backing
for his position.
On May 14 Nasser made the first of his fateful moves. Egyptian troops
were ostentatiously sent into Sinai, posing an unmistakable challenge to
Israel, if not yet a serious military threat. President Johnson and his key
advisers were quick to sense the danger in the new situation. Because of
his well-known sympathy for Israel and his forceful personality, Johnson
might have been expected to take a strong and unambiguous stand in
support of Israel at the outset of the crisis, especially as such a stand might
have helped to prevent Arab miscalculations. Moreover, reassurances to
Israel would lessen the pressure on Prime Minister Levi Eshkol to resort
to preemptive military action. Finally, a strong stand in the Middle East would signal the Soviet Union that it could not exploit tensions there
without confronting the United States.
The reality of U.S. policy as the Middle East crisis unfolded in May
was, however, quite different. American behavior was cautious, at times
ambiguous, and ultimately unable to prevent a war that was clearly in the
offing. Why was that the case? This is the central puzzle in Johnson's
reaction to the events leading up to the June 1967 war. Also, one must ask
how hard Johnson really tried to restrain Israel. Some have alleged that
Johnson in fact gave Israel a green light to attack, or in some way colluded
with Israel to draw Nasser into a trap. These charges need to be carefully
assessed. And what role did domestic political considerations play in Johnson's
thinking? Did the many pro-Israeli figures in Johnson's entourage
influence his views?
Initial Reactions to the Crisis.
Nasser's initial moves were interpreted in Washington primarily in political
terms. Under attack by the conservative monarchies of Jordan and
Saudi Arabia for being soft on Israel, Nasser was seen as attempting to
regain prestige by appearing as the defender of the embattled and threatened
radical regime in Syria. Middle East watchers in the State Department
thought they recognized a familiar pattern. In February 1960 Nasser
had sent troops into Sinai, postured for a while, claimed victory by deterring
alleged Israeli aggressive designs, and then backed down. All in all,
a rather cheap victory, and not one that presented much of a danger to
anyone. Consequently the initial American reaction to Nasser's dispatch
of troops was restrained. Even the Israelis did not seem to be particularly
alarmed.
On May 16, however, the crisis took on a more serious aspect as the
Egyptians made their initial request for the removal of the United Nations
Emergency Force (UNEF). This prompted President Johnson to sound out
the Israelis about their intentions and to consult with the British and the
French. On May 17 Johnson sent Eshkol the first of several letters
exchanged during the crisis, in which he urged restraint and specifically
asked to be informed before Israel took any action. "I am sure you will
understand that I cannot accept any responsibilities on behalf of the
United States for situations which arise as the result of actions on which
we are not consulted."
From the outset, then, Johnson seemed to want to avoid war, to restrain
the Israelis, and to gain allied support for any action that might be taken.
Two possible alternative courses of action seem not to have been seriously
considered at this point. One might have been to stand aside and let the
Israelis act as they saw fit, even to the extent of going to war. The danger,
of course, was that Israel might get into trouble and turn to the United
States for help. Johnson seemed to fear this possibility throughout the crisis,
despite all the intelligence predictions that Israel would easily win a war
against Egypt alone or against all the surrounding Arab countries.
The second alternative not considered at this point was bold unilateral
American action opposing Nasser's effort to change the status quo. Here
the problems were twofold. A quarrel with Egypt might inflame the situation
and weaken American influence throughout the Arab world. The
Suez precedent, and what it had done to British and French positions in
the region, was very much in the minds of key American officials. Nasser
was not noted for backing down when challenged. Moreover, U.S. military
assets were deeply committed in Vietnam, ruling out a full-scale military
confrontation with Egypt. But even if American forces had been
available, Congress was in no mood to countenance unilateral military
action, even in support of Israel. Therefore, the initial United States effort
was directed toward restraining Israel and building a multilateral context
for any American action, whether diplomatic or military.
Eshkol's reply to Johnson's letter reached Washington the following
day, May 18. The Israeli prime minister blamed Syria for the increase in
tension and stated that Egypt must remove its troops from Sinai. Then,
appealing directly to Johnson, Eshkol requested that the United States
reaffirm its commitment to Israeli security and inform the Soviet Union in
particular of this commitment. Johnson wrote to Premier Aleksei Kosygin
the following day, affirming the American position of support for Israel
as requested, but suggesting in addition a "joint initiative of the two powers
to prevent the dispute between Israel and the UAR [United Arab
Republic, or Egypt] and Syria from drifting into war."
After Egypt's initial request for the withdrawal of the UNEF on May
16, there was danger that Nasser might overplay his hand by also closing
the Strait of Tiran to the Israelis. The opening of the strait to Israeli shipping
was Israel's one tangible gain in the 1956 war. American commitments
concerning the international status of the strait were explicit. It
was seen as an international waterway, open for the free passage of ships
of all nations, including Israel. The Israelis had been promised that they
could count on U.S. support to keep the strait open.
The UNEF had stationed troops at Sharm al-Shaykh since 1957, and
shipping had not been impeded. If the UNEF withdrew, however, Nasser
would be under great pressure to return the situation to its pre-1956 status. Israel had long declared that such action would be considered a
casus belli.
In light of these dangers, one might have expected some action by the
United States after May 16, aimed at preventing the complete removal of
the UNEF. But the record shows no sign of an urgent approach to UN secretary
general U Thant on this matter, and by the evening of May 18 U
Thant had responded positively to the formal Egyptian government
request that all UNEF troops leave Egyptian territory.
The strait still remained open, however, and a strong warning by Israel
or the United States about the consequences of its closure might conceivably
have influenced Nasser's next move. From May 19 until midday on
May 22, Nasser took no action to close the strait, nor did he make any
threat to do so. Presumably he was waiting to see how Israel and the
United States, among others, would react to withdrawal of the UNEF. The
United States made no direct approach to Nasser until May 22, the day
Nasser finally announced the closure of the strait. It issued no public
statements reaffirming the American view that the strait was an international
waterway, nor did the reputedly pro-Israeli president respond to
Eshkol's request for a public declaration of America's commitment to
Israel's security.
On May 22 Johnson finally sent a letter to the Egyptian leader. The
thrust of the message was to assure Nasser of the friendship of the United
States while urging him to avoid any step that might lead to war. In addition,
Johnson offered to send Vice President Hubert Humphrey to Cairo.
Johnson ended the letter with words he had personally added: "I look forward
to our working out a program that will be acceptable and constructive
for our respective peoples." The message was not delivered by
ambassador-designate Richard Nolte until the following day, by which
time the strait had already been declared closed to Israeli shipping and
strategic cargoes bound for Israel.
Johnson informed Eshkol the same day that he was writing to the
Egyptian and Syrian leaders, warning them not to take actions that might
lead to hostilities. In addition, another message from Johnson to Kosygin
was also sent on May 22. Reiterating his suggestion of joint action to
calm the situation, Johnson went on to state, "The increasing harassment
of Israel by elements based in Syria, with attendant reactions within Israel
and within the Arab world, has brought the area close to major violence.
Your and our ties to nations of the area could bring us into difficulties
which I am confident neither of us seeks.

</file>

<file= AmE06_J24>

Bike messengers work in a dirty and dangerous occupation with low pay and
no benefits. At the same time, many messengers consider their occupation to
be their primary source of identity. In contrast to "dirty work" studies, which
focus on organizational structures and workgroup ideologies, this paper explores
how the creativity and spontaneity of courier labor allows messengers
to become emotionally attached to their job. Bike messengering brings the
thrill-seeking of leisure pursuits into the workplace, which creates an authentic
self intimately tied to the occupation - an exceedingly difficult feat in an
increasingly rationalized system of labor.
INTRODUCTION: DIRTY WORK AND CENTRAL LIFE INTEREST.
In The Protestant Ethic and the Spirit of Capitalism, Weber (1958)
envisioned the future as life in an iron cage. Within the cage, spiritualized
values and meanings are superceded by capitalism's internal logic
of accumulation. "Doing one's job" becomes a task with no higher purpose.
Conversely, decades of occupational research has shown that even
the most rationalized work is not completely devoid of meaning. Workers
in stigmatized occupations pose a particular challenge to sociologists
(Ashforth and Kreiner, 1999). Bike messengers are an excellent example of "dirty work" (Hughes, 1951). Not only is the job physically dirty and
dangerous, but also because couriers regularly disregard traffic rules in
performing their tasks, many non-messengers find the job morally reprehensible.
As a columnist for the Washington Post ranted, "In my gentler
moments, I've called them law-flouting, obscenity-spewing, bath-needing,
wild-riding, pedestrian-smashing madmen" (Levy, 1989:E4). Despite such
condemnations, many couriers forge exceedingly strong identities from
their occupation. The messengers in this study do not simply work as
bike couriers; their entire lives are wrapped up inside a distinct messenger
lifestyle that cherishes the thrills and threats of dodging cars as they speed
through the city.
For Ashforth and Kreiner (1999), dirty workers overcome the physical,
social, and moral taint of their occupations by reframing, recalibrating,
and refocusing the stigma of outsiders. In Thompson and Harred's (1992)
study of exotic entertainers, for example, dancers rationalize and neutralize
negative opinions by denying any moral wrong doing, condemning the
condemners, and appealing to higher moral issues (e.g., making money
to support one's children or putting one's self through college). In their
later work (Thompson et al., 2003), they also address the issue of cognitive
and emotive dissonance. Dancers perform deviant acts by constructing
fictional personas capable of behaving in ways their "real selves" would
not.
Missing from the research on dirty and dangerous occupations is an understanding
of emotional involvement in the creation of the self. For Dubin
(1992), activities become central life interests - they feel meaningful - when
they involve emotional commitment. Emotions become involved when situations
allow for creative and spontaneous action. In this paper, I address
a gap in the occupational literature by connecting the strong selfidentification
of bike messengers to the affective attributes of the job. That
is, if self-identity is created through emotional involvement, the analysis of
workers in dirty occupations must not only describe how work cultures subvert
outsiders' opinions or detach insiders' emotions.Against the dirty work
literature, I argue that it is primarily the emotional investment made possible
by the game-like potential of messengering that makes the occupation
a central life interest.
I first provide an overview of the literature on the consequences of
rationalized labor, the emotional components involved in the creation of
authentic selves, and the shortcomings of dirty work studies. I next explain
my research methods. Third, I give an account of messenger life,
explaining why it is a dirty occupation. Fourth, I detail how messenger
work allows for the creative spontaneity usually found in leisuretime
pursuits. I conclude by discussing the relevance of this paper to research on alienation and the implications for future studies of stigmatized
occupations.
LITERATURE REVIEW: FROM ALIENATION TO EDGEWORK.
Like Weber, Blauner (1964) argues that bureaucratic organizations
and complex divisions of labor divest workers from a sense of purpose
at work. Further, individuals rarely have control over the terms of their
labor. Together, such powerlessness and meaninglessness leads to selfestrangement.
The estranged individual - like Weber's (1958) modern economic
man - feels no attachment to her occupation. "Particularly when an
individual lacks control over the work process and a sense of purposeful
connection to the work enterprise, he may experience a kind of depersonalized
detachment rather than an immediate involvement or engrossment in
the job task" (Blauner, 1964:26). In Sennett's (1998) study of bakery workers,
the potential for pride and satisfaction in the craft of baking has been
eliminated through computerization. Blauner's powerlessness and meaninglessness
are compounded by unfulfilled potential as the strength, dexterity,
and thought formerly required in industrial work is replaced with
routinized button pushing. In Leidner's (1993) ethnography of McDonald's
employees, the same corrosion of character is seen in the service industry.
Everything in McDonald's, including the width of the pickles, has been
standardized. Through semi-automated food preparation and computerized
cash registers, non-scripted encounters with customers - even requests as
simple as selling a cup of half decaffeinated, half regular coffee - are not
possible.
The rationalization of the workplace has brought play and leisure to
the fore in experiencing an authentic self (Aronowitz, 1973; Caillois, 1961;
Dubin, 1992; Gecas, 1994; Lasch, 1978; Lyng, 1990). "[T]he declining possibilities
for creative self-realizing action in the economic and other institutional
spheres in modern society can be compensated for in skillful
leisure-time pursuits of play, particularly those forms of play that involve
skill" (Gecas, 1994:146). The importance of leisure in the development of
the self can be seen in Wolf's (1991) ethnography of an outlaw motorcycle
club. For the outlaw biker, the frustrations of limited occupational
mobility are dampened by making creative mechanical variations to their
self-customized motorcycles, and demonstrating the skills required to ride
such "hogs" at excessive speeds. At the opposite end of the class spectrum,
Mitchell's (1983) study of primarily upper-middle class mountain climbers
equally demonstrates the urge to exercise one's creativity and skill in ways
not utilized in one's occupation. Mountaineering involves not only physical exertion, but also involves innovative tasks of problem-solving (e.g., routes
of ascent and descent, tool choices, assessment of weather conditions,
etc.).
The common thread in both outlaw biking and climbing is the use of
creativity and skill to manage the uncertainty of self-created dangers. That
is, the hogs bikers build for themselves - with customized handlebars that
extend upwards above shoulder level - are intentionally difficult to operate.
Mountaineers are less concerned with making it to the tops of mountains
and more concerned with the technological limits they place on themselves
in getting there (i.e., the less equipment a climber uses on her way to the
peak the better, and if she uses too many tools, her peers might not recognize
the climb). Intentionally entering life-threatening situations to test
one's ability is what Lyng (1990) calls "edgework." "Edgework involves not
only activity-specific skills but also a general ability to maintain control of a
situation that verges on total chaos" (871).
As Dubin (1992) explains, contemporary work tends to lack emotional
involvement. In Mead's (1962) conception of the mind, the self comprises
the "I" and the "me." The "me" is the mediating voice of society that directs
the spontaneous action of the "I." As Leidner (1993) shows, work has exceedingly
separated itself from the "I" and fallen into the realm of socially
required, but emotionally flat, roles of the "me." Under the constraints of
rationalization, labor lacks spontaneity. Work increasingly consists of routinized
behaviors revolving around credentialization and economic status - 
one-dimensional humanity (Marcuse, 1964). For Lyng (1990), such overreification
is countered by the intoxicating thrills provided in edgework. In
edgework, the "me" disappears as survival becomes purely a matter of the
moment-to-moment responses of the individual - the "I." "The experience
of self in edgework, then, is the direct antithesis of that under conditions of
alienation and reification. If life under such circumstances leads to an oversocialized
self in which numerous institutional 'me's' are present but ego is
absent, edgework calls out an anarchic self in which ego is manifest but the
personal, institutional self is completely suppressed" (878).
Subsumed by the moment, the edgeworker loses track of time. Such
non-reflexive awareness is the exact opposite of Blauner's (1964) selfestrangement.
It is the loss of time which is essential to genuine play.
"While clock-watching is a characteristic disease of those burdened with
alienated labor, those engrossed creatively are oblivious of the passage of
social time or the dimensions of physical space. We 'lose ourselves' and
cease to measure our activities in so many units of minutes and hours"
(Aronowitz, 1973:62). Of course, it is not only in tempting death that one
can become "engrossed creatively." Csikszentmihalyi's (1990) concept of "flow" is a more general process of self-actualization. In flow, a person's
tasks are perfectly matched by her skills. In a game of tennis, for example,
playing against an opponent far below one's skill level is boring, whereas
playing against someone with far superior skills is stressful. In either case,
the players are consciously aware of themselves and the game. Matched
against someone of equal talent, however, the game takes on an entirely
different character. At the threshold of boredom and anxiety, players can
truly focus their attention in the moment, and - as they do not in normal,
reflexive thought - only in the moment.
Dubin (1992) has argued for the connection between the spontaneity of
the "I" and the emotional involvement of the individual. Through creative
management of risk, activities can become the foundation for one's "true
self." The risk Dubin is referring to does not necessarily involve the mortal
threats found in edgework; the issue is simply having agency over uncertain
outcomes. Dubin champions the classic professions as the remaining bastion
of meaningful labor in contemporary society. However, less prestigious
jobs can also contain elements of spontaneous creativity. In Burawoy's
(1979) ethnography of factory labor, workers transformed their tasks into
a flow-inducing game of "making out." In making out, workers willingly
chose to increase their output in an effort to earn a higher rate of pay.
While workers desired the extra pay, the ultimate motivations for playing
the game were not financial. As Burawoy notes, "Playing the game eliminated
much of the drudgery and boredom associated with industrial work"
(89). Decades earlier, Roy (1953) wrote of the same factory experience,
"[T]he writer was finding values in the piecework system other than economic
ones. He struggled to attain quota 'for the hell of it,' because it was
a 'little game' and 'keeps me from being bored.'" (510). Echoing Caillois's
(1961) insistence that the outcome of play must be uncertain, both Burawoy
and Roy note that making out was only enjoyable if the end result of the
day's game was unknown. Most importantly, when the worker makes time
pass faster, the labor loses its alienated quality - it becomes engrossing.
The literature on dirty work has ignored the connection between
meaning, identity, and the involvement of the "I." In Hood's (1988) study
of custodial work, satisfaction and dissatisfaction with one's job are determined
primarily by the presence or absence of outsiders. On the night shift,
workers can have a sense of control and ownership of their assigned areas.
During the day shift, workers are continually reminded (through interactions
with the building's tenants) that they are low-status workers cleaning
other people's messes. Control and ownership is also the focus of Perry's
(1978) ethnography of garbage collectors. Through owning shares in their
company, workers have a sense of belonging to a collective and working for something greater. Conversely, in Rosecrance's (1985) study of backstretch
workers at horse-racing tracks, workers fail to form community relations,
and the stigma of their dirty labor is not successfully neutralized. The research
on degraded occupations is summarized by Ashforth and Kreiner
(1999), who argue that strong work cultures are the result of cognitive efforts
to subvert outsider's designations. As demonstrated by Thompson's
and Harred's (1992) topless dancers, dirty workers are socialized into an
occupational ideology that negates and ignores condemnation.
While the literature on dirty work has tended not to engage the affective
elements of culture, some passages in Perry's (1978) work note the
importance of novel situations.

</file>

<file= AmE06_J25>

Research on the educational enrollment of immigrants has typically asserted that today's immi
grant children are educationally disadvantaged and that earlier waves of immigrants were
more readily absorbed into the American educational system. This article addresses these
assumptions, drawing on traditional assimilationist and status competition approaches to
racial and ethnic stratification. Data from the 1920 IPUMS census database are used to ana
lyze the school participation of 15 to 18 year olds who were living in their parents' homes. The
findings demonstrate that the process of assimilation was not uniform for all groups. Some
groups achieved parity with the native stock by the third generation, and others took at least
an additional generation or experienced declines in the proportions who were enrolled in
school. In general, the results suggest strong parallels between the educational experiences of
white ethnic youths in 1920 and those of today's immigrant youths.

Throughout its history, the educational
system in America has shaped the life
time opportunities of immigrant chil
dren and youths. However, much of the cur
rent research on schooling has asserted that
today's immigrants are disadvantaged com
pared to foreign-born youths who arrived in
the United States at the turn of the century
(Fernandez, Paulson, and Hirano-Nakanishi
1989; Gans 1992; Portes and Zhou 1993).
Such studies have generally assumed that
early 20th-century immigrants accepted the
importance of schooling within one or two
generations?demonstrating "the time-hon
ored portrayal of growing acculturation and
parallel integration into the white middle
class" (Portes and Zhou 1993:82). In con
trast, much of the contemporary literature
has argued that today's immigrants often
experience downward mobility, termed "sec
ond-generation decline" or "segmented
assimilation" (Gans 1992; Massey 1995;
Portes and Zhou 1993; Zhou 1997).
Transformations in the economy may have
affected educational incorporation and, cor
respondingly, general levels of success.
Whereas today's technological society places
a premium on highly skilled and educated
workers, the industrializing America of the
early 20th century provided jobs for numer
ous unskilled workers. Still, many parallels
exist between immigrants who arrived in the
early and concluding years of the 20th centu
ry. Immigrant and second-generation chil
dren historically lived in families that experi
enced economic hardship and poverty
(Haines 1981; Perlmann 1988; Sassler 1995).
Schools in large cities were frequently over
crowded and had few resources (Jacobs and
Greene 1994; Meyer et al. 1979; Perlmann
1988). Furthermore, while some immigrant
groups welcomed schooling as a means of improving their children's life opportunities,
many parents worried about their children's
exposure to the negative influences of school
or expressed concern that school officials
were impinging on their parental authority
(Kessner 1977; Zelizer 1985). As a result,
immigrant children's adaptation to life in
America may not have encompassed enroll
ment in school to the extent that contempo
rary scholars of education have assumed.
Whether the descendants of immigrants
who arrived at the early 20th century
remained disadvantaged in the final years of
the 20th century and how more recent immigrants have fared in this competition contin
ue to be widely debated (Alba, Lutz, and
Vesselinov 2001; Borjas 1999). Immigrant
youths' experiences in the early years of the
century demonstrate patterns of ethnic
group adaptation to life in the United States
that were far from uniform, whether in terms
of school enrollment (Jacobs and Greene
1994; Lieberson 1980; Perlmann 1988) or the
combination of education and employment
(Horan and Hargis 1991; Sassier 1995;
Walters and james 1992). In the interest of
expanding contemporary studies of segment
ed assimilation, this article examines the link
among the generational status, ethnicity, and
secondary school attendance of working-age
young adults in 1920. That time is an ideal
one to situate such a study, since it followed
several decades of exceedingly high immigra
tion to the United States, along with the
expansion of schooling, and preceded the
restrictions on immigration that were legislat
ed in the 1920s.
This analysis differs from prior research in
several important ways. First, whereas con
temporary studies have been limited to two
generations (Hirschman 2001; Rumbaut
1994) and historical studies have examined
school enrollment among at most three generations (Jacobs and Greene 1994; Perlmann
1988), the unique data from the Integrated
Public Use Microdata Series (IPUMS) census
database allowed me to construct four generations of white ethnics.1 Second, I explore
how children of mixed-ancestry fared in the
educational market. Although intermarriage
is assumed to be the final stage in the assimilation process (Gordon 1964), historical stud
?es of the outcomes of the offspring of inter-marriages have been rare. I conclude by dis
cussing how the results of historical studies of
school participation may shed light on contemporary immigrant youths' patterns of
adaptation to life in the United States.
THE CHANGING CONTEXT OF SCHOOL ENROLLMENT.
Two important factors helped shaped the
environment of the American educational sys
tem in the early decades of the 20th century.
The first was the large and steady stream of
foreign-born persons who arrived in the
United States. As of 1920,13.2 percent of the
population of the United States was foreign
born (Gibson and Lennon 1999), and even
larger shares of Americans had at least one
foreign-born parent. Immigrant origins shift
ed in the late 19th century. In the middle of
the 19th century, western and northern
Europeans, including more than 5 million
Irish and Germans, dominated the influx to
the United States. By 1920, a substantial pro
portion of these "old" immigrants were second- or third-generation U.S. residents.
Despite continued immigration from these
countries, by the end of the 19th century, the
bulk of immigrants arrived from southern,
eastern, and central Europe. These "new"
immigrants differed sharply from the "old"
immigrants and their descendants in their
rates of literacy, ability to speak English, and
familiarity with the skills that were needed in
an urban environment (Kessner 1977;
Lieberson 1980).
Second, in response to the influx of immigrants and the children they brought with
them or bore in the United States, school
became one mechanism to encourage immigrant youths to acculturate and eventually
assimilate. Secondary school enrollment
expanded rapidly in the early decades of the
20th century (Walters 1984). Nonetheless,
high school graduation remained a relatively
rare event. In most states, school attendance
was compulsory only through age 15 (Jacobs
and Greene 1994). As a result, the shares of
young adults who were enrolled in school declined with age, and less than one-third of
young adults aged 14-17 (31.2 percent)
were enrolled in either public or private
school during the 1919-20 academic year
(U.S. Department of Education 1999). Of this
select group, even fewer actually graduated;
only 231,000 young adults received high
school diplomas in 1920 (U.S. Department of
Education 1998). For most young adults, a
year or two of high school sufficed.
declined with age, and less than one-third of
young adults aged 14-17 (31.2 percent)
were enrolled in either public or private
school during the 1919-20 academic year
(U.S. Department of Education 1999). Of this
select group, even fewer actually graduated;
only 231,000 young adults received high
school diplomas in 1920 (U.S. Department of
Education 1998). For most young adults, a
year or two of high school sufficed. 
Immigrants, Educational Prospects, and Mobility.
A sharp disconnect exists between research
on the educational participation of immigrant
youths and historical studies of the expansion
of school enrollment, partly because of differences in areas of specialization, with sociolo
gists of education and scholars of immigration drawing from distinct theoretical tradi
tions (Hallinan 2000). Indeed, historical
research on the expansion of school enrollment, particularly that dealing with ethnic
variation in primary and secondary school
education, has tended to rely on functionalist,
class-conflict, or status-competition perspectives (e.g., Bowles and Gintis 1976; Collins
1971; Fuller 1983; Meyer et al. 1979; Walters
1984). In contrast, immigration scholars have
drawn on the assimilation paradigm when
examining ethnic-group adaptation, in general, and educational participation, in particu
lar (Hirschman 2001; Jacobs and Greene
1994; Kao and Tienda 1995).
The assimilation perspective is concerned
primarily with how the foreign born shed
their distinctive behaviors and come to
resemble the native-stock population in terms
of educational and occupational attainment,
to name but two key outcomes of interest.
The central premise is that with increasing
duration in the United States and familiarity
with behaviors that are common among
long-term residents, the foreign born and
their descendants will eventually come to
resemble this population and shed the distinguishing traits of ethnicity (Gordon 1964;
Park and Burgess 1969). Numerous studies of
early 20th-century immigrants have asserted
that ethnic differences declined with each
succeeding generation in the United States (Alba 1985; Jacobs and Greene 1994;
Lieberson and Waters 1988). According to
this "straight-line" conceptualization, the
pursuits of first-generation immigrants differ
in important ways from those of longer-term
U.S. residents. Such distinctions should narrow by the second generation. The process of
assimilation is assumed to be complete by the
third generation. It should come as no surprise, then, that the key focus of immigration
scholars has been on the duration of native
ethnic distinctions.
Contemporary scholars of immigration
have challenged the standard assimilation paradigm and its unilinear conception of adapta
tion. Although straight-line assimilation may
have been theoretically appropriate for
explaining the adaptation process of newcomers who arrived in America in the early 20th
century, they have suggested, American society offers vastly different prospects for mobility
to today's immigrants (Gans 1992; Portes and
Zhou 1993; Rumbaut 1994; Zhou 1997). The
American landscape now consists of various
separate, segregated, and unequal sectors
(Massey 1995; Massey and Dent?n 1993;
Portes and MacLeod 1996). There is a wider
array of possible avenues for acculturation and
assimilation. The educational prospects of
young adults in disadvantaged central cities,
for example, should hypothetical^ differ from
those of young adults in affluent suburbs. In
addition to the traditional monotonie progres
sion of the classic straight-line assimilation
framework, then, today's immigrants also face
the distinct possibility of experiencing down
ward mobility (Gans 1992). In such an
instance, the second generation does not sur
pass the achievements of their foreign-born
counterparts.2 Contemporary scholars who
have explored why different patterns of adaptation emerged among late-20th-century
immigrants have labeled this theoretical
approach "segmented assimilation" (Portes
and Zhou 1993; Zhou 1997).
It is not surprising that the segmented
assimilation perspective focuses more heavily
on those who fail to achieve mobility or those
who experience downward mobility, rather
than on the usual success stories. Immigrants
who fail to achieve status mobility, it appears,
have fewer familial and individual resources and lower levels of human capital and are res
idential^ concentrated in poor areas where
youths have low educational and mobility
aspirations (Hirschman 2001; McKeever and
Klineberg 1999; Neckerman, Carter, and Lee
1999; Portes and Zhou 1993). Evidence of
downward mobility is reflected in the lack of
educational progression of today's racial
minorities (Hirschman 2001; Portes and
MacLeod 1996; Waters 1999), although support for segmented assimilation has been
mixed, at best (Glick and White 2003;
Hirschman 2001).
Less well appreciated is that educational
mobility is also a function of the structure of
educational opportunity and educational
access - a point that immigration researchers
have tended to overlook, despite its centrality
to educational scholars (Hallinan 2000).
Educational historians, in particular, have
focused on whether school enrollment
expanded because industrial elites sought to
inculcate working-class children with the values that were needed for success as factory
workers (Bowles and Gintis 1976), as a
response to technological innovation that
required increasingly skilled workers (Fuller
1983; Meyer et al. 1979), or as particular
groups sought to maintain their status advantage over others (Collins 1971). Even though
the connection between schooling and
assimilation has been a central concern of
educational historians (Bowles and Gintis
1976; Ralph and Rubinson 1980; Rubinson
1986), these studies have overlooked how the
impact of ethnicity may change across the
generations.
The historical literature contains abundant
testimony to the sacrifices that immigrant
families made to send their children to
school, suggesting that education was seen
as a way to improve employment opportunities.  
Nonetheless, empirical support for a
functionalist explanation of school expansion
has been scant. Furthermore, the data at
hand have allowed researchers only to infer
reasons for changes in immigrant youths'
school enrollment.

</file>

<file= AmE06_J26>

Facing AIDS: Denial, Indifference, and Fear.
First Signs.
The second South African epidemic emerged slowly, like water seeping languidly to the surface. Raziya Bobat, a pediatrician born in Durban in 1954 and trained in its "nonwhite" medical school, encountered her first AIDS patient in 1987 at King Edward VIII Hospital at about the same time as the first Black cases appeared in the Transvaal. Only recently aware that AIDS existed in South Africa, she had just begun reading about the disease, when one of her colleagues alerted her to an anomalous case: 
She was an African child about five years old; 99 percent of our patients are Black African. Our specialist had looked at the skin and said that there was something strange about this child; and the more she looked at her, the more she seemed to fit this description that everybody was talking about, about HIV. Of course, then all of us looked at this kid; then we took out the books and we read about it. And everybody sort of said, "Yeah, this certainly seems like it." 
It was also in 1987 that James McIntyre, then 31 years old and working on the vast campus of Baragwanath Hospital, serving the populous Black township of Soweto, had to grapple with a deeply disturbing piece of news. Already aware, as a gay man, of the extent of infection in his own community, he now faced laboratory evidence that HIV was making its presence felt in the impoverished populations he treated.
The South African Blood Transfusion Service brought in their test kits and decided to try them on some stored blood specimens that they had from us and found, to their shock and horror, that some of them were positive for the HIV antibody. They didn't know what to do because there was no patient consent. There had been no discussion with anybody at that stage. Worried about the women, we then called them back in. 
His colleague in pediatrics, Glenda Gray, saw her first HIV case, a Black child, a year later. Slowly she was discovering AIDS, case by case, in her township patients. "We started seeing a few, occasionally," she said. "But, basically, it was quite rare at that stage." Also at Baragwanath in those years was another politically committed doctor, Saul Johnson, who was involved with the ANC underground in the late 1980s and remembered a trickle of HIV cases and a flood of other diseases in Soweto. "I saw very little HIV disease. I think we were starting to see a little bit of pediatric disease, because the pediatric disease clinically pre-dates the adult disease."
It was not easy to identify those first cases. Although AIDS had been treated in South African gay men for over five years, most doctors, given their specialties, the segregation of hospitals by race, and their avoidance of the gay epidemic, had no experience with the disease. Henry Sunpath, who would eventually assume a leadership role in the care of AIDS patients at Durban's McCord Hospital, was still a young doctor, just starting out at Durban's public R. K. Khan Hospital. He was candid about how little he knew then: 
Initially, we began seeing a few cases: young people with strokes. We began seeing people wasting, people with chronic diarrhea, and a few people with myocarditis or with cardiac failure. And in our investigations of these patients, having done everything else, we came to the diagnosis of HIV by exclusion. 
Even clinicians with years of professional experience were stymied. Jerry Coovadia, already a seasoned pediatrician at King Edward VIII, recalled that "in 1988 we began to see these kids, and we didn't know how to diagnose them; the serology was there, but for babies it was no good."
Reflecting the pace at which cases emerged in the epidemic's first years, doctors acknowledged into the mid-1990s their difficulty in recognizing AIDS. Baffled by clinical symptoms that should have responded to conventional interventions, they began to discover that the manifestations of HIV disease could take many forms. Lucky Ndokweni, who had trained at the University of Natal and had specialized in orthopedics, struggled to find a diagnosis before he finally identified his first AIDS case in 1993: 
That case was a middle-aged male, about 30, who was thrown from pillar to post through all the departments of the hospital. We investigated that person in orthopedics because the presenting complaint was stiffness in the lower back and difficulty in mobility. I never seemed to come up with a reasonable diagnosis. He improved, and we subsequently discharged him, but he came back again. I then decided to take help from my consultant, and we both worked him up. Then, out of the blue, my consultant said, "Why don't we investigate this person for HIV?" And when we investigated, that it was. That it was. 
In the large crowded wards of public hospitals, where patients suffering from communicable diseases were common, it was all too easy for doctors to fail to recognize that the typical infectious illnesses they had seen for years were now the consequence of an underlying immune disorder - until ordinary disease patterns changed dramatically. Pinky Ngcakani, now a private practitioner, was a 26-year-old intern in 1994 in Port Elizabeth, an industrial and commercial city in the Eastern Cape, when anomalous cases started to accumulate in the wards. "We would see a lot of ordinary cases, but they were just too bizarre. We'd see a lot of chest infections coming out of nowhere. We knew that TB was endemic, but it was getting out of proportion." As the number of "ordinary" patients with diseases that eluded cure proliferated, Ngcakani and her colleagues began to ask them for consent to test for HIV. To her amazement, about 8 percent of those tested were already positive.
Even those who had begun to develop experience in diagnosing AIDS could still find it difficult. Tugela Ferry, approximately 140 miles northwest of Durban, is a desperately poor, dry, densely populated rural settlement studded with traditional, round, thatched-roofed, single-roomed huts. There, François Eksteen, a devoutly Christian doctor who traces his ancestry to a 1740 Dutch immigrant to South Africa, headed the pediatric services at the Church of Scotland Hospital. Although by 1995 the hospital had already identified a few patients with AIDS, it had not yet faced a full-scale epidemic among its patients. Eksteen knew to be alert to peculiar cases where illnesses proved resistant to treatment.
You often realize that something more is wrong with the child. It might be marasmus or kwashiorkor, with clinical symptoms that are associated with severe malnutrition, and you realize the child is not responding to the usual approaches. You diagnose tuberculosis, but you realize that this child is not doing well. Eventually we would do the HIV test, and it would come back positive. And regularly what we saw was that the mother was still quite healthy, but the baby was already quite sick. 
Having discovered cases of HIV infection in heterosexual men and women and in young children, doctors began to realize that other cases might have passed unnoticed. What meager statistics were assembled by those who first encountered AIDS were almost certainly an undercount. In ruminating on her first HIV case, Glenda Gray concluded, "We probably had been seeing a lot of younger children who died of opportunistic pneumonias which we didn't even think was HIV. We probably had missed a whole lot of the rapid progressors, because we just weren't thinking of it."
Once the flow of patients increased, sooner in some regions than others, doctors began to recognize new cases more readily and became more sensitive to the gravity of their situation. In 1992, Haroon Saloojee, then a 31-year-old politically engaged pediatrician, started his own informal surveillance of newborns at Baragwanath.
We were suddenly beginning to see more kids dying. Obviously at that stage we weren't doing a lot of testing. It wasn't routine to test every child who potentially had HIV/AIDS. We began to recognize this syndrome: there was this group of children with lymphadenopathy who looked malnourished and obviously had much more serious illnesses than other children. Some of us were already beginning to say that we were not doing enough about this problem. 
Epidemic Takeoff.
Saloojee was unusual in that he had a sense of foreboding. "There's a doomsday scenario in ten years' time; now is an opportunity." Such concerns were shared by those who had worked abroad and who had come face to face with AIDS in countries already ravaged by the epidemic.
In the first years of the 1980s, Robin Wood, a British-born, Oxford-trained physician who, a decade later, would work at Somerset Hospital with Frank Spracklen treating gay men with AIDS in Cape Town, ran a small clinic in Lusaka, Zambia. At that time, he was responsible as a doctor for "groups participating in game hunting and safaris in the far east of the country, treating crocodile bites and leopard maulings." There, he began to see men dying soon after he diagnosed them with oral candida or wasting syndrome. He and his colleagues tried to make sense of this new phenomenon, in the process following the trajectory of hypotheses proposed by researchers in the United States for what appeared to be a similar disease.
We would be reading the journals and the information coming out of the States, and we would look at each other and say, "They think it's due to poppers, inhalation of amyl nitrite, by gay men," and we would look at each other and shake our heads and say, "No, that's not right." Next, they thought anal injection of semen was immunosuppressive. Eventually someone put forward that it looked like hepatitis B and was transmitted sexually, and that sounded as though it could be right. But we had no concept of the underlying pathology. 
Some years later, in 1987, Clarence Mini, who would return to South Africa only after the apartheid government had lifted the ban on the African National Congress, was working as a doctor in a hospital in Harare, Zimbabwe, treating the military forces of the ANC. There he was mentored by an Indian professor of medicine with an avid interest in HIV and Kaposi's sarcoma, the cancer that is one of the hallmarks of full-blown AIDS. "He would go around taking biopsies of all these funny skin lesions, and at the time it was something new to us. He was doing research, so he was really forcing all of us who were working there at the time to follow those cases." In addition, Mini and his colleagues were ordered by his professor to test anybody diagnosed with shingles for HIV infection. Testing occurred, of course, in the absence of effective treatment.
In time, Mini began to realize that the wasting he now recognized in Zimbabwe he had previously seen in Lusaka among ANC cadres passing through from military training in Angola to underground warfare in South Africa. There, he had been puzzled by the paradox of significant weight loss and tuberculosis in these well-fed young fighters. With alarm and concern, he concluded these men were viral carriers, bringing the disease with them as they crossed borders.
We were saying to ourselves, now these guys are in transit, these guys are going home, and at some stage, if they are going to meet up with girls, this virus is going to spread. When we get home, we would say to ourselves, we just have to immerse ourselves in this and not have it devastating our country like in Zimbabwe. 
A few others, like James McIntyre, already foresaw the epidemic's potential sweep. As a Zimbabwean and a gay man with personal and professional ties to Steven Miller, Dennis Sifris, and others involved in gay HIV treatment and organizations, he was more aware of the epidemic forecasts being made at home and overseas. By the early to mid-1990s, McIntyre recalled, he was predicting that the HIV prevalence rate in South Africa would peak at 30 percent of the population.

</file>

<file= AmE06_J27>

Eye movement desensitization and reprocessing (EMDR) is gaining
acceptance as efficacious treatment for posttraumatic stress disorder
for individuals but not for couples. This article reports three case
studies of couples in which EMDR is combined with Gestalt therapy
in a single session to resolve relational trauma effects, increase empathy
and awareness in the supportive partner, and deepen intimacy
within the couple. Case studies are described, and implications for
research and clinical applications are discussed.
The purpose of this article is to report the findings of three
case studies of couples counseling in which eye movement
desensitization and reprocessing (EMDR) is used to
achieve three goals: (a) to resolve the effects of trauma for the
traumatized partner (TP) that result from relational trauma
previously inflicted by the supportive partner (SP) during the
course of the relationship, (b) to increase empathy in the SP
by increasing cognitive awareness of the other partner's
trauma effects, and (c) to deepen intimacy within the couple.
Furthermore, this article demonstrates that by combining
EMDR with Gestalt techniques in an experiential couples
counseling context, it is possible to achieve treatment goals in
a single counseling session.
BACKGROUND.
Following a brief but stormy history, EMDR is now
accepted as an efficacious treatment for posttraumatic stress
disorder (PTSD; American Psychiatric Association, 2004;
Chambless et al., 1998; Chemtob, Tolin, van der Kolk, &
Pitman, 2000; Foa, Keane, & Friedman, 2000; Foa &
Rothbaum, 1998; Shapiro, 2001; Veterans Health Administration
[VHA] & Department of Defense [DOD], 2004).
Carole Alto (2001) selected 21 controlled research studies of
EMDR for inclusion in her meta-analysis. A cursory perusal
of the Digital Dissertations database via the Internet yielded
15 doctoral dissertations in which EMDR was the focus of
controlled experiments. Although effect sizes varied among
the 36 studies, all of them indicated efficacy in the use of
EMDR.
The origin of EMDR (Shapiro, 1989a, 1989b) has been a
source of controversy as has the role of eye movements
(Acierno, Tremont, Last, & Montgomery, 1994; Levin,
Lazrove, & van der Kolk, 1999; Rosen, 1995, 1997; Welch,
1996). In the first comprehensive text on EMDR, Shapiro
(1995) claimed that more than 10,000 therapists had been
trained in the method. Although Shapiro continued to advocate
for her proprietary training, she published details on the
protocol that could be followed by non-EMDR-trained
researchers. Shapiro's early reluctance to divulge details of
her method may have added to the controversy. Some early
studies failed to adhere to the EMDR protocol (Devilly,
Spence, & Rapee, 1998; Merckelbach, Hogervorst,
Kampman, & de Jongh, 1994; Muris & Merckelbach, 1997;
Muris, Merckelbach, Holdrinet, & Sijsenaar, 1998; Sanderson&
Carpenter, 1992). These studies were less than enthusiastic
about EMDR. Chastising Shapiro for making replication
of her research difficult was a common theme among
early EMDR investigators (Herbert & Mueser, 1992). Many
of these authors live and work abroad, which may have created
logistical problems in receiving EMDR training. In any
case, it seems likely that methodological problems
contributed to negative reports by these researchers.
Maxfield and Hyer (2002) found that methodological
standards were highly correlated to effect size. McNally
(1999) summarized the elements of the controversy in a particularly
unflattering analysis of EMDR. However, a substantive
majority of the more than 330 articles that have been published on EMDR since 1989 (Baldwin, 2004) have supported EMDR for PTSD treatment and show promise for EMDR's
efficacy in the treatment of other problems, as well (Maxfield
& Hyer, 2002; Perkins & Rouanzoin, 2002; Shapiro, 2002;
Shapiro & Forrest, 1997). In addition, physiological measures
such as neuroimaging have aided in the understanding
of the role of eye movements in EMDR (Levin et al., 1999;
van der Kolk, 2002).
Since 1989, EMDR has been used to treat survivors and
emergency workers, including disaster relief counselors,
worldwide. EMDR has been used effectively in Oklahoma
City, Belfast, Zagreb, Rwanda, Dunblane, Sarajevo, Columbine,
and Londonderry; with veterans fromWorldWar II, the
KoreanWar, Beirut, and the VietnamWar; and in many other
applications (Shapiro, 2002; Silver&Rogers, 2002). EMDR
effects exceed those of nonspecific effects shared by all treatments
and are independent of client expectations (Perkins &
Rouanzoin, 2002;VanEtten&Taylor, 1998).Moreover,EMDR
effects are at least equal to effects of cognitive-behavioral
therapy, and EMDR often requires less time than other models,
with less client attrition (Silver & Rogers, 2002; VHA &
DOD, 2004). Shapiro has continued to advocate for proprietary
training. Tobin (2004) recently reported that more than
40,000 practitioners have been trained in Shapiro's model.
Couples Counseling.
Conjoint therapy is a time-honored clinical practice
(Bowen, 1978; Pesso, 1996; Roth&Chasin, 1994). However,
EMDRwas not widely studied in conjoint therapy prior to the
21st century. One reason for the dearth of early couples
research may be because of Shapiro's caution toEMDRclinicians
about using EMDR in the context of couples therapy
(Shapiro, 1995, 2001). Shapiro noted mixed results and indicated
the key in deciding whether to use EMDR in a conjoint
session was the level of commitment to support by the SP.
However, Shapiro did not cite the research on which her position
was based. EMDR therapists have reported therapeutic
gains with the use of EMDR in couples therapy (Flemke &
Protinsky, 2003; Protinsky, Sparks, & Flemke, 2001; Snyder,
1996). Moses (2003) advocated for the use of EMDR in couples
counseling in a model that promotes safety and balance.
Protinsky et al. (2001) reported thatEMDRfits within experientially
based treatment and believe that it can increase therapeutic
effectiveness. More recently, Flemke and Protinsky
(2003) successfully integrated EMDR with Imago Relationship
Therapy. Snyder (1996) reported therapeutic gains in
increasing intimacy with a lesbian couple in conjoint sessions
using EMDR with one partner, and the partner not receiving
EMDR provided emotional support in alternate sessions.
Role of Gestalt Therapy.
Experiential therapy has been shown to be effective in
symptom reduction in depression (Greenberg & Watson,
1998). This factor is believed to play an important therapeutic
role in couples counseling because major depressive episodes
and dysthymia are often associated with dyadic dissatisfaction
(Whisman, 1999). Gestalt techniques in conjoint therapy
afford the SP the opportunity to gain visceral awareness of the
other's pain and to experience the trauma resolution of the TP
at a deep level, thereby gaining awareness and empathy for
the partner. Experiential techniques, borrowing heavily from
Gestalt therapy in which "experiment is the cornerstone"
(Zinker, 1977, p. 123), have been used effectively in dealing
with both trauma and "attachment injuries" in couples counseling
(Greenberg & Malcolm, 2002; S. M. Johnson, 2003;
S. M. Johnson, Makinen, & Millikin, 2001). In keeping with
traditional experiential family therapy, the goal of integrating
EMDR in couples counseling with Gestalt techniques is
growth and integrity, that is, congruence between inner experience
and outward behavior (Nichols & Schwartz, 1991).
Definitions.
Because of the novelty of EMDR therapy, many terms
associated with the model may not be immediately clear to
the reader. In addition, this article contains terms and acronyms
that might be obscure. The following list of operational
definitions and explanations is provided to assist the reader
who is unfamiliar with EMDR.
Attachment injury. Attachment injury is a traumatic relational
event that occurs early in life and tends to shape relatively
enduring relational patterns. The traumatic event itself
is typically one that is not outside the range of normal human
experience and therefore is not usually associated with a diagnosis
of PTSD. It can be a single incident or a series of similar
events, described by Shapiro (2001) as "small t traumas."
Balance. This is a therapeutic technique in which both
partners receive equivalent treatment to minimize the perception
that one is "the patient" or has "more issues" than the
other (Moses, 2003; Weeks & Treat, 2001).
Body scan. A body scan is a procedure in which the client
is invited to notice any physiological sensations and to
describe them to the counselor (Shapiro, 2001).
Containment. Containment refers to the handling of
charged material within and between sessions. It includes
assessment, development, and installation of internal and
external resources of the partners; placement and coaching of
the SP; and being reasonably available by telephone between
sessions when necessary (Moses, 2003).
Desensitization. This refers to the implementation of sets
of eye movements while focusing on the disturbing material.
Between sets, the counselor instructs the client to breathe, to
"let it go," and asks the client what he or she is experiencing.
This procedure is the heart of the model of EMDR and is
repeated until the disturbance is removed or minimized and
the positive cognition is strongly endorsed by the client
(Shapiro, 2001).
Installation. Installation is the use of additional eye movement
sets to reinforce the endorsement of the positive cognition
(Shapiro, 2001).
Relational trauma. Relational trauma is operationally
defined as any physical, psychological, or emotional insult
that is perpetrated by one partner in a primary dyad toward the
other partner.
Safety. In response to risks of increased vulnerability as
the result of conjoint counseling, safety refers to assessing
client stability and fidelity to a clear structure with both partners
and the counselor committed to the process (Moses,
2003).
Supportive partner. SP is operationally defined as the former
perpetrator of relational trauma whose aberrant behavior
is in full remission and is seeking to mend the primary
relationship.
Traumatized partner. TP is operationally defined as a survivor
of relational trauma who is currently manifesting posttraumatic
stress symptoms and is seeking to mend the primary
relationship.
RATIONALE
The treatment method under discussion is believed to have
wide applicability to diverse populations. EMDR has been
effectively used in resolving trauma in the aftermath of wars,
terrorist acts, and natural disasters worldwide for individuals
from diverse populations (Silver & Rogers, 2002). An assumption
of this study is that the diverse applicability for individuals
will extend to couples. There is a fairly large degree of
diversity of ethnicity, gender, and age in the three cases
currently discussed (see "Methods").
At least one previous case study reports that conjoint couples
EMDR therapy may be effective in increasing intimacy
in a lesbian primary relationship (Snyder, 1996). However,
the three cases in this study involve heterosexual couples in
which the TPs are female. It is assumed, based on controlled
research, that traumatized males and females benefit equally
from EMDR in the treatment of trauma effects (VHA &
DOD, 2004). However, females may be more likely to report
trauma effects in couples counseling. This assumption is
based on the wide body of literature that demonstrates that
substance abuse is more common in males (Substance Abuse
and Mental Health Services Administration, 2004) and that
perpetrators of intimate partner violence are 5 times more
likely to be male (Rennison &Welchans, 2000). In addition,
at least one recent study indicates that females are more likely
to be traumatized than males (Breslau, Wilcox, Storr, Lucia,
& Anthony, 2004). Based on Snyder's (1996) methodology,
the work of Flemke and Protinsky and their colleagues, and
the universal applicability ofEMDR(Silver&Rogers, 2002),
it seems likely that attachment injuries and relational trauma present barriers to intimacy in primary dyads without regard
to gender or worldview of the couple.
Gestalt therapy has long been used with success in the
treatment of substance abuse (V. Johnson, 1973; Ramey,
1998). Gestalt techniques, such as empty chair and making
the rounds, have been shown to be useful in helping substance
abusers attain visceral awareness that leads to congruence.
Both Alcoholics Anonymous (AA) and Al-Anon Family
Groups use the 12 steps, in which the 4th and 5th steps
embrace experiential therapy techniques (Anonymous,
2001). Unfortunately, experiential techniques that are geared
toward the substance abuser do not address the experiences of
the family in relation to the substance abuse. Edwards (1990)
stated that individual therapy techniques cannot address any
systemic experience unless they are utilized in the context of
couples or family therapy and approached systemically by the
therapist. S. M. Johnson, Hunsley, Greenberg, and Schindler
(1999) reported that experiential therapies, such as emotionally
focused couples therapy (EFT), utilize Gestalt techniques
in encouraging a visceral reaction that tends to resolve
unfinished business (Greenberg & Malcolm, 2002). EFT utilizes
techniques that lead to deepening of feelings to promote
affinity (S. M. Johnson, 2003).

</file>

<file= AmE06_J28>

Most of the chapters in this volume were presented as papers at a small
research conference held in 2001 at the Kellogg School of Management
of Northwestern University in Evanston, Illinois. The purpose of this conference
was to explore new ideas about the psychology of leadership, an
important and long-enduring research topic within the field of social psychology.
It was the opinion of the editors of this book and the conveners
of the conference that the social psychological study of leadership had
launched off into several new, interesting, and important directions. It
was also our belief that interest in the topic, within both social and organizational
psychology as well as within the business community, had
grown rapidly. It was an ideal time, therefore, to ask some of the world's
leading scholars to come together to describe their thinking and research.
This book is the result of those efforts. The contributions span traditional
social psychological areas as well as organizational theory. They examine
leadership as a psychological process and leadership as afforded by 
organizational constraints and opportunities. Our goal has not been to
focus the chapters on a single approach to the study and conceptualization
of leadership but rather to display the diversity of issues that surround
the topic.
Leadership scholars have identified a host of approaches to the study of
leadership. What are the personal characteristics of leaders? What is the
nature of the relation between leaders and followers? Why do we perceive
some people to be better leaders than others? What are the circumstances
that evoke leadership qualities in people? Can leadership be taught? And
so on. The contributions to this book examine these important questions
and fall into three rather coherent categories. Part I concerns conceptions
of leadership. How has leadership been defined? What are the social and
psychological processes that constitute leadership? There are four chapters
that fall within this category.
Part II includes contributions dealing with factors that influence the
effectiveness of leadership. Some conditions make leadership relatively
unimportant, whereas others make good leadership essential. Some modes
of relating to other people enhance the effectiveness of leaders, whereas
others reduce the influence of leaders. This part of the book contains five
chapters.
Part III examines a less popular but essentially important topic in leadership
scholarship, namely the effects of being in a position of leadership
on the leader himself or herself. If we were to observe that leaders have
some qualities in common, it could either be that people with these qualities
ascend to positions of leadership, or that the position of power or influence
creates these qualities in whomever accepts the role. The arrow of
causality could point in either or both directions.
In chapter 2, Michelle Bligh and James Meindl examine the thousands
of books that are available on the topic of leadership. They ask if there
are some "natural" categories into which these titles fall. By coding these
legions of books by their characteristics, and using a "natural learning"
process for classification, they find that seven distinct categories of leadership
books emerge. These categories range from books about leading
change in organizations to books about leadership and religion. This vast
range of books not only signals the breadth of interests in the topic of
leadership, it also sets the stage nicely for the variety of approaches to
leadership that are offered in this book.
One of those approaches, and a rather modern one, is described by
Michael Hogg in the chapter 3. Hogg sees leadership as a relational concept,
as does Messick in chapter 4. However, Hogg's emphasis is on the fact that the leadership relationship often occurs in a group that has assumed
qualities and characteristics. Hogg's theory notes that many groups can be
thought of as having a "prototypical" member, someone who most embodies
the qualities of the group. This member will be perceived to be more
influential than others, will be liked more than others, and, partly as a
result, will be seen has having better leadership qualities than the other
members. This person will also have an edge in maintaining the perception
of leader over time. One interesting implication of this theory, an implication
that derives from the social identity theory of group psychology, is
that a person need not actually be more influential than others to be seen
as a leader. If one is prototypical, one may be better liked and seen as more
central than another, and be believed to be influential and charismatic.
This perception may then become a self-fulfilling prophecy; such a person
may actually derive more influence because of these perceptions. Hogg
guides the reader through some of the clever research literature that supports
these hypotheses.
Messick's relational theory is of a different sort; it asks why people
voluntarily become leaders and/or followers. Coming from more of an
interdependence perspective, Messick asks what the benefits are that are
afforded to both parts of this relationship. His theory identifies five dimensions
along which such benefits may be exchanged. Like Hogg's theory,
this is a relational theory, but it is one in which social identity plays only a
modest role. Instead, it highlights the important psychological benefits followers
gain from the relationship. In particular, Messick argues that followers
are often given vision, protection, and achievement by leaders. These
are among the task effectiveness dimensions that have been discussed by
past theorists. They are also given social inclusion and respect, qualities
that are subsumed by the traditional role of social-emotional leadership.
Leaders in return, get focus, loyalty, and commitment, respectively, from
their followers. They also get self-sacrifice and pride in the social domain.
The proposal by Messick is that the exchange is not a contractual quid pro
quo but rather an exchange that results from mundane social psychological
processes. From this view, leadership and followership are social roles
that emerge from everyday ordinary psychological activities.
The final chapter in Part I of the book is Goethals' reevaluation of
Freud's theory of leadership from the perspective of modern social psychological
theory. Although Freud has been largely dismissed by modern
psychologists, Goethals notes that aspects of his theory strike a modern
chord. He seems to predate the concept of charismatic leadership in some
of his descriptions, for instance. Moreover, his analysis seems to highlight the extent to which the leader exemplifies prototypical traits of the followers,
as emphasized by social identity theorists like Hogg. Leaders influence
followers through the stories that they tell, according to Freud, presaging
the approach to leadership taken by Howard Gardner in his book,
Leading Minds. Leaders' ideas, the ideas that can motivate and influence
people, are communicated by stories that delimit and expand the leaders'
vision, that communicate the "message" to the people who are the followers.
Finally, Goethals notes the "illusion of equal love," the perception that
all are the same in the eyes of the leader. This point is made again by Tyler
in a later chapter, although Tyler would argue that the equal and respectful
treatment of members of a group or organization should not be a mere
"illusion," it should be genuine to the extent possible. Goethals thus suggests
that Freud presaged the idea of charismatic leadership, highlighted
the role of storytelling as a form of communication, emphasized the common
social identity of leaders and their followers, and he glimpsed the
importance of what we now refer to as procedural justice in leadership.
The second part of this book deals with the conditions under which
leadership is more or less effective. What are the dimensions of effective
leadership? What do leaders attempt to promote among team members?
Are there better or worse ways of achieving these ends? Part II begins
with a chapter by Richard Hackman that calls into question the standard
research approach of many social psychologists and leadership researchers.
Hackman questions the assumption that excellent team performance is the
product of excellent leadership, an assumption he refers to as the "leader
attribution error." In chapter 6, Hackman reviews evidence that suggests
that leaders may provide the conditions under which teams may excel or
fail, but that these conditions should not be confused with "causes" in the
traditional social science sense of the word. Hackman then outlines four
conditions that tend to increase the chances that groups will function well.
These conditions include creating real (as opposed to bogus) teams, giving
the teams compelling directions in which to work, giving them an enabling
design (a structure that does not handicap them from the outset), and providing
expert coaching to help with the rough patches. Hackman not only
spells out and illustrates these points, he also discusses the timing of the
conditions. Perhaps his most original contribution is in noting that some
types of teams are so constrained that the quality of leadership is immaterial
to their performance. What difference does it make how well a plane's
flight crew works together if the plane is being flown on automatic pilot?
Chapter 7, by Peterson and Behfar, adopts the framework of selfregulation
to group functioning. These authors identify three conditions for successful group performance to balance the often-conflicting demands of
getting the problem right while maintaining group cohesion, maintaining
both group identity as well as recognition for the individuals involved, and
keeping the right mix of willingness to change and stability. These three
conditions are a sense of group self-awareness, having clear standards and
goals, and developing the willingness and the ability to make changes.
Peterson and Jackson make the intriguing proposal that leadership may
derive from a person's ability to help groups maintain these three functions.
Leaders, in other words, function as regulatory mechanisms that
aid groups in understanding themselves, in maintaining their goals and
their knowledge of where they are with regard to the achievement of these
goals (a feature highlighted in chapter 6 by Hackman), and in providing
the encouragement for and resources to enable change within the group.
This chapter not only overlaps nicely with the preceding and succeeding
chapters, it also provides a conceptual framework that allows the authors
to generate novel hypotheses about the functions of effective leadership.
Tyler (chapter 8) offers a theory of process based leadership, which
builds directly from his previous research on the social psychology of procedural
justice. At the heart of this important chapter is the core idea that
procedural fairness, more than positive outcomes, is the power that motivates
people to cooperate in groups, to refrain from disruptive behaviors,
and to work for a common collective good. To the extent that this characterization
is true, it has important implications for leadership because it
suggests that it may be more important for leaders to be fair and just in the
processes they adopt than it is for them to provide rich rewards and successes
for their members. This is precisely the picture that Tyler paints in
his chapter. Summarizing research from several prior studies, he marshals
evidence that people are more sensitive to the fairness of procedures than
to the favorability of their outcomes in determining their commitment to
organizations and in their willingness to follow rules and abide by group
principles. In places, the story that Tyler tells echoes the theory of Hogg in
highlighting social identity; in places it resembles Peterson and Jackson's
thoughts about self-regulation and the mechanisms that maintain it. But
Tyler probes into the sources of people's concerns with fair process and
concludes that the major source of this concern has to do with the ability to
construct and nurture a positive image of oneself. Pride and positive selfregard
seem to be the drivers of the system, and leaders who understand
the importance of this psychological need are likely to excel as leaders. 
One cannot be an effective leader unless one is in a position to exercise
leadership. This observation leads to the puzzling question raised by Bowles and McGinn, as to why it is, when the bulk of the research evidence
says that women are at least as good at being leaders as men, that
women hold proportionally fewer leadership roles in organizations than
men.

</file>

<file= AmE06_J29>

Introduction: Family Talk.
Families are the cradle of language, the original site of everyday discourse, and a touchstone for talk in other contexts.1 Families are created in part through talk: the daily management of a household, the intimate conversations that forge and maintain relationships, the site for the negotiation of values and beliefs. Yet there has been a greater focus on language in workplaces and other formal institutions than on discourse in this first institution. The chapters in this volume fill this gap in socio-linguistic research by bringing together a variety of linguistic studies based on a single set of data: the naturally occurring, face-to-face interactions of four American families. The studies emerged from a three-year socio-linguistic project carried out at Georgetown University to examine how parents in dual-income families use language to constitute their identities as parents and professionals at home and at work, as well as the interactional and social consequences of these ways of speaking. Since the workplace has received relatively more attention in studies of discourse, this volume focuses on the language the four women and four men use as they interact with one or more family members, with the addition of one father's social talk about family at work.
The volume contributes to the discourse analysis of naturally occurring interaction in general, and of family interaction in particular, while also making significant contributions to theories of framing in interaction and the linguistic creation of identity. Together, the chapters extend our knowledge of family discourse and identify new ways in which family members construct, negotiate, and enact the individual and collective identities that constitute a family. Furthermore, whereas the vast majority of work on family discourse focuses on mothers, several chapters consider the relatively understudied language of fathers. Likewise, a substantial amount of research on family discourse has been based on mealtime interactions recorded in the presence of researchers (Blum-Kulka 1997; Erickson 1982; Ochs & Taylor 1995; Ochs et al. 1992; Pontecorvo & Fasulo 1999; Paugh 2005). In contrast, the studies in this volume are based on interactions that occur in a wide range of settings, which more fully represent these families' experiences. As such, the chapters contribute to the sociolinguistic analysis of face-to-face interaction, some in areas that have been studied primarily through experimental observation, self-report surveys, and interviews. The data set is unique in discourse analysis because the audio recordings span a more extended period of time than previous studies, encompass a wider variety of contexts, and are relatively uncen-sored, as recordings were made with no researcher present. In addition, we, the researchers, were able to get to know the speakers well enough to place their utterances in context; and we have kept in contact with the families and have thus been able to observe how the families have changed during the seven years since the recordings were made. The volume is also unique in the fact that the same families are considered from multiple perspectives by different authors. This is especially notable in those instances in which different authors examine the same interactions for different purposes.
Three broad themes emerged in the authors' considerations of family discourse in these four families: the underlying dynamics of power and solidarity in the family context in general, and in the interactional framing of individual and shared family identities in particular; the negotiation of gendered identities in conjunction with family identities, especially in relation to the challenges dual-income couples face; and, finally, the complex discursive means through which family members actively assert, negotiate, and confirm their family's beliefs and values when children are present to create individual and shared family identities. These themes are explored further in the next section. In subsequent sections, I describe the genesis of the volume, explain the research methodology, introduce the families, and provide an overview of the chapters and the organization of the volume.
Themes in Family Discourse.
Interactional Dynamics: Framing, Power, and Solidarity.
The chapters in this volume use an interactional sociolinguistic approach, each drawing on research in one or more areas: linguistic anthropology and the ethnography of communication (e.g., Erickson 1975; Gumperz & Hymes 1972); discourse strategies (e.g., Becker 1995; Gumperz 1982; Tannen 2007 [1989]); dimensions of power and solidarity in interaction (Tannen 1994a); framing theory (e.g., Bateson 1972; Goffman 1981; Tannen 1994b; Tannen & Wallat 1987); and those frameworks similar to framing in their use of stance, alignment, and/or positioning (e.g., Davies & Harré 1990; Ochs 1993; Schiffrin 1996). Two of these frameworks in particular are fundamental to understanding language in interaction in general and family discourse in particular: the frameworks of power and solidarity and framing. In previous work, Tannen (1994a) develops a framework of the ambiguity and polysemy of power and solidarity, arguing that in studying interaction, scholars need to understand that power (or hierarchy, or control) and solidarity (or connection, or intimacy) are not separate, but inextricably intertwined. Tannen (2003) applies the model to family discourse to address the trend to focus exclusively on the power dimension of discourse in families at the expense of the equally influential dimension of intimacy and connection (Blum-Kulka 1997; Ervin-Tripp et al. 1984; Ochs & Taylor 1995; Varenne 1992; Watts 1991). She considers the role of "mother" to illustrate that both family relationships and gender identities within families are negotiated along the dimensions of both power and connection.
In her contribution to this volume (chapter 2), Tannen further demonstrates that family interaction is an ideal site for exploring the complex interrelationship between power and solidarity as relationships among family members are "fundamentally hierarchical" and also "intensely connected." She argues that discourse analysts must consider an utterance's 
potential for being both a power maneuver and connection maneuver simultaneously. Marinova (chapter 5) uses Tannen's framework to explain the challenges parents (in this case, a father) face in balancing connection and control as children get older. Although it is not the focus of her analysis, Johnston (chapter 7) uses Tannen's framework to reconcile a seeming contradiction between what some parents say and what they do. She suggests that parents may simultaneously serve as gatekeepers in domains such as caregiving and financial management while explicitly expressing egalitarian views for professional and parental role sharing because retaining control of these domains may fulfill family members' co-existing needs for connection and control.
Within interactional sociolinguistics, framing theory is equally crucial for understanding conversational dynamics in family discourse. In the first sociolinguistic volume on framing, Tannen (1993) demonstrates that framing theory is fundamental to linguistic discourse analysis and, conversely, that discourse analysis provides valuable insight into the linguistic means by which frames are actively created. Drawing on Bateson (1972), she explains that "no communication move, verbal or nonverbal, can be understood without reference to a metacommunicative message, or metamessage, about what is going on - that is, what frame of interpretation applies to the move" (Tannen 1993:3). In 1974, Goffman elucidated the levels and types of framing that constitute everyday interaction; and, in his later work (1981), he focused more specifically on language in examining "footings," which he used to capture the subtle shifts in framing and the multiple layers of framing in everyday life. Several chapters in this volume use a framing approach - drawing on the concepts of framing, footing, positioning, and alignment - to show how family members use particular linguistic resources to effect shifts in framing that, among other functions, contribute to family members' negotiations of shared family identities (Tannen, chapter 3), and the individual family identities of "mother" (Gordon, chapter 4) and "father" (Marinova, chapter 5).
Gender, Work, and Family.
The second theme in family discourse addressed in this volume is the discursive creation of gendered identities in the family, especially in relation to the links between work and family. Studies of gender in the family are surprisingly rare, given the significance of gender in this domain and the substantial literature on gender and language in other domains. As Kendall suggests elsewhere (2004a), the relative lack of research on gender in family discourse may be a reflex of the women's movement of the 1970s, when the study of gender in linguistics emerged (Lakoff 1975). In a sense, women were demanding choices besides marriage and motherhood, so the focus was on getting women out of the home, not back into it. For this reason, discourse analyses of gender have focused on women's discourse in the workplace (e.g., Ainsworth-Vaughn 1998; Kendall 2004b; Tannen 1994b, c; Kotthoff 1997; McElhinny 1993; West 1990). In addition, although sociologists, anthropologists, and feminists have long recognized the family as a social institution, the study of gender in the family has been excluded from research in this area. For example, Drew & Heritage (1992:59 fn. 1) explicitly exclude family discourse from institutional language: "Notwithstanding the standard sociological usage within which the family is also a social institution, we will avoid using the term to describe activities that would be glossed as family dinners, picnics, and the like."
Gender infiltrates nearly all the chapters in this volume but is fore-fronted in three chapters that focus on gender as an achieved identity in a social constructivist paradigm. Scholars have found that women and men create gendered identities by using discursive strategies that index socio-cultural expectations associated with gender as these individuals use language to accomplish other tasks (e.g., Ochs 1992; Tannen 1994b). Gordon (chapter 4) builds on this research and other work on language and identity to identify the discursive strategies through which one woman creates a gendered parental identity in a common domestic encounter: the babysitting report.
Other chapters bearing on gender contribute to our understanding of how language is used to negotiate, perpetuate, and mitigate challenges faced by dual-career families with children, particularly those families who have made a commitment to sharing childrearing responsibilities. The families participating in the research project represent a growing trend in the United States for both mothers and fathers to be employed outside the home (Clark 2002; Coontz 2000; Waite & Nielsen 2001). This change in the structuring of families raises the question of whether and how this shift influences families' daily lives and the gendered identities that mothers and fathers create both at home and in the workplace. These sociocultural factors provide the backdrop for the chapters by Kendall (chapter 6), Johnston (chapter 7), and Gordon, Tannen, and Sacknovitz (chapter 8). The chapters by Kendall and Johnston examine the interactions between spouses 
to consider how these women and men negotiate caregiving roles (Johnston) and caregiving and breadwinning roles (Kendall). Gordon et al. approach this question from the perspective of the workplace, examining how one man talks about family at work and relating the patterns to past findings on gender and social talk at work.
Family Values, Beliefs, and Shared Identities.
The third theme of family discourse in this volume is the linguistic negotiation of family values, beliefs, and identities. In linguistics and linguistic anthropology, the study of family discourse has appeared primarily in the work of researchers in child language acquisition, such as Shoshana Blum-Kulka (1997) and Elinor Ochs and her colleagues and students (e.g., Ochs & Taylor 1995). These and other scholars have considered how parents socialize children to embrace family values: points of view and beliefs that characterize a family (Liebes & Ribak 1991, 1992; Liebes et al. 1991; Ochs 1992, 1993; Ochs, Smith & Taylor 1996; Pontecorvo & Fasulo 1997, 1999; Ribak 1997; Tulviste et al. 2002). Research on family discourse has focused on the socialization of interactional patterns that vary by culture, such as narrative practices (e.g., Blum-Kulka 1997); and it has been based primarily on mealtime interaction (e.g., Ochs, Smith & Taylor 1996; Pontecorvo & Fasulo 1999; Paugh 2005). Three chapters in this volume contribute to the extensive work addressing the socialization of family values and beliefs. However, these chapters do not focus on the child's language but instead widen the lens to identify new ways in which family members jointly assert, negotiate, and confirm their family's beliefs and values when children are present: through multiple, intertextual discourse strategies in the creation of a family's shared political identity (Gordon, chapter 9); by creating "common ground" in discussions about physical space (LeVine, chapter 10); and through dialogic interaction with the television (Tovares, chapter 11).

</file>

<file= AmE06_J30>

THE PSYCHODYNAMIC APPROACH: SIGMUND FREUD.
Sigmund Freud was just over 40 when he conceived of writing
the book that would become The Interpretation of Dreams. By all accounts
it was his greatest work, introducing a model of the human
psyche that was to profoundly change the way that later generations
would think about themselves and their world. Although the
book appeared in print in November 1899, the publication date
printed on the title page was 1900, as if to lay claim to the emergence
of a new era. Freud himself wrote of it in his 1931 preface
to the third English edition: "It contains, even according to my
present-day judgement, the most valuable of all the discoveries it
has been my good fortune to make. Insight such as this falls to one's
lot but once in a lifetime." But at the dawn of the 20th century, an
analysis of dreams would seem an unlikely topic for an important
work, especially for a man who considered himself a scientist, not
a mystic. So it is worth asking what led Freud to undertake such a
thing. Before we look directly at the book itself and the theory it
presents, let us see what led Freud to the study of dreams.
In the late 1890s Freud held a medical degree from the University
of Vienna and had set up practice in Vienna as a specialist
in neurology. As a specialist of physical disorders in the nervous
system, in this time and place, the most frequent illness that he
encountered in treatment was hysteria. Hysteria was an astonishing
syndrome that baffled the medical community. It was characterized
by various physical problems that on superficial analysis
seemed to be based in neurological damage: problems such as
blindness, deafness, paralysis, or anesthesia of limbs. But fuller
examination would reveal the impossibility of a physical cause.
For one thing, it was not uncommon for the symptoms to move
from one part of the body to another. A patient might first show
signs of blindness, then weeks later appear with loss of hearing,
later with paralysis in a leg, and later still with lack of feeling in a
hand.
One account tried to explain this by viewing hysteria as resulting
from a generally weak nervous system that produced random
effects. But there was a further problem: Some of the symptoms
made no sense neurologically. Take one common symptom of
hysteria, glove anesthesia, which involves loss of feeling in the hand.
Anatomically, there are three separate nerves that run the length
of the arm from shoulder to fingertips, and this means that damage
to any one would result in partial loss of feeling in the whole
arm. Despite its psychological reasonableness, there is no way to
account neurologically for total loss of feeling in the hand only.
An alternative explanation of hysteria, proposed by confused
and no-doubt frustrated practitioners, held that the syndrome
was just an expression of malingering, intentional fakery by the
patient in order to get attention or to avoid responsibility. Support
for this account could be inferred from another feature of
the disorder, this one involving mental functioning. Patients with
hysteria often alternated between two different states of consciousness:
one a normal state and the other a hallucinatory state. In
the hallucinatory state, patients would behave as if experiencing
events that were not actually occurring, and would fail to experience
events that were occurring in front of their eyes. A woman
might carry on a conversation with an imaginary figure, but make
no response to her husband despite his attempts to get through to
her.
But there was also counter-evidence to the account of intentional
fakery to explain hysteria. For example, some patients appeared
with hysterical pregnancies that involved actual physical
changes, such as swelling of the stomach and breasts, which could
not be faked. In other patients, symptoms such as partial blindness
were discovered in the course of a medical examination that had
not been noticed previously by the patient or by others in her life.
In the midst of these flailing attempts to come to terms with this
remarkable disorder, there was one man in Vienna who had suggested
a wholly new approach to hysteria. This innovative thinker,
Josef Breuer, was a renowned researcher and a highly esteemed
clinician. Freud was a medical student when he first met Breuer,
and he was fascinated when Breuer told him of his treatment of a
hysterical woman whose case has since become famous under the
pseudonym of "Anna O."
Anna O. was 21 when she first developed symptoms of hysteria,
after nursing her father for months through what was to be a fatal
illness for him. Over the course of her hysteria she developed
a host of symptoms: paralyses and anesthesias in her arms, legs,
and neck; deafness and visual disturbances; the loss of ability to
speak her native German. (For a period she spoke only in English,
without knowing that she was doing so, having disputes with her
nurse who could not understand a word.) She also experienced
a delirious state in which she had hallucinations, at one point
in her illness hallucinating day after day the actual events that
had occurred exactly 1 year before. At first Breuer was unable to
make any sense of Anna O.'s hallucinations, for she was unable
to relate to him while in her delirious state, and was unable to
recall her hallucinations when she returned to a normal state of
consciousness. But one day he discovered that she could answer his
questions about the hallucinations if she was in a state of hypnosis.
What he learned from her answers was that her hallucinations
were always related to her current symptom. Further, he was astonished
to discover that if she was given the opportunity to talk out
all memories associated with the symptom she was experiencing at
the time, that symptom would suddenly disappear when the final
memory was recounted. For example, in the course of her illness
she developed a visual disorder in which she saw objects as large
and blurred. Under a state of hypnosis, she recounted to Breuer a
series of memories of times when she had visual difficulties, until finally arriving at one that originated from early in her father's
illness. In this memory she sat by her father's sickbed, crying while
he slept. When her father awoke and asked her the time, she swallowed
her sobs and tried to blink back her tears so that he would
not know she was crying. To answer his request she had to bring
the clock close to her face to read it through her tears. This made
the timepiece looked large and blurred, as all objects looked in the
symptom that developed after this experience. But as soon as she
recounted her memory of this experience to Breuer with its associated
feelings, her visual symptom suddenly disappeared.
According to the story that Breuer told to the transfixed young
Freud, each of Anna O.'s symptoms was cured one by one as she
was able to recall emotionally disturbing memories that were associated
with the symptoms. Each ultimately went back to a memory
of her time nursing her fatally ill father. This suggested to Breuer
that hysteria was both caused and cured by psychological processes.
It was not, as most physicians assumed, a disorder caused
by physical damage to the nervous system to be cured by physical
intervention. Nor was it, as others proposed, a sham to get
attention that should be dismissed as a nonillness. Rather it was
an illness in which pathological processes took place in a mental
domain rather than in a physical one.
Breuer's proposal was that hysteria results when psychologically
traumatic events create an excess of emotion that cannot be expressed.
As a result, the unexpressed emotional experiences find
symbolic expression in physical symptoms. In Anna O.'s case, the
trauma of the fatal illness of her beloved father led to painful
but censored emotional experiences (e.g., she sought to hide her
tears while crying at her father's sickbed). These found symbolic
expression in symptoms that related to her emotionally disturbing
memories of his illness (e.g., her visual disturbance). If the
experiences could be remembered and their associated emotions
expressed, there would no longer be any need for the symbolic
symptoms. It was a radical conceptualization of the disorder in
psychological terms, requiring a psychological treatment.
Freud was deeply impressed by Breuer's work, and as he set up
his own medical practice in Vienna he was determined to take a
psychological approach to hysteria. But there was a challenge to
identifying the psychological origins of a patient's disorder, for
Breuer had found that they could not be recalled in a normal waking state. Freud originally began his treatment of hysteria by
trying Breuer's method of hypnosis. He found hypnosis unsatisfactory,
however, partly because he was never very good at it. And
so he began to search for other ways to access information that his
patients seemed to know at some level, but were unable to report
in a normal waking state. In pursuing this search, Freud came to
the study of dreams. In his patients' dreams Freud was able to
find clues to the meanings of their symptoms. It was with the goal
of finding the meaning and cure of this remarkable syndrome of
hysteria, then, that Freud first came to the analysis of dreams.
As it turns out, however, it was not only in his work with patients
that Freud found the fruitfulness of dream analysis. In the 1890s,
he also began to analyze his own dreams. For Freud himself suffered
from a neurosis. This neurosis was most pronounced during
the years 1897 to 1900, when he pursued his self-analysis most
systematically and wrote the book on dreams at the same time.
Although Freud at times referred to his neurosis as "hysteria,"
he probably meant anxiety hysteria, a disorder different from that
described earlier and what today would probably be called panic
disorder. His primary symptoms were mood related, involving a
change of mood from periods of elation to periods of anxiety and
depression; in particular, he suffered fears about dying (what he
called "death deliria") and about traveling by train ("travel anxiety").
A heart arrhythmia was probably also a neurotic symptom,
which he described as "the most violent arrhythmia, constant tension,
pressure, burning in the heart region; shooting pains down
my left arm... and with it a feeling of depression, which took the
form of visions of death and departure." He also suffered from
gastrointestinal problems or irritable colon, which is common in
people with panic disorder.
So Freud had turned to dream analysis in part to find the meaning
of his own neurosis, and the discoveries that he reported in
The Interpretation of Dreams are discoveries that had revealed to him
the source of this disturbance. A hint of the source of Freud's
neurosis can be found in two events in the late 1890s that coincided
with the worsening of his symptoms. Both events involve the
loss of intimate male authority figures: his mentor Josef Breuer
through estrangement and his father through death. For over a
decade, Josef Breuer was not only Freud's collaborator in studying
hysteria but also a supportive mentor with whom Freud shared an affectionate relationship. Breuer had provided Freud friendship
and advice, as well as substantial financial loans. Yet by the
1890s this relationship began to deteriorate, primarily because of
a disagreement over how hysteria should be conceptualized. In
1896 there was such bad feeling between them that Freud would
write to a friend "I simply can no longer get along with Breuer
at all; what I had to take in the way of bad treatment and weakness
of judgement that is nonetheless ingenious during the past
months finally deadened me, internally, to the loss."  
But despite
this claim, Freud continued to be deeply affected by their break,
and he wrote to the same friend only one week later "I would like
indeed to have Breuer's letter; in spite of everything, I find it very
painful that he has so completely removed himself from my life."

</file>

<file= AmE06_J31>

There is an important convergence taking place between Jungian psychology,
evolutionary psychology, and neuroscience. Many readers will
be acquainted with recent developments in neurotheology, which seeks to
understand the neurological bases of spiritual experiences and practices. Perhaps
less familiar will be the field of evolutionary psychology, which seeks
to understand human psychology in terms of its adaptive role in our species'
evolution and by comparison with the evolution of other species. Orthodox
Jungians might worry about the effects of this encroachment of materialist
ideas and methods. Will this not lead to a reduction of psychical experience
to neurons and genes, which would suck the life out of Jungian psychology? I
hope to show that this is not the case, and that each of these three disciplines
may reinforce and expand the others, if we take the appropriate approach of
recognizing psychical reality alongside material reality. (Much of what I say
is based on the work of Jungian analyst Anthony Stevens.)
ARCHETYPE AND INSTINCT.
The lynchpin connecting the three disciplines of Jungian psychology, neuroscience,
and evolutionary psychology is simple: The archetypes are psychical
correspondents of human instincts. That is, when you are behaving
instinctually, you experience yourself to be in an archetypal situation. Activation
of an instinct structures an animal's perception and behavior, and when
you are that animal, you experience a myth unfolding in which you are a key
actor. Some people might find the idea of "human instincts" to be objectionable,
for we have long flattered ourselves with the idea that we are completely
autonomous in our behavior, and that this autonomy separates us from "the
beasts." However, evolutionary psychologists have shown that we are no different
from other animal species in possessing a wide range of instincts that
have promoted the evolutionary success of our species. Jungians, too, are familiar with the ego's inflated opinion of its own autonomy and understand its
more modest function in relation to the Self and the collective unconscious.
Indeed, Jung understood the connection between the archetypes and
instincts. For example, he wrote, "To the extent that the archetypes intervene
in the shaping of conscious contents by regulating, modifying, and motivating
them, they act like the instincts" (CW 8, par. 404), and "The hypothesis of
the collective unconscious is . . . no more daring than to assume that there are
instincts" (CW 9, pt. 1, par. 91).
The instincts tune perception and behavior in order to fulfill some purpose
important to our species, be it mating, infant care, cooperation, social
organization, defense, or competition for mates. When an appropriate releasing
stimulus activates the instinct, you may feel as if you were living a myth
or that you are possessed by a spirit with its own agenda. To take an obvious
example that almost everyone will recognize, when stricken by love, you may
feel as though Aphrodite or Eros is governing your behavior; your perception
of the beloved will be transformed, and he or she will appear numinous and
divine; indeed, all of life may be experienced in a magical or uncanny light.
EVOLUTION AND THE INSTINCTS.
Konrad Lorenz (1903 - 1989) was the founder of ethology. Stevens (2003) observes
that Lorenz and Jung can be considered complementary, for Lorenz
focused on external behavior, whereas Jung focused on internal experience.
Of course, there are significant differences: Jung was more interested in people,
Lorenz in nonhuman animals. Also, Lorenz and other ethologists are interested
in placing behavior in its evolutionary context, whereas Jung was not so
interested in an evolutionary perspective. Finally, contemporary neuroethologists
are interested in the neurological substrates of behavior and how brains
have evolved, whereas Jung abandoned neurology, and most Jungians have
avoided it (perhaps in reaction to the prevalent reductionist materialism of
our time).
Ethology is the discipline that studies animal behavior in the context of
its evolutionary development; that is, it seeks to understand how a species'
instincts have evolved in interaction with its historical environment, and how
they have promoted the survival of the species in that environment. The basis
of ethology is the recognition that an instinct must be understood in terms of
a species' environment of evolutionary adaptedness, that is, the environment
in which it has evolved and to which that instinct has served to adapt
the species. This is the context in which we may explain the purpose of an
instinct (that is, its adaptive function). (Indeed, many evolutionary psychologists
refer to evolved mechanisms or adaptations rather than instincts.)
Therefore, to understand the purpose (adaptive function) of the archetypes (as the psychical correlates of the instincts), we must consider Homo sapiens'
environment of evolutionary adaptedness. In addition to their interior
aspect, which is well-known from Jungian psychology, the archetypes have an
exterior aspect when they manifest in behavior, and the exterior aspect is especially
relevant as functional (purposeful, adaptive) behavior in the human
environment of evolutionary adaptedness.
Thus, instincts, understood in their evolutionary context, give us another
perspective (an exterior perspective) from which to understand the
archetypes. We can explore how these instincts have been adaptive in our
environment of evolutionary adaptedness, and we can investigate similar instincts
in related species (e.g., nonhuman primates) and how they relate
to their environments. Because a species' instincts have evolved in an environment
of evolutionary adaptedness, this is the environment in which the
function of those archetypes is easiest to understand. If we want to understand
the function of the archetypes and thereby gain a better understanding
of their structure, then we should look to the environment of evolutionary
adaptedness of Homo sapiens.
Modern humans (Homo sapiens sapiens) are believed to have evolved
about 200,000 years ago in Africa. Further, as Stevens (1993) notes, we have
spent about 99.5% of that time as hunter-gatherers, until animal husbandry
and agriculture began to appear about 10,000 years ago. In evolutionary terms
this is not much time, so genetically we are very similar to our hunter-gatherer
ancestors. That fact means that our environment of evolutionary adaptedness
and our instincts and archetypes are essentially the same as those of paleolithic
hunter-gatherers.
Based on studies of contemporary hunter-gatherer societies, Stevens
(1993) has outlined the characteristics of human society throughout most
of our history - in other words, the sort of life to which we would expect
our instincts and corresponding archetypes to be adapted. Comparisons with
closely related primate species also add to a behavioral understanding of
the instincts, which complements the interior structure described in Jungian
psychology and evident in myths and dreams. Stevens observes that huntergatherer
groups typically contain 40 - 50 interrelated individuals, of whom 6 - 
10 are adult males, 10 - 20 adult females, and the remainder juveniles. Sexual
relations are not necessarily monogamous. Such groups spend much of their
time in isolation but do encounter other similar groups from time to time,
which may lead to fighting or exogamous mating.
One of the dilemmas facing modern humans is that our contemporary
environment is very different from our environment of evolutionary adaptedness.
Therefore, the archetypes that served us well through nearly 200,000
years of hunting and gathering may not fit so well with contemporary culture
and lifestyles. As is explained later, part of the goal of individuation is to achieve a reconciliation and accommodation between our contemporary lives
and our genetic heritage, represented in the Self.
NEUROPSYCHOLOGY OF THE ARCHETYPES.
Just as the various physiological processes in our bodies are functions of our
organs as they develop in a normal environment, so human instincts and the
corresponding archetypes are rooted in the body, primarily in the brain, as it
develops in a normal environment. Although there is individual variation, the
brain's gross anatomy as well as its detailed organization are the same for all
people; so also the archetypes are common to all people, which is why we can
speak of a collective unconscious and an objective psyche.
Nevertheless, it is unlikely that we will discover a simple relationship
between the archetypes and brain structures (as is sometimes suggested by
evolutionary psychologists' use of such terms as "mental organ" or "module").
The mother archetype, for example, depends on many brain systems: perceptual,
emotional, motivational, attentional, learning and memory, and so forth.
The neural structures subserving an archetype extend through many regions
of the brain, from the brainstem to higher cortical areas. Furthermore, as Jung
emphasized in his later writings, because archetypes are dynamic structures,
not static images, we can expect different brain systems to be involved in different
phases of an archetype's activation. Eventually, with improved imaging
studies and a deeper overall understanding of the brain, we may be able to
chart all of the brain regions subserving an archetype, but that time is well
in the future. Nevertheless, in the meantime, even a partial understanding of
the neurological substrate of the archetypes will improve our understanding
of them.
It may be worthwhile to emphasize that the archetypes, even as dynamic
structures, are not fixed throughout an individual's life; that is, the dynamic
structures restructure through time according to a developmental program.
The first archetype to develop is, perhaps, a generalized parent archetype,
which soon becomes more specific as the nascent mother archetype; later,
the father archetype is differentiated, and so forth. Adolescence accelerates
development of some archetypes and causes a general reorganization of them
all. This pattern is consistent with what we know about the development of
the brain, which develops rapidly into the mid-20s, but continues to transform
itself thereafter (menopause being an obvious example). The old dogmas
about the cessation of neuron growth are slowly collapsing, and every year
brings new evidence of the brain's plasticity throughout life. So although the
archetypes (as abstract structures) are eternal and unchanging, our relationships
with them mature along with our bodies.Amiddle-agedmanexperiences
Eros differently than does an adolescent boy.
Even the developmental change in the brain that has been longest
acknowledged - neuronal death - should be understood more broadly than
"the inevitable decline of old age." We now know that programmed neuronal
death (neuronal apoptosis) is an important mechanism in the brain's selforganization
(which takes place in interaction with the environment). The
unborn infant's brain grows many more neurons than it needs, and in a competitive
process known as "neural Darwinism," it organizes, tunes, and optimizes
connections by eliminating approximately half of its neurons in a process that
begins in the final trimester and continues for several years. There is another
spurt of neural growth shortly before puberty, followed by another wave of
competitive elimination that continues well into the 20s. Clearly, neuronal
death is not always a bad thing; it may serve a useful organizational function
(rather like weeding a garden or pruning a tree), and even the gradual
neuronal loss of our adult years may serve some adaptive function. In any
case, neuronal death is apparently an inevitable part of the human life cycle,
suggesting that the corresponding changes in our relationships with the
archetypes are also a part of our life cycle - changes that lie along our paths
to individuation.
Because of genetic variation among individuals as well as differing environmental
influences during development, archetypes are experienced differently
by each of us. Nevertheless, there is overall similarity among us, which
is why we can speak of a collective unconscious comprised of the archetypes.
However, in addition to the developmental processes that I have mentioned,
it's important to keep in mind that learning extends and modulates the dynamic
processes governed by the archetypes, and in this way they may become
much more individual, that is, they may engender personal complexes.
I return to this issue later.
THE HUMAN GENOME AND THE OBJECTIVE PSYCHE.
Each individual's genotype - that is, the abstract sequence of approximately
300 million base pairs - defines that person's genetic makeup. When encoded
in the DNA of a fertilized egg, this sequence governs (in interaction with the
environment) the development of the organism, including its brain. Therefore,
the seed from which the archetypes grow, as you experience them, resides in
your genotype. Your genotype corresponds precisely to a number of approximately
180 million digits; in principle, a person genetically identical to you
could be created using this number (it has been done already for viruses).

</file>

<file= AmE06_J32>

Bad Language: Realism versus Relativism.
What does the phrase "bad language" mean to you? Perhaps you think of swearing or slang. Is it bad language to curse if you hit your thumb with a hammer? Is it bad language for a novelist to use the f-word in dialogue? What about the speech of a sitcom character, newscaster, or presidential candidate? Is it bad for young people to use slang like dude, chill, my bad, hook up, or bling-bling? 
Some people think of regional or ethnic dialects as bad English. Texas writer Molly Ivins, for example, once suggested that to Northerners "a Southern accent is both ignorant and racist." She cited the World War II genre of movies, whose stock characters included "a Midwestern hero, a wise-cracking New Yorker, and a dumb Southerner" as one source of dialect stereotypes.  Is it really bad English to pronounce ten and pen as "tin" and "pin" or to use the pronoun y'all? Another stigmatized dialect form is the New York City dialect. Are pronunciations like "cawfee" and 
"chawklit" bad English? And what about Ebonics? Are pronunciations like "aks" or grammatical usages like "He been married" bad?
People also identify bad English with a foreign accent or with English mixed with another language. In a 1952 episode of the program I Love Lucy, Lucy Ricardo hires an English tutor to polish her speech so her son won't learn her bad habits. Commenting on her husband Ricky's Cuban accent, she says "Please, promise me you won't speak to our child until he's nineteen or twenty."  Is it bad English to speak with a Cuban accent and mix Spanish with English, as in "Okay, gracias" (or "Yo quiero Taco Bell")? More than fifty years after "Lucy Hires an English Tutor," accent modification is big business in the United States and internationally.
Even if you don't have a definite opinion on swearing, slang, and accents, it is likely that you equate bad English with certain forms of grammar or word use. I once heard someone talk about his office being made the escape goat for problems. Even though the phrase was a natural re-interpretation of the word scapegoat, it suggested to me that the speaker hadn't read much. When I teach writing, I try to model standard usage and I correct nonstandard forms such as "I seen the Cascade Mountains," "We should of found another route," and "There had to be a way in which to do that." But there are many so-called errors that I do not correct. Consider, for example, these items, from practice tests in an early twentieth-century grammar correspondence course: 
What do you think of (me - my) going to town? 
I was frightened at (that examination's length - the length of that examination). 
You must act (quicker - more quickly). 
The order was (only intended - intended only) for the major. 
You must report to me (more often - oftener). 
In each case, the test maker intended the first choice to be marked out as incorrect. But none of these seem to me to be errors worth correcting, and the last example actually seems smoother with more often rather than oftener.
Bad English is hard to define. One way is simply to say that it is English that doesn't follow the rules. That of course raises a new question. What do we mean by the rules of a language? Broadly speaking there are two ways of thinking about rules. One view is that rules describe the regularities that speakers follow in using their language. For example, consider how English speakers form simple questions. We invert the helping verb (also known as the auxiliary verb) and the subject. In order to make a question from the simple statement Mary has left, we shift the auxiliary verb has to the front yielding Has Mary left? Similarly, the sentence Is John busy? is the question form of John is busy; Will you close the door? is the question form of You will close the door; and so forth.
What if there is no auxiliary verb in the statement form? In that case we add the special auxiliary verb do and put that in front of the subject. So Do you see that? is the question form of You see that. A rule of English is that questions are formed from statements by moving the auxiliary to the left of the subject or by adding a form of do when no other auxiliary is present. Such rules document how the English language works. They also provide building blocks for deeper investigation into the patterns of language. From very simple questions such as these, we could go on to investigate questions with interrogative pronouns, such as What are you reading? or we could study the use of do in negative sentences, as in I didn't see that movie.
The study of language that focuses on patterns of use and rules like this is known as descriptive grammar. Descriptive grammar is the basis for dictionaries, which record changes in vocabulary and usage, and for the field of linguistics, which aims at describing languages and investigating the nature of language. Taking the description of language seriously means paying attention to such details as how questions are formed and similar patterns of language. It also means recognizing that these patterns are subject to variation and change. Dictionaries provide the most obvious illustration of the way that language changes. As new editions of dictionaries are published, they document new words that arise like Botox, identity theft, phat, and bioterrorism. They document 
new meanings of existing words such as the use of the word nuke as a verb meaning to heat by microwaving, or the change in the meaning of the verb print from mechanical to electronic reproduction of text and images.
Rules of grammar change as well. A moment ago I discussed the pattern of question formation in present-day English. But Shakespeare's characters ask questions that reflect a different pattern of question formation. They ask such questions as What said he? and Came he not home to night? These questions are formed by inverting the main verb rather than by adding do, which was the common pattern in earlier English. In Shakespeare's time, questions could be formed by inversion of the main verb or by adding do, as in And did you not leave him in this contemplation?  The different options were used in different styles and situations, but eventually the forms with do became the only pattern for questions not having an auxiliary. In other words, the rules for forming questions have changed over the centuries.
The idea of following a rule is so deeply associated with correct behavior that it is tempting to think of rules as applying only to formal language and to view informal language as being without rules or being lax about them. But informal speech obeys rules as well. Take contraction, for example, a process that occurs in much informal language. Contraction of the verb is takes place in sentences such as Where's my pen? Who do you think's going to be there? and What's going on? But contraction does not typically occur at the end of a sentence. We can say I wonder who that is but not I wonder who that's.  Other informal language is systematic as well. Even apparent conversational filler words have a system and meaning to them.  Like, for example, is used to indicate approximation or exaggeration, as in He has, like, six sisters and He's, like, 150 years old. It may also serve as an informal synonym for says, as in the sentence, I tried to play some music and he was all like "Quit making so much noise." 
It is also tempting to think of regional dialects as breaking the rules of good English. The grammar of regional speech varies from that of Standard English, but the variation is systematic and patterned. It is more accurate, then, to view dialects as having different rules from the standard. In Appalachian English, for example, speakers sometimes put an a-sound before words that end in  - ing. This results in familiar expressions like a-hunting we will go. But speakers who use this dialect feature do not simply put the a- before any word that ends in  - ing. Appalachian speakers who use the a- prefix might say Everyone went a-hunting but not Everyone likes a-hunting or I bought a new a-hunting dog. The a- is possible when the  - ing word is a verb, but not when the -ing word is a noun or adjective.  So the grammar of Appalachian English has a fairly sophisticated rule of a-prefixing that is missing in Standard English.
Dialect variation also occurs in the usage of individual words as well. An example involves the use of the adverb anymore. In Standard English, its primary use is in negative sentences (such as Yogi Berra's famous statement that "Nobody goes there anymore. It's too crowded") and in questions (such as Do you go there anymore?).  In some parts of the county, however, speakers also use anymore in sentences such as Everyone is cool anymore or Anymore you're working too much. The extension of anymore to positive statements does not mean breaking a rule. Rather, different speakers have different rules for using this adverb, and dictionaries recognize such variation in usage notes. The eleventh edition of Merriam-Webster's Collegiate Dictionary, for example, notes that positive anymore "is now reported to be widespread in all speech areas of the U.S. except New England."
The idea that dialect and informal speech are organized systems with rules is an important one. Suppose you try to write a novel with an Appalachian or Midwestern character. It is possible to use the rules of the dialect to draw a convincing portrait of that character's speech or to bungle the rules of the dialect to create an ineffective portrayal. Or suppose you are an advertiser or politician wishing to appeal to an audience of young people, blue- and pink-collar workers, or senior citizens. You are likely to want to tailor the level and formality of your 
speech to them, and to follow the rules for the speech variety you believe is most effective for your audience.
The variability of language is significant in another way. It means that good and bad language cannot be defined in absolute terms. The standard language of one era, generation, medium, or region might well differ from the standard of another. The editorials of the New York Times or Wall Street Journal differ from the arts and culture reporting in Time magazine. Educated speech in Atlanta or Austin differs from educated speech in Seattle or Boston. Speakers shift their styles depending on their audience, using vocabulary, pronunciation, and grammar that fit well. And vocabulary, grammar, and pronunciation norms all change. If they did not change, we might still be using dictionaries from one hundred years ago and Chaucer and Beowulf would be much more popular with students.
Language change does not mean that any novelty automatically becomes the norm and achieves widespread educated usage. Some innovations become widely used while others fail. Television is an excellent source of newly coined vocabulary but some new vocabulary takes hold and some does not. A coinage like regift, from the series Seinfeld, seems to me to be a likely candidate to become standardized. Other novelties, such as low-talker, hand-sandwich, and shushee (also from Seinfeld) or kitteny, frowny, girl-powery, knifey, and huntery (from Buffy the Vampire Slayer) are unlikely to become widely used.  
Innovation and variation in pronunciation and grammar evokes similar strong feelings. Some variation is unremarkable while other variation is contentious and stigmatized. Consider the pronunciation of the words economic, Uranus, Oregon, and nuclear, for example. The variable pronunciation of the first two words is typically treated as a matter of alternate standard pronunciations: EEKonomic versus ECKonomic or YOURunus versus youRAYnus. The pronunciation of Oregon as or-uh-GUN or or-uh-GAWN varies according to whether one is native or an outsider.  The pronunciation of nuclear is different. The pronunciation of nuclear is different. For many it is a marker of education and refinement whether one says NU-clee-ur or NU-cu-lar.

</file>

<file= AmE06_J33>

The Learning Community in Online Learning. 
When Teaching and Learning Leave the Classroom.
I n the last ten years, significant change has occurred in online learn- ing. Once viewed as a less rigorous, softer, easier way to complete a course or degree, faculty now realize that the time involved in the development and delivery of a high-quality online course is substantial, and students are now realizing that completing courses and degree programs online is hard work. There is no longer a need to spend time defining what online distance learning is or is not; it is now com- monplace in higher education and is gaining popularity in the K-12 arena as well. Ten years ago, we were trying to decide what constituted distance learning and asked questions such as, 'If the class meets face-to-face two or three times during the term, is that a distance learning course'. Today we know that distance learning takes several forms, including fully online courses, hybrid or blended courses that contain some face-to-face contact time in combination with online delivery, and technology-enhanced courses, which meet predominantly face-to-face but incorporate elements of technology into the course. In addition, academic institutions are experimenting with that depart from the traditional semester or quarter in order to more effectively deliver online classes. It is not unusual now to see six-week intensive courses or courses with flexible start and end dates. If we examine all the ways in which distance learning is occurring now, it is possible to state that almost every course delivered via some form of technology is a distance learning course. There is one important element, however, that sets online distance learning apart from the traditional classroom setting: Key to the learning process are the interactions among students themselves, the interactions between faculty and students, and the collaboration in learning that results from these interactions. In other words, the formation of a learning community through which knowledge is imparted and meaning is co-created sets the stage for successful learning outcomes. Ten years ago, the notion of building community online was seen as 'fluff' or just one more thing an instructor might pay attention to in the delivery of an online course. However, much research has been conducted in recent years regarding the importance of community in an online course and in online teaching in general (Garrison, n.d.; Rovai, 2002; Rovai and Jordan, 2004; Shea, Swan, and Pick- ett, 2004; Wenger, 1999) and, further, into the concept of social presence, defined as the ability to portray oneself as a 'real' person in the online environment (Gunawardena and Zittle, 1997; Picciano, 2002; Richardson and Swan, 2003; Rovai and Barnum, 2003). The findings of these research studies and others have supported our notion that the key to successful online learning is the formation of an effective learning community as the vehicle through which learning occurs online. Adams and Sperling (2003) note that the community building process embedded in online courses has helped transform teaching and learning in higher education. Some of the changes they describe for students include greater availability and accessibility of information, engagement of different learning styles, and promotion of increased responsibility for teaching and learning. The changes faculty are experiencing include greater accessibility to and availability of information but also encompass the development of new skill sets for teaching and the need to rethink pedagogy, redefine learning objectives, reevaluate assessment, and redefine faculty work roles and culture. We also see these changes in a number of college classrooms today, not just in online classrooms. And we continue to learn more about how people learn. Carol Twigg (1994b) indicated that many students are concrete-active learners, that is, they learn best from concrete experiences that engage their senses. Their best learning experiences begin with practice and end with theory (Twigg, 1994b). Many instructors, seeking to improve their practice and the learning outcomes for their students, have incorporated active learning techniques such as working collaboratively on assignments, participating in small-group discussions and projects, reading and responding to case studies, role playing, and using simulations. These practices transfer well into the online classroom. However, instructors need to be diligent and deliberate in ensuring their success. When learners cannot see or even talk to each other, the use of collaborative assignments becomes more challenging but far from impossible. (We offer suggestions for implementing collaborative learning techniques in the online classroom in Chapter Eight.) Learning in the distance education environment cannot be passive. If students do not enter into the online classroom - do not post a contribution to the discussion - the instructor has almost no way of knowing whether they have been there. So students are not only responsible for logging on but they must also contribute to the learning process by posting their thoughts and ideas to the online discussion. Learning is an active process in which both the instructor and the learners must participate if it is to be successful. In the process, a web of learning is created. In other words, a network of interactions between the instructor and the other participants is formed, through which the process of knowledge acquisition is collaboratively created. (See Chapters Eight and Nine for a discussion of collaborative learning and the transformative nature of the learning process.) Outcomes of this process, then, should not be measured by the number of facts memorized and the amount of subject matter regurgitated but by the depth of knowledge and the number of skills gained. Evidence of critical thinking and of knowledge acquired are the desired learning outcomes. Consequently, cheating on exams should not be a major concern in an effective online environment because knowledge is acquired collaboratively through the development of a learning community. (The assessment of student performance in this environment is discussed in Chapter Ten.) Institutions entering the distance learning arena must be prepared to tackle these issues and to develop new approaches and new skills in order to create an empowering learning process, for the creation of empowered learners is yet another desired outcome of online distance education. Successful online teaching is a process of taking our very best practices in the classroom and bringing them into a new, and, for some faculty, untried, arena. In this new arena, however, the practices may not look exactly the same. Take, for example, a recent discussion with a professor in a small college where a distance delivery model was being implemented for a master's degree program. A software program was chosen and a consultant hired to install it on the college's server. There it sat for almost a year until the college decided to begin using it more extensively. Because of our expertise in faculty training and development for the delivery of distance education programs, we were consulted about the best way to improve a program that was not working very well. The professor informed us that the software had been used by a couple of instructors for a couple of courses. However, with further inquiry, we discovered that a course syllabus had never been posted online in any of these courses; nobody knew that an extensive faculty handbook for course development and delivery was embedded in the software. All they had been doing was using this potentially powerful software package as an e-mail system rather than for creating a distance learning environment. Was distance education and learning really happening here? No, of course not. So what does it take to make the transition from the classroom to the online arena successfully? What are the differences we face in this environment? And finally, what issues do we need to be concerned with? We answer the last question in the next section through a discussion of the issues and concerns related to online education. The answers to the other questions follow in subsequent chapters. 
ONLINE ISSUES AND CONCERNS. 
When instructors begin to use technology in education, they experience a whole new set of physical, emotional, and psychological issues along with the educational issues. Many of these issues relate to the development of social presence. As we struggle to define ourselves online, we may experience emotions and try out behaviors that have not been part of our repertoire. The new issues also include the physical problems that can be experienced as the technology is used extensively, such as carpal tunnel syndrome, back problems, headaches, and so forth. Psychologically, students and faculty can become addicted to the technology. In fact, there are now centers devoted to the study and treatment of Internet addiction. Students and faculty can begin to fantasize and experience personality shifts while online, and their minds can drift. They may have a difficult time setting reasonable boundaries and limits around the amount of time they spend online. We have not had to address these issues in the traditional classroom, but we must do so as we teach online because they affect the ways learners interact with each other and with course material. In the traditional classroom, if a student experiences mind drift it may not be noticeable to the instructor or to the other students in the class. The student may be physically present but psychologically absent. In the virtual classroom, however, if a student drifts away, that absence is noticeable and may have a profound impact on the group. Online learning has brought a whole new set of issues and problems into academics; as a result, instructors and their institutions have had to become more flexible and learn to deal with these problems. Professors, just like their students, need the ability to deal with a virtual world in which, for the most part, they cannot see, hear, or touch the people with whom they are communicating. Participants are likely to adopt a new persona, shifting into areas of their personalities they may not have previously explored. For example, an instructor, like a student, who suffers from performance anxiety in the face-to-face classroom may be more comfortable online and more active in responding to students. A colleague of ours who has wanted to teach for several years and who feels that he has a contribution to make is very nervous about entering a classroom and facing a group of students. He has been offered several opportunities to teach because of the expertise he would bring to a learning situation, but he has resisted. When offered an opportunity to teach online, however, he accepted readily, acknowledging that the relative anonymity of the medium feels more comfortable for him. The idea of being able to facilitate a discussion from the comfort of his home office was very appealing to him, whereas doing the same thing face-to-face was intimidating. However, the opposite may also be true: an instructor who does well face-to-face may not be successful online. We were told the story of an accounting professor who was extremely personable in his face-to-face classes. To assist students in memorizing dif- ficult concepts, he would compose songs and play them in class, accompanying himself on his guitar. He was approached to teach online but resisted strenuously because he did not feel he could adequately transfer his musical approach to accounting to the online environment, even with the use of attached audio files. His first attempt at online teaching was not well-received by students and he decided not to continue with online teaching. Just as all instructors are not successful in the classroom setting, not all will be successful online. It takes a unique individual with a unique set of talents to be successful in the traditional classroom; the same is true for the online classroom. The ability to do both is a valuable asset in today's academic  institutions.
STUDENTS ONLINE. 
Some attributes make students successful online when they are not in the face-to- face classroom. For example, what about the introverted student? Will such a student, who does not participate in the face-to-face class, blossom in the virtual classroom? Research conducted by one of us indicates that an introverted person will probably become more successful online, given the absence of social pressures that exist in face-to-face situations.

</file>

<file= AmE06_J34>

A model of speech production is proposed in which the input is a planning stage at which
lexical items are arrayed, accompanied by the full panoply of phonological representations from
distinctive features to their attendant tree structures. A set of instructions for control of the vocal tract is calculated leading to a sound output. Two parallel processes are involved in the calculation of these instructions, both of which replace the planning-stage representation by the appropriate motoric instructions. One of these processes is universal and involves replacing each distinctive feature with an appropriate set of motoric instructions, either unmodified or modified by the process of overlap. We postulate a parallel language-specific process that is sensitive to those features in danger of losing their perceptual saliency as a consequence of the environment in
which they appear. This process, referred to as ENHANCEMENT, adds additional motoric instructions
to enhance the saliency of the jeopardized features. We provide a number of examples to illustrate
how enhancement works. We conclude from these examples that whereas defining gestures related
to distinctive features are, in many instances, weakened or even absented from the speech stream,
enhancement gestures, once added to the set of motoric instructions, appear never to be subject
to obliteration by overlap.
INTRODUCTION: GENERAL CONSIDERATIONS. 
We assume as a starting point the correctness
of distinctive feature theory. This statement may to many seem on a par with
a theoretical physicist claiming to have made his/her peace with, say, quantum mechanics.
That said, there are many working in the field of phonetics for whom distinctive
feature theory is by no means so firmly entrenched. Such researchers are not swayed
by results of phonology such as accounts that distinctive feature theory makes possible
for omnipresent phenomena such as diachronic sound change, or the morphological
variation one finds in such systems as the English plural. Rather they are swayed by
the tremendous amount of variation in the instantiation of phonemes in a given language,
for example, the multitude of realizations of English /t/.
This skepticism is discounted in phonological circles because of the tremendous
gains in explanation offered by distinctive feature theory. But that in and of itself does
not make the reasons for the skepticism disappear. There is, indeed, substantial variation
in the realization of distinctive feature configurations, and it is an important question
for those working at the interface of phonology and phonetics to try to come to grips
with it. The purpose of this article is to address the question of variability head-on
while maintaining the integrity of distinctive features as a cornerstone advance in our
understanding of human language.
In what follows we develop the theory of enhancement presented in earlier manifestations
(e.g. Stevens et al. 1986, Stevens & Keyser 1989, Keyser & Stevens 2001).
Our strategy involves preserving the character of a given segment's distinctive feature
representation as suggested by phonological considerations in the face of phonetic
properties that seem completely unrelated. A paradigm example is the rounding that
accompanies the production of /f/ in English as opposed to the lack of rounding in
English /s/. There is no reason to suppose that rounding is a distinctive part of the
representation of the segment /f/. But this is true only from a phonological point of view. There is no contrast between a rounded and an unrounded /f/ or a rounded and an
unrounded /s/. For the phonologist, then, rounding is extraneous. But to the phonetician
it is important. In English no description of /f/ that does not include rounding can be said
to be complete.
In the face of this, some phoneticians might take the position, not unreasonably, that
however one describes the segment, rounding must be a part of that description. From
our perspective we are forced to ask the question: if rounding is not distinctive for /+/
and /s/ in English, why is it there at all? This question makes sense only if one is
willing to honor the intentions of both phonologists and phoneticians. In what follows
this is what we try to do.
THE PLANNING STAGE. 
There is widespread agreement (e.g. Ladefoged & Maddieson
1996, Johnson 2003) that speech production begins in discrete representation and
ends in continuous sound. This shift takes place in the vocal tract and its attendant
musculature, the arena where representation becomes sound-producing gesture. Underlying
this view is the assumption that representation and sound are fundamentally
different in character. The former is digital in that it is composed of a variety of discrete
entities: for example, features, syllables, feet. The latter, however, is primarily - though
not exclusively - a continuous sound stream generated by a flow of gestures in the
vocal apparatus from one intention to the next. This dichotomy between a discrete
representation and an analogue output device is represented schematically in Figure 1
with the box labeled GESTURE CALCULATIONS mediating between the two.
The existence of a gestural stage has led some researchers to conclude that lexical
and gestural representation should be collapsed into a single stage. Were it true, this
conclusion would greatly simplify the system. That is, instead of going from a distinctive
feature representation and its attendant representations (foot structure, syllable structure,
etc.) to gestures to sound, one would go directly from gesture instructions to sound
(see Browman & Goldstein 1986).
Phonologists have, in general, rejected the view that gestures are lexical, the main
reason being that phonological representation in the form of distinctive features allows for the expression of a vast number of what appear to be true generalizations about
language, for example, phenomena of vowel harmony, vowel lengthening and shortening
in specific phonological environments, the Great Vowel Shift in English, stress
placement, and so forth.
While there is some truth in the gestural notion of representation, it can be argued
that gestural representations should not be localized in the lexicon, but must occur at
a later stage in the speech process. One type of evidence for this view comes from a
consideration of speech-error phenomena. These phenomena argue for the existence
of something called a PLANNING STAGE in the production of speech utterances. We
suggest that gestures must occur after this stage (see §4.2 below). Here we focus on
arguments for the existence of the planning stage itself.
Consider two typical speech errors drawn from Fromkin (1973:245 - 46). Suppose a
speaker wants to produce a form like wind mill but, in fact, produces the utterance [mìnd
wìl] by transposing the onsets of each member of the compound. The erroneous [mìnd
wìl] does not occur as a separate entry in the lexicon nor is it the result of phonological
rules. Similarly, Fromkin reports the phrase hash or grass becoming the erroneous hass
or grash as a result of final consonant transposition. Here, too, the resultant utterance
contains nonsense words which could not possibly have been drawn from the lexicon.
Speech-error theorists have postulated that these errors must occur outside the lexicon
and the phonology but prior to articulation. They propose a level that we call the planning
stage. Errors occur here (see Shattuck-Hufnagel 1986, Levelt et al. 1999).
We assume the planning stage is a level at which discrete items drawn from the
lexicon and modified by the phonological component are arrayed in a serial fashion.
In Figure 2 we expand on Fig. 1 by inserting the planning stage after phonological representations. Representations in the planning stage are fully articulated. That is to
say, at this stage we find the full panoply of phonological representation.
GESTURE CALCULATIONS.
THE BASIC FUNCTION. 
In the simplest case, representations in the planning stage
are read off by a component whose function it is to replace representations by gestures.
We suppose that the component that the planning-stage string feeds into is one that
calculates how the various articulators in the vocal apparatus should be controlled,
including the relative timing of the movements of these articulators.
To see how this component works, let us imagine that the planning stage contains
the sequence the tag (see the spectrogram in Figure 3a). We focus on the gestures that are
needed to produce the consonant /t/, which has the features [+anterior, +consonantal,
-continuant, -sonorant, +stiff vocal folds]. We note also that this segment is in
syllable-initial position, which coincides with word-initial position, and that the entire
word is dominated by a single syllable. The gesture-calculations component needs to
turn this representation into a series of instructions to the musculature of the vocal
apparatus which produces the relevant acoustic event. Thus, this component encounters
the feature [+anterior] and translates it into an instruction to position the tongue blade
anterior to the alveolar ridge. The features [+consonantal, -continuant] require that
this articulator make a complete closure in the midsagittal region of the vocal tract,
followed by a release. An acoustic consequence is a noise burst with spectrum energy
at high frequencies, as the spectrogram shows. During the time that this closure is
made, the pressure should build up behind the constriction, as specified by the feature
[-sonorant]. The feature [+stiff vocal folds] requires that the tension in the vocal
folds be increased during the closure interval and be relaxed following the release of
the consonant. As a consequence of these instructions, the vocal apparatus produces
an acoustic pattern that reflects the set of features for this segment. This acoustic pattern
extends from the closure through the release of the alveolar consonant. We see later
that these instructions are modified by additional calculations that are not derived directly
from the planning stage.
Note that while the input to the gesture-calculations component is a phonological
representation, the output is not. Rather, the output is a series of instructions to the
musculature. This entails that the phonological representation disappears at this point,
being replaced by motor instructions. Hence, if the birthplace of lexical representation
is in the lexicon, its demise is in the gesture-calculations component.
VARIABILITY IN GESTURE SEQUENCES. 
Consider now the difference in the tag and
top tag, focusing on the gestures relevant to /t/ in tag. While the gestures for /t/ in both
expressions are identical, the output is not. In particular, the tongue-blade closure for
the second occurrence of /t/ in top tag usually occurs during the time when the lips
are closed for the preceding /p/. Thus, unlike the tag, the time of closure for /t/ is not
registered in the sound. The absence of evidence for the /t/ closure is illustrated in the
spectogram in Figure 3b. Likewise, the release for /p/ occurs while the tongue blade
is in the closed position, and consequently this release is either not registered or only
weakly registered in the sound. However, acoustic evidence for the tongue-blade feature
is still present in the release of the /t/, just as acoustic evidence for the labial is still
present in the closure of the /p/. This masking of acoustic evidence is the result of
overlap of gestures for two different articulators (Browman & Goldstein 1990). In the
case of top tag the closure of the /p/ masks the closing gesture of the /t/ and the closure
for the /t/ masks the release of the /p/. Of course, in careful speech there can be an acoustic record of the /p/ release and the /t/ closure. Such an utterance is illustrated in
the spectrogram in Figure 3c. Such overlap is a universal phenomenon and reflects a
universal tendency to conserve time and, presumably, energy (see Lindblom 1990).
Overlap pervades the speech stream. Consider the /nt/ sequence in teen tag. For both
segments the instruction from the planning stage is to make a closure of the tongue
blade and then to release it. In this case, however, the closure is retained through both
segments and there is, in effect, no separate release for /n/ and no separate closure for
/t/. In the phrase teen bag (see the spectrogram in Figure 3d) the closure for the labial
/b/ would normally occur during the alveolar closure for /n/. Soon after this labial
closure the soft palate is raised and pressure is built up in the oral cavity.

</file>

<file= AmE06_J35>

The syntax of Moses-Columbia Salish has so far escaped description in the literature leaving a gap in the comparative Salishan studies that have begun to emerge. This paper describes a small fragment of Moses-Columbia syntax, the basic patterns and behaviors within the Moses-Columbia determiner phrase (DP). One of a small number of articles usually occupies the determiner position of the DP. These articles are similar in function and distribution to the demonstratives of the language, but this may reflect their shared historical origins rather than common class membership. Articles may be absent from DPs and no rules for their absence have been identified. The typical complement in DP is a noun phrase; clausal complements may also be possible. Like its three sister lan- guages - Okanagan, Coeur d'Alene, and Kalispel - Moses-Columbia does not encode a referentiality distinction through article choice, although other languages of the Interior Salishan branch appear to do so. Despite differences in detail, the main properties of Moses-Columbia DPs are similar to those in other Interior Salishan languages. Intra- group variation in the properties of articles and the role they play in the interpretation of DPs suggest a complex group history that warrants closer study.

Introduction. Since the earliest modern work on Salishan languages,
describing even the basic grammatical distinction between noun and verb has
proved difficult. All lexical w ords in Salishan can function as predicates and
inflect for person and number in apparently category-neutral ways. In the following
examples from Moses-Columbia Salish, a Southern Interior Salishan
language also known as Nxa?amxcín, all the words are inflected for intransitive
subject person.
The same set of markers used to indicate possessor person and number on
putative nouns marks intransitive person and number on putative verbs. In
the Moses-Columbia examples that illustrate this (2 and 3), the first and second
singular possessive prefix /in-/ is [i-] before [s], [in-] elsewhere.
In (4), we see a noun, inflected for possessor person, inflected as an intransitive
predicate.
Despite these readily observable data, support for the noun/verb distinction
on other grounds has burgeoned in the recent literature (e.g., van Eijk
and Hess 1986, Beck 1995, Matthewson and Davis 1995, Mattina 1996, and
Jelinek and Demers 2002) and the debate now centers on which syntactic
categories to recognize in Salishan grammar. One point of consensus in syntactic descriptions is that a division exists between "predicate" and "referring
expression." In many, perhaps most, Salishan languages, referring expressions
begin with articles or "complement particles" that, in the words of one
analyst, "precede the noun they determine" (Vogt 1940). It has become increasingly
common to refer to such expressions as determiner phrases, with
the assumption that an article heads the determiner phrase and precedes an
obligatory complement phrase.
Not all Salishan languages have had their syntax closely studied, and
those that have not stand to test the comparative generalizations to which
we have become accustomed. One such language is Moses-Columbia. Today,
this language is spoken by a small number of elders on the Colville
Reservation in north-central Washington. There being little published material
on Moses-Columbia syntax, the language has acquired a word-of-mouth
reputation for exceptional behaviors. Moses-Columbia is in fact typical of
its close relatives but differs in ways that may raise questions about some
current characterizations of Salishan languages generally. This paper presents
a preliminary description of the basic patterns and behaviors within
the Moses-Columbia determiner phrase (DP) so that additional comparative
work might be encouraged. A case can be made that Moses-Columbia
has a class of elements that function as articles in DP, for example. But, as
with so much else in Salishan, this case is fraught with puzzling overlaps
and indeterminacies, not resolved here. The data and analysis are not exhaustive.
They are drawn primarily from my 1994 - 2002 fieldwork with
Moses-Columbia speakers living on the Colville Reservation, published and
unpublished papers written by others, and notes and texts given me by the
late M. Dale Kinkade, distinguished scholar of Moses-Columbia and other
Salishan languages.
Paul Kroeber, in The Salish Language Family: Reconstructing Syntax
(1999), provides succinct characterizations of DPs in Salishan that inform
this paper. Of the Southern Interior Salishan languages Kroeber (1999:68)
writes:
The article systems of the Southern Interior languages are difficult to characterize,
in part because none of them has been very fully described. All of them
allow DPs without an overt article. DPs without an article often are found in
contexts suggesting nonreferentiality, but there also seems to be a tendency to
omit articles in the presence of a preposition. Conversely, nonreferential DPs
are often marked with a preposition, since they are likely to function syntactically
as obliques of one sort or another, for example as oblique objects of
antipassives. . . .
These statements raise a number of questions that a description of Moses-
Columbia determiner phrases ought to answer: Does Moses-Columbia have
articles? Do all determiner phrases require an article? Is article choice influenced
by the syntactic status or referentiality of the DP? I consider each of
these questions in the sections that follow.
Does Moses-Columbia have articles? 
Most Salishan languages are
described as having articles, with the caveat that they are not much like English
articles. Kroeber (1999) defines articles in Salishan as those elements
that can occupy the determiner position in DP. This definition, importantly,
does not preclude the appearance of articles in other syntactic positions, with
other functions. It acknowledges tacitly that when an article appears in the
determiner position, it is said to function as a determiner; in other positions,
an article may have other functions. Assuming that articles in determiner
function require a complement (here labeled XP, for any complement type),
we can depict DPs as having this basic structure:
[DP art [XP]]
Those sources on Salishan languages that do identify a class of articles in
a particular language do not use the term "article" in the narrow sense of
Trask's definition (1993): an element "which lacks independent meaning but
serves to indicate the degree of definiteness or specificity of the noun phrase
[with] which it occurs." Rather, they adopt a broader definition of the class,
one which includes elements that contribute deictic, pragmatic, or semantic
information to the noun phrases that follow them. In Interior Salishan, the
branch of the Salishan family to which Moses-Columbia belongs, articles include
what in English are referred to as "demonstrative adjectives" (e.g., this
and that) since they share the same syntactic position, do not co-occur with
one another, and indicate the reference of a DP. Formally, articles in Interior
Salishan are particles or clitics. Particles may bear stress if bisyllabic; clitics
are unstressed elements that require a phonological host.
Drawing on these general observations about Interior Salishan articles, I
identify a set of five articles in Moses-Columbia. These articles differ perceptibly - 
if not consistently - in form, function, and distribution from the
larger class to which they are historically linked, the demonstratives. Four of
the articles are particles; the fifth is a proclitic. The forms appear in (5).
Their syntactic position is illustrated in (6).
The four particles consist of the increment ?a- added to a demonstrative
root; for convenience these are referred to as the ?a- articles. The demonstrative
roots are reconstructible as Proto-Interior-Salishan *xa?, *ci, *-u?,
and *ni. The first three are found in dozens of lexical items, including demonstrative
pronouns, adverbials, and verb stems (Kinkade 1967). The root
*ni, though not productive in Moses-Columbia, occurs in deictic words in
other Interior Salishan languages. Three of the ?a- articles distinguish three
degrees of proximity relative to the speaker: ?axá? 'close to the speaker';
?ací 'away from the speaker'; ?a-ú? 'further away from the speaker'. The
fourth article, ?aní, makes no observable proximity distinction. Table 1 shows
the article forms in all the Southern Interior Salishan languages. Kinkade
(personal communication) notes that the Moses-Columbia ?a- increment is
likely cognate with Ok i?, since Proto-Salish *a is fronted to i in Okanagan
but retained in Moses-Columbia. Forms in parentheses are demonstratives cognate with Moses-Columbia articles. The sound correspondences are
transparent, with the exception of unexpected xw in Coeur d'Alene for
Moses-Columbia x and some reinterpretations of the vowels.
The clitic t has the distribution and function of an article but corresponds
to Southern Interior Salish forms usually referred to as prepositions. Each of
the Northern Interior Salishan languages has an article that is cognate with
t, as can be seen in table 2. Where it occurs in Interior Salishan, t is referred
to as the oblique marker because it often introduces adjunct phrases. This
case-like function has been noted by many. However, in some languages t
co-occurs with articles and in some it does not. In those languages where t
does not co-occur with articles, i.e., Kalispel and Moses-Columbia, it is held
to occur in the determiner position in DP.
Interior Salishan article inventories range from two to ten forms, descended
from perhaps a dozen demonstrative roots. Although the Moses-Columbia articles are strikingly different in form from other Interior Salishan articles,
each bears a resemblance to articles in the Southern or Northern languages.
Here, as elsewhere, some of Moses-Columbia's strangeness retreats when it
is seen against the backdrop of the entire Interior Salishan subgroup.
In an early working paper, Kinkade (1967:2) was reluctant to classify the
?a- articles as articles, preferring to call them "the weakest demonstrative
forms." He notes that the ?a- forms are not always translated as demonstratives,
"indicating a use approximating a definite article - the closest [Moses-
Columbia] comes to having an article." Yet the formal differences between
articles and demonstratives suggest that a sharper distinction should be
drawn in Moses-Columbia, as it is in all other Interior Salishan languages.
In Interior Salishan, demonstratives are typically more complex than articles,
consisting of a CV(C) formative extended by additional material, as
in Kalispel yé-?e 'this one, this here' (Speck 1980:128) or Thompson xé-
?e 'right here' (Thompson and Thompson 1992:142). They may also be reduplicated
forms, as in Shuswap Py-Pi? 'this one' (Kuipers 1974:57) and
Coeur d'Alene xw-xwi? 'this here' (Reichard 1938:656). The typical Southern
Interior Salishan article appears to be phonologically reduced from a CV(C)
demonstrative root to a C(V) clitic; demonstratives are always longer than
articles.
The contrast between article and demonstrative form suggests a pattern of
historical development that has been observed in many other languages.
Speaking of the history of articles generally, Seiler (1978:323) concludes
that "quite often the fixation potential of demonstratives gets weakened, so
[demonstratives] turn into articles." As a result, Seiler notes, "new and still
stronger demonstratives (or local expressions) have to be added." If this
observation is applied to Southern Interior Salishan, the difference between
the article and demonstrative forms might reveal that the old demonstrative
roots "weakened" - semantically and phonologically - into articles in one
historical development while new, longer demonstratives arose in another. In
this scenario, the modern bisyllabic demonstratives filled the gap created as
the old demonstratives began to shorten and function as articles.
In Moses-Columbia, this two-pronged development from common demonstrative
roots, if it occurred, did not involve phonological reduction of
the original demonstrative roots. Instead, Moses-Columbia articles and demonstratives
are all of the shape CVCV(C). The formal distinction between
the two sets is made by contrasting segments and stress rather than through
length. Demonstratives have the increment ?í- in place of the ?a- found in
articles. Concomitant regularization of the root vowel in demonstratives
results in the forms in (7). Articles are stressed on the second vowel, demonstratives
on the first. 
Determining whether modern articles and demonstratives developed
from common roots, as frequently occurs in languages, is beyond the scope
of this paper. However, Seiler's cross-linguistic observations reflect the assumption
that differences of form correspond to differences in function. That
is, a patterned difference of form between articles and demonstratives in
Moses-Columbia raises the possibility that articles and demonstratives perform
separate functions, as they do in closely related languages. Yet, if a
clear formal distinction between articles and demonstratives suggests distinct
functions, we would expect to find distributional and positional evidence
to support the case for a class of articles. Such evidence exists in
Moses-Columbia.

</file>

<file= AmE06_J36>

Civil War in Jordan, 1969-1971.
THE 1970-71 CIVIL WAR IN JORDAN fundamentally redefined what was possible in Palestinian and Jordanian politics. It also facilitated Jordanian- and Palestinian-origin distinctions in the country, which played a pivotal role in the direction taken by the Democratic Front branch that was later established in the country. The DFLP's most important central party structures and partisans were in Jordan between the front's establishment in February 1969 and the end of the civil war in late 1971. Partisans were active in the civil war itself and the rise of the Palestinian resistance movement that preceded it, and were dramatically impacted by these experiences.
The main bases for PLO military activity between 1967 and 1987 were the largely unwilling Arab host countries of Jordan and Lebanon. Jordan was the primary ground for these early activities by dint of its contiguousness with the West Bank and Israel, its large Palestinian population, and the lack of legitimacy of the Jordanian military and security apparatus with the loss of the West Bank to Israeli occupation (Brand 1998, 99). By 1970, resistance organizations expanded to such a degree that the Palestinian Resistance Movement had established parallel political, military, and social institutions in Jordan. Moreover, the influence of the PRM was such that between mid-1967 and 1971 it became the "backbone" of the Jordanian opposition, whose programs and activities became "Palestinianized," focusing on "the liberation of Palestine, anti-imperialism, and rejection of UN Resolution 242" (al-Khazendar 1997, 104-5). In turn, the identities of the PRM and the Palestine Liberation Organization were forged in relation to conditions in Jordan during this
period, and thus very much Jordanized. The constituent organizations learned from the Jordan experience to be cynical toward the professions of Arab state support for the Palestinians, the costs of frontal challenges to the authority and self-interest of Arab states, and the unlikelihood
that Palestinian self-determination could be won in all of historic Palestine. Soon after the 1970-71 civil war, the focus of the PRM turned to gaining international legitimacy for the PLO, and most Palestinian organizations came to support a two-state solution, often with ambivalence.
Buildup to War.
A month after the June 1967 Israeli occupation of the West Bank, the Jordanian government reiterated the "all-Arab" nature of the Palestinian cause and the "sacredness" for Jordan of the unity of the two banks, a bond that, according to the prime minister, "will never be broken under the leadership of King Hussein" (Abu-Odeh 1999, 140). Within two months, the East Bank, where Fateh was supported by many Jordanian military personnel and residents, became a staging ground for cross-border guerrilla (fedaa'i) raids against Israel (158). Popular support was eventually corroded by hundreds of Israeli reprisals and air strikes on Jordanian villages and Palestinian refugee camps, which drove the guerrillas deeper into Jordanian cities such as Amman (174). The Jordan regime's desire to reign in the PRM was difficult given regional popular support for Palestinian resistance, the military power and influence of Egypt (Brand 1988, 57), and the threat of Syrian intervention from the north. Nevertheless, on 2 February 1968, the Jordan royal forces attacked Fateh guerrillas in Karameh, reportedly the only area from which the guerrillas were undertaking attacks against Israel(Hawatmeh 1973, 86). Tension between the PRM and the Hashemite regime came to a head on 10 February 1970 when, after returning from a Cairo conference that included state representatives of Syria, Egypt, and Iraq, King Hussein clamped down on the PRM (Abu-Odeh 1999, 175). On 25 June 1970, Nasser accepted the U.S.-sponsored Rogers Plan (earlier rejected by Israel), and on 29 August of the same year Hussein accepted it. The Rogers Plan called for implementation of the substantive aspects of UN Security Council Resolution 242, which emphasized the "inadmissability of the acquisition of territory by war" and called for Israeli withdrawal to its pre-June 1967 borders in return for recognition of its right to exist by Egypt and Jordan. A major flaw in the Rogers Plan from a Palestinian perspective was that it would have reestablished Egyptian and Jordanian sovereignty over the West Bank and Gaza Strip, excluding the possibility of Palestinian self-governance. By 1970, increased U.S. financial aid had allowed the Hashemite regime to expand the state to the point that it was the country's largest employer of Jordanians, many of whom were now invested in its stability. Moreover, since pan-Arabist opposition organizations had disintegrated, Palestinian opposition to the state was increasingly isolated (Brand 1988, 171), although the PRM included many nonPalestinians. Confrontations intensified between the Jordanian state and the PRM, and on 15 September King Hussein formed a military cabinet and ordered the disarming of the guerrillas. The major battle of the civil war ensued from 17 September to 27 September 1970 ("Black September") in Amman and its surrounding areas, which ended with what turned out to be a brief reconciliation between Hussein and Arafat that was mediated by Nasser, who died of heart trouble on 28 September (58). Clashes between Jordanian and PRM forces continued until the second major battle between 12 July and 17 July 1971, when the army evicted the last of the guerrillas from their remaining strongholds in the mountainous woods of 'Ajlun and Jerash (Abu-Odeh 1999, 177-87). During and following the civil war, thousands of Palestinian activists surrendered to Jordanian military forces and were imprisoned or expelled, others were forced to work underground, and the PLO "closed" its institutions in Jordan. The International Committee of the Red Cross estimated that three thousand people had been killed and ten thousand wounded in the war, largely refugee-camp residents (Quandt, Jabber, and Lesch 1973, 128; Brand 1988, 171).
Modernity and Leftism in the Early DFLP.
The leading DFLP partisans brought with them the ideologies they had developed within the Arab Nationalists Movement. They believed that Zionism could not be defeated without a "national democratic revolution" that challenged the class interests of economic elites and the political interests of Arab governments (particularly Jordan), since both were local beneficiaries of imperialism (Hawatmeh 1973, 85). A modernist orientation with political, economic, and socializing aspects structured party ideology and practices. For the leftist guerrillas active in the Jordan civil war, modernity included a commitment to rationality, socialism, heavy industrialization, agrarian reform, and an organized political strategy, as opposed to feudalism, sectarianism, tribalism, and fatalism. Such an orientation would "avoid the errors of the past," particularly the military defeats that led to the colonization of Arab lands by the Zionist movement and later the State of Israel (Franjieh 1972, 76; Kadi 1969, 153). The radical socialist orientation of many of the post-1967 leftist guerrillas was constituted in relation to the worldwide revolutionary movements (and postcolonial states) of the 1950s and 1960s. Their concerns with modernity, however, were similar to those expressed by Arab organic and traditional intellectuals in their critiques and analyses immediately following the 1948 defeat (Hasso 2000).
On the ground in Jordan, DFLP partisans incorporated MarxistLeninist, Maoist, Guevaran, and other liberation ideologies and took lessons from the Vietnamese, Cuban, and Chinese revolutionary experiences, among others. Much of the DFLP's early political education, according to a leftist French intellectual who lived with the PRM in Jordan between 1969 and 1970, was designed to encourage "rational and scientific thought," in addition to "greater ideological homogeneity" (Chaliand 1972, 91). DFLP mobilizing narratives often focused on creating class consciousness:
We have learned that the liberation of Palestine is the Palestinians' own business. And the poor are the ones most able to fight for their own interests. The problem is to understand why we want to use arms: not to kill the Jews, but to liberate ourselves from all the foreign and national classes who are the cause of our poverty. Our struggle is both a national and a social [read: class] struggle. You have been stripped of everything. Why? Not by the will of God, but because of exploitation by the rich and by the Arab governments who represent them....
The government protects the wealthy classes and is always ready to knock you on the head if you protest.... It has been happening since 1948, and it happened before that. So when you fight, you are not only fighting Zionism but also to liberate yourselves. (Chaliand 1972, 112) The early DFLP leaders and cadres were greatly impacted by European New Left ideologies, and the party attracted "dozens, perhaps hundreds, of European youths who flocked to its camps" in Jordan (Y.
] Sayigh 1997, 231). A number of DFLP partisans in Jordan also reported being shaped by the anti-Soviet and anti-Maoist "third path" orientations of New Left ideologies during studies and political work in European universities (al-Nimri 2000). Jamil al-Nimri, a former partisan of Jordanian origin who was a high school student in the late 1960s, remembered a difference between the focus of the party's core activists and the competitive militia logic of building a guerrilla movement in Jordan: There was a distance between the theoretical and cultural level of the
party leaders and the system that was applied [in militia work]....And the DFLP was not different from other organizations in this respect.., because the effort was to... build a militia with the most numbers.... Of course, they worked on raising awareness, etcetera, but the process was very difficult and the lecture that we used to present was complicated-Marxist thought is not easy. So there was a problem.., between the atmosphere at the base, a militia atmosphere like the rest of the organizations, including a little bit of corruption, and between the leadership atmosphere, which was a very appealing environment. (al-Nimri 2000) Fateh, PFLP, and DFLP Perspectives on the Civil War Interviews conducted with Fateh, PFLP, and DFLP leaders in early 1972, following a period of systematic evaluation and assessment of the civil war experience, indicate that the Palestinian resistance viewed Jordan as a natural base from which to attack Israel, particularly given its large proportion of Palestinian refugees. Allowing Palestinians to fight to regain their land was seen as part of the obligations of Arab masses and states. By mid-1969, the DFLP was insistently calling for "resolving the duality of power in Jordan" between the PRM and the Jordanian government, on the recognition that the regime was making the resistance movement rather than Israel its target, and began developing "elected people's councils" to prepare for this (Hawatmeh 1973, 91-92; Y. Sayigh 1997, 248). Nayef Hawatmeh believed that the period between February and July 1970 would have been the most feasible for directly battling the regime for power, but this idea was rejected by Fateh (1973, 94). During the emergency session of the PNC on 27-28 August 1970, both the PFLP and DFLP "formally advanced proposals.., calling for the overthrow of the royal regime," and even Fateh reportedly "created a secret apparatus" to work toward this end while urging the "synthetic groups [PFLP and DFLP] to stay out of Jordanian politics" (Abu-Odeh 1999, 179, 188). According to Hawatmeh, who attended this meeting, the central committee of the PNC agreed to "organize the revolutionary process so that it would lead to the establishment of nationalist rule in Jordan" (1973, 92). After September 1970, leftist forces argued that the PRM in Jordan should go underground and "intensify the struggle for establishing a national democratic regime in the country," but "all these appeals found no response among the Fateh leadership" (94). The conflict was exacerbated by competition (muzayada) between the DFLP
and PFLP (Y. Sayigh 1997, 244). By the end of the civil war in 1971, all the leaders believed, in the words of Fateh's Khaled al-Hassan, that there was a fundamental contradiction "between the submissive nature of the regime and the militant nature of the Palestinian revolution, between the regime's will to surrender and the resistance's will to struggle" (1973, 39). According to the PFLP's George Habash, the PRM should have treated the regime as a colonial creation with goals similar to Israel's (1973, 69, 70). Habash believed that the PRM was duped, imagining "that the Jordanian regime could be friendly or neutral-because it did not obstruct the revolution after the June War, because of its own deceitful slogans, and because of the Arabic name it bears" (70).

</file>

<file= AmE06_J37>

Of all the bilateral relationships between Arab states, the Jordanian-Syrian
relationship has been among the most tumultuous. Jordanian-Syrian relations
have, more often than not, been marked by varying degrees of mutual hostility
and even violence. These periods of animosity have been so frequent that they
amounted to a local "Cold War" even in the midst of the many other conflicts in
the region. But with regime changes in both Amman and Damascus, a marked
thaw has emerged in Jordanian-Syrian relations, seemingly ending another long
period of acrimony. But this type of event has happened once before: in the late
1970s when Jordan and Syria shifted from antagonism to alliance. This article
examines both the historic and current attempts to end the Jordanian-Syrian
Cold War, so that the earlier episode may shed some light on the present and
future of Jordanian-Syrian relations.
L hroughout their histories as independent states, Jordan and Syria have had a tenuous
relationship at best, marked by temporary military alliances during wars with
Israel, but more often by varying degrees of mutual hostility. These long periods of
hostility were so extensive, in fact, that they amounted to a local "Cold War" in the
midst of the many other conflicts already operating in the region. Today, however, a
marked thaw has emerged in Jordanian-Syrian relations. The thaw began only in
1999, but within two years it had shifted already from a cold war, to a cold peace, and
then even to more meaningful cooperation and coordination. By 2001, some officials
were even talking of the potential for a full Jordanian-Syrian alliance.'
The successful ending of the Jordanian-Syrian Cold War certainly marks a new
chapter in the history of these two states' bilateral relations. But despite the long
history of acrimony between Amman and Damascus, this type of event has actually
happened once before: in the late 1970s when Jordan and Syria shifted from antagonism
to full-scale alliance. This article examines both the historic and current attempts
at bringing the Jordanian-Syrian Cold War to an end, providing an empirical
analysis of both episodes with the hope that the earlier case may shed light on the present and future of the Jordanian-Syrian relationship. This change in Jordanian-
Syrian relations marks a critical but largely unnoticed shift in the international relations
of the modem Middle East. The change is especially important in the context of
renewed violence in the region  -  from the second Intifada to the insurgency in Iraq
 -  and given the atmosphere in Washington, DC, of hostility, threats, and economic
sanctions against Syria, even as the US-Jordanian alliance seems closer than ever. In
the analysis that follows, I will first provide an overview of the history of tensions
between Jordan and Syria, before moving on to examine in tum the origins of the
1975-79 Jordanian-Syrian alliance and finally, the current warming in Jordanian-
Syrian relations since 1999.
THE JORDANIAN-SYRIAN COLD WAR.
In the 1950s and 1960s inter-Arab relations were characterized by an "Arab
Cold War."^ The Arab Cold War, usually associated with the era of Egyptian President
Gamal 'Abd al-Nasir, featured an inter-Arab struggle for power and influence between
Nasir and his Ba'thist rivals. But it also pitted conservative, pro-Western monarchies
(such as Saudi Arabia, Kuwait, and Jordan) against more radical, revolutionary
republics (such as Egypt, Iraq, Libya, and Syria). But even within that broad
regional dynamic, Jordan and Syria remained in many respects a classic study in
opposites, as well as in rivalry.'
Jordan under King Husayn became the classic conservative monarchy with a
foreign policy that was virulently anti-communist, moderate in its policies toward
Israel, and an ally of Western powers. Syria, in contrast, remained fiercely anticolonial
and became virtually the archetype of the revolutionary republic, led by
various colonels and generals following a succession of coups d'etat, until the 1970
coup brought Hafiz al-Asad to power. Asad changed the country's image as a coup
factory, establishing his own Ba'thist authoritarian regime and ruling until his death in 2000. While Jordan allied itself closely with the US, Syria allied itself with the
Soviet Union.
At both the global and regional levels, both countries had constructed identities
that stood in contrast to one another.'' The Hashimite Kingdom was, of course, decidedly
royalist, moderate in its foreign policy, cautious, and conservative. Syria was
anti-monarchist, more militant in its foreign policy, revolutionary, and radical. These
socially-constructed images are held even today by many participants and observers
of Middle East politics. But these remain constructs from as early as the 1950s, and
are today only partly accurate, and partly national stereotype. While still different
from one another in many significant ways, Jordan and Syria are no longer the stark
pair of opposites they once were. With major regime changes in both countries, their
policies toward one another have changed dramatically. But for most of their modern
histories, the two countries more often than not had maintained a cold war of their
own. Recriminations between the two were particularly harsh after the 1967 War,
given the sheer magnitude ofthe loss. Israel had launched a surprise attack (in its view
a preemptive strike) and within six days it had destroyed the air forces and later
defeated the armies of Egypt, Syria, and Jordan. For Syria, the 1967 defeat included
the loss of the strategic Golan Heights to Israel. For Jordan, the disaster had cost the
Hashimites all of the West Bank, including East Jemsalem, while thousands of Palestinian
refugees fled across the border into the Jordanian Kingdom.
In 1970, tensions exploded within Jordan in the form ofthe Jordanian Civil War
between the Hashimite armies of King Husayn and the guerrilla forces of the Palestine
Liberation Organization (PLO). This bloody affair, known to Palestinian nationalists
as "Black September" and to some Jordanian nationalists as "White September,"
was not limited to Palestinian-Jordanian fighting alone. Indeed, in September 1970,
Syrian military forces crossed the Jordanian border, launching an unsuccessful invasion
of northern Jordan in support of PLO forces. The Syrian forces were eventually
defeated and turned back, but thereafter any Syrian military build-up on the border
had to be regarded in Jordan as more than a hypothetical threat.
Yet, by October 1973, Egypt and Syria had colluded in launching an attack on
Israel, later supported by the Saudi-led oil embargo, in a conflict known alternatively
as the October, Ramadan, or Yom Kippur War. Jordanian forces refused to open a
third front against Israel, but did send minimal forces to defend Syria on the Golan.
According to one of Jordan's top generals at the time, war was not an option for
Jordan, still recovering from the debacle of the 1967 War and the disastrous 1970-71
Civil War:
We lacked air defense, and sufficient numbers of troops. And in addition to
that, they [Egypt and Syria] did not choose to inform Jordan in the first place
[about the planned attack]. And they then tried to drag Jordan in, with inevitable catastrophic effects. I asked [Syrian President] Asad, why do you now
ask us? Asad argued that there were political advantages to be reaped. But I
responded that there will be no Jordan left to reap the advantages. If Syria
liberates the Golan, Egypt liberates the Sinai, then you've got a point ... We
convinced them, that if Jordan joined the battle, it would become the tnain
target of the Israelis. We were dragged into the 1967 War and we lost the West
Bank. We couldn't afford to lose the East Bank ... Asad seemed to understand
our position, but he still disagreed. But what they were facing in the Golan
and Sinai still left the vast majority of Israeli forces facing Jordan ... Anyway,
we did compromise. We did send units to the Golan. But this did not involve
direct confrontation with Israel through our lines.
The gesture may have seemed small, but it was iti the aftermath of that campaigti,
between 1975 and 1979, that Jordan and Syria would finally shift from hostility to
alliance.
By the end of the decade, however, the more familiar pattern of animosity had
returned. Thus by 1980, Jordan and Syria had de-aligned once again with extensive
saber-rattling on their mutual border. The Jordanian-Syrian Cold War had indeed
returned in full force. Throughout the 1980s, Jordan supported Iraq, while Syria
supported Iran in the eight-year long Iran-Iraq War. But beyond throwing their support
behind opposite powers in the 1990-91 Gulf War, the tensions between Jordan
and Syria also had profound domestic consequences as each intervened in the domestic
politics and stability of the other.
It remains unclear how direct a role either government played in the destabilization
that followed, but suffice it to say that each blamed the other, in particular for
subverting domestic security. The Syrian government specifically charged the Jordanians
with providing aid, support, and sanctuary to Syrian Islamists. Even Jordan's
own prime ministers do not agree on this point. Former Prime Minister Ahmad
'Ubaydat (1984-85), for example, vehemently denied that any such subversion ever
took place with support from the Jordanian government. One of his rivals, also a
former prime minister, suggested that Jordan did indeed support the Muslim Brotherhood
in Syria, but that this policy (a mistake in his view) was carried out without the
knowledge of King Husayn. Another former prime minister argued that Jordanian-
Syrian problems remained broader still, and were rooted in disregard for the other's
sovereignty and security:
They [the Syrians] have even at times claimed us as southern Syrians. Then
again, we Hashimites have at times seen them as our northern inheritance. But Syria likes to be the power. It likes its hegemony. But Jordan could never
accept this. Syria works through Lebanon or through Jordan to irritate Israel.
But we resent this. It also affects our own security.'"
Regardless of whether opponents of either regime received external support or not, in
many respects the 1980s version of the Jordanian-Syrian Cold War appeared to have
become a mukhabarat v. muiciiabarat struggle, in a sort of localized version of the
global US-Soviet Cold War and its associated espionage campaigns.
In 1990, after Iraq invaded Kuwait, Jordan and Syria took almost opposite stands
once again  -  but in each case, surprising many observers. Syria elected to join the
US-led coalition against Iraq, while Jordan attempted to mediate between the antagonists
(unsuccessfully) while calling for an inter-Arab solution rather than foreign
intervention. Jordanian policy-makers argued that Jordan's position was the defmition
of balance: neither sending troops to defend Iraq, nor joining the US-led military
coalition.  Three years after the Gulf War, in 1994, Jordan signed its peace treaty
with Israel, but more than a decade later, Israeli-Syrian relations remained coldly
hostile. Throughout these decades of policy differences and animosity, the Jordanian
and Syrian regimes differed not only on relations with Israel, but also on relations
with the PLO. One Syrian policy-maker, and sometime advisor to President Hafiz al-
Asad, emphasized this as a central factor in inter-Arab politics, but particularly so in
Jordanian-Syrian relations and rivalry. Regarding this time period, he argued that:
The nnost important factor in inter-Arab relations is each country's relationship
with the PLO. Jordan and Syria are the only two Arab states adjacent to
Israel who are eligible  -  after [Egyptian President] Sadat's visit to Jerusalem
 -  to "control" the PLO, let's say. So the question is who will exercise more
control over the PLO, Amman or Damascus?
Beyond even the causal factors, however, the Jordanian-Syrian Cold War seemed to
have developed its own tragic inertia. Indeed, beyond even policy differences, the
level of mutual suspicion and distrust between the two regimes also became deeply
personally entrenched in the persons of President Hafiz al-Asad and King Husayn.
With the passing of these two powerful antagonists, and the emergence of a new
generation of leaders, Jordan and Syria were able to make a second attempt at ending
their longstanding cold war. Thus, for decades, the prevailing trends in Jordanian-Syrian relations amounted simply to varying degrees of hostility  -  from diplomatic rifts, to political threats, to actual acts of military and civil violence.

</file>

<file= AmE06_J38>

Nachituti's Gift.
The Kazembe Kingdom and Owners of the Land.
My first encounter with the story of Nachituti was in October
1997, during a visit to Lunde, the royal graveyard of the Kazembe rulers. Mwata Kazembe XVIII Munona Chinyanta, who died less than one year later, invited me to a ceremony to introduce me to Lunda customs; he also wanted me to photograph the ceremony and to transport
dignitaries in my vehicle. The ceremony was unprecedented. Before his
imminent death, Munona wanted to pay respect to his ancestors and ensure that the graveyard was clean and well maintained. En route to the graveyard, situated in Chief Kanyembo's area about five miles off the main road that runs through the valley, our trucks stopped at streams and termite mounds, which signified past boundaries and sacred places of rest. Lunda aristocrats fired muzzle-loading guns at each stop, and the Mwata requested permission from the gravekeepers and owners of
the land to pass through their lands. When we finally reached the graveyard, the rough dirt track, which resembled no more than a footpath, ceased at a wooden barrier beyond which lay a field recently cleared of bush scrub and trees. We climbed out of our trucks, took off our shoes, and tied white cloth on our arms, for we were about to enter the burial place of chiefs and needed to show respect and protect ourselves from their spirits. We proceeded from one grave to the next with the keeper of Lunde reciting the praises of the dead kings and, together with the Mwata, giving libations of millet beer and throwing white earth called impemba (or ulupemba). Munona then placed copper plaques on the twelve previously unmarked mounds (five kings are buried outside the royal graveyard). After a drenching thunderstorm, we trudged in the mud for about one hundred feet to another grave on the outskirts of Lunde, where the Mwata began to give offerings. There was no copper plaque for this grave, so I asked my research assistant to whom it belonged. 
"Nachituti," I was told, "the queen of the Lunda." His answer was unexpected. Although I knew that the people of Luapula had strong matrilineal affiliations and women had often attained 
positions of political significance, I thought that royal and aristocratic descent in Kazembe's eastern Lunda kingdom was patrifilial, with women playing a relatively minor role in the affairs of state. Yet here was a woman important enough to be buried next to the royal graveyard, the
highest honor for anyone associated with the eastern Lunda kingdom. During that research trip, I heard more about Nachituti, especially from the elders in their stories of how the Lunda came to rule Luapula. I reread the classic ethnographic and missionary texts about the Luapula Valley and indeed found that Nachituti featured in the idiosyncratic narratives of the missionary Dan Crawford, the articles and book by the Rhodes-Livingstone anthropologist Ian Cunnison, and the oral history of the Lunda written down by a collection of elders under the direction of Mwata Kazembe XIV Chinyanta Nankula. Yet her importance, and that of the valley's matrilinealages in general, were always submerged under the history of the eastern Lunda kingdom and the Kazembes patrifilial descent, first drawn by Ian Cunnison and since then frequently reproduced and much discussed (for an updated version see figure i). I returned to the Luapula Valley two years later, convinced that my previous study had underestimated the importance of the stories of the valley, and especially that of Nachituti. I suspected that the people of Luapula explained the elusive rules that governed the politics of access to their resources-the land, rivers, and lakes of the valley-and articulated relationships between owners of the land and lagoons and the Kazembe Kingdom through such oral traditions. To further explore these stories and their political impacts, I extended a study of patterns of production to consider how productive resources were embedded within cultural narratives. Specifically, I examined how narratives provided a "constitution" that explained and legitimized resource access at least until the coming of the colonial states in the early 189os, and even afterward.
Historians and anthropologists have long noted that narratives, especially those that describe traditions of "genesis," are ways of describing contemporary culture, society, and political struggles rather than historical facts. Even Jan Vansina, who defends the historical validity of
oral tradition against the many critiques of structural anthropologists, admits that such traditions "are expressions of present and past world views, that may reflect past events but do not necessarily do so."
Vansina describes oral traditions as "palimpsests" that do not "reflect  exactly and necessarily the world view held now, but still incorporates elements of different ages." Used with care, the intellectual or cultural historian can employ an oral tradition to explore the relationship between cosmologies and sociocultural and political change. A bourgeoning field in historical anthropology has been the exploration of how hegemonic fictional narratives articulate with and impose themselves on existing cosmologies, especially in processes of conversion, where biblical narrative sequences influence the representation of everyday life and hence quotidian agency in social, political, and economic realms. John and Jean Comaroff, for example, argue that missionary narratives replaced southern Tswana forms of representation and became instruments of cultural hegemony; J. D. Y. Peel has offered a more dialogical approach whereby biblical narratives appear persuasive to potential converts when they adapted to existing, vernacular narratives. It is because of the strength of previous narrative traditionsrather than their weakness, as argued by the Comaroffs-that biblical narratives became so influential.5 In the case of northeast Zambia, Robert Cancel has demonstrated a narrative tradition rich in allegory. The Nachituti narrative provides an opportunity to explore the relationship between narrative and social agency in a precolonial setting. In telling and interpreting Nachituti's story, I am less concerned with historical accuracy than with the fiction that has provided the idiom and ideology that structured political relationships between the peoples of the valley and allotted "charters" of rule and ownership over the land and lakes. Rather than verify the story of Nachituti's gift, this chapter explores how the story came to define Lunda political control and Shila ownership over the land and lakes. The purpose is not to deny or prove the historical validity of oral traditions but rather, followingJ. M. Schoffeleers, to demonstrate some historical dimensions to the ideological confrontations found within oral traditions. In doing so, this chapter will not provide a historical reconstruction of the Kazembe Kingdom; those interested in a detailed history of the kingdom should consult the work of Giacomo Macola.8 However, we will locate the emergence of the Nachituti narrative in the early nineteenth century and follow its spread as a charter of rule and ownership in the latter half of the century when the Kazembe Kingdom weakened and came to rely on the agency of dispersed aristocrats and owners of the land and owners of the lagoons (abine ba mpanga/kabanda, sing. umwine wa mpanga/kabanda).
In the Luapula Valley, a story about the past was an integral part of the present. Characters in historical narratives were not ossified in the past; ancestral personae were adopted in the present through "positional succession" and old kin relations kept alive by what Ian Cunnison 
called "perpetual kinship." Historical narratives defined political titles or offices, which would have an ongoing historical presence. Thus, the stories of the characters in Nachituti's narrative, like "Mwata Kazembe," "Nkuba," "Kalandala," and "Nachituti" herself, could refer to the activities of a number of different individuals who occupied that title at different times (with the partial exception of the Kazembes, who have distinct historical personae due to praise poems for each "Kazembe" combined with a chronological king list, an invention of the twentieth century). If the dynamics of this system are not well understood, attempts at historical reconstruction will be flawed; on the other hand, historians possess raw material in the form of living political relationships that are the articulation of past kin ties. Details about identities and relationships between lineages were preserved as political offices as long as the ancestors continued to be inherited by the living. Past kin relations thereby had ongoing implications for contemporary life. This was a two-way process: contemporary conflicts also impacted on
the history that was told. For example, a fight between perpetual relatives, say an uncle and nephew, might have transformed the emphasis of a story or superimposed another narrative on the original. Once we understand the dialectical interplay of past and present in Luapula, we can begin to explore the web of relationships between the narration of a story, the politics of the narrators, and the making of history. The story of Nachituti's gift is a story of two gifts, both of which describe relationships of economic and political reciprocity between giver and recipient." The first gift is the head of Nachituti's brother, Nkuba, which the Lunda aristocrats, Kalandala and Nswana Ntambo, gave to Kazembe. By doing so, Kazembe was indebted to the Lunda
aristocrats for his first bride, Nachituti, freed from her dead brother, Nkuba. The giving of this gift was reenacted through the continued giving of aristocrats' sisters and sisters' daughters as the Kazembes' brides (ntombo). Kazembe thereby remained obligated to his aristocrats, who 
described themselves as "owners" of the king. The second is Nachituti's gift of the land and lakes to Kazembe; this ensured that Kazembe was indebted to Nachituti and recognized her people, the autochthones of the valley, as owners of the land and lagoons. Nachituti's people continued to give Kazembe produce from the lands and lakes-cassava, millet, game meat, fish, and other wild produce-and in return Kazembe bestowed on their leaders the titles of owner of the land or owner
of the lagoon. 
The Narration.
Eastern Lunda oral traditions link the beginnings of the Kazembe Kingdom in the Luapula Valley to a series of encounters between emissaries of Mwaant Yaav, the central Lunda paramount, and southern
Luba lineage heads. Chinyanta, a warrior who had previously surrendered to Mwaant Yaav, led an expedition to Luapula to find the Lunda blacksmith Lubanda, who had fled some years earlier. Instead, Chinyanta found salt pans. When he attempted to inform Mwaant Yaav of his lucrative discovery, Mutanda, the owner of the salt, murdered Chinyanta by drowning him. Mwaant Yaav appointed Chinyanta's son, Ng'anga Bilonda, as "Kazembe" and told him to avenge the death of
his father by killing Mutanda. These are the murky tales of the beginnings of eastern Lunda rule
on the Mukelweji and Luapula Rivers. We know with some certainty that Kazembe conquered and settled in the Luapula Valley toward the middle of the eighteenth century. The Kazembe or eastern Lunda entourage called those whom they conquered 'AbaShila," meaning fishers; this became a generic term for Luapula's autochthones. The most famous and common story of eastern Lunda conquest of the Shila, which is still told in the valley and has become part of the "universal"
history of the valley accepted by both Shila and Lunda narrators, is related to a period of Lunda expansion under Mwata Kazembe I Ng'anga Bilonda's son, Mwata Kazembe III Ilunga. In the first part of this chapter, I will concentrate on the telling of the story in the late twentieth century and suggest some reasons for its particular emphasis. The remainder of the chapter will locate the emergence of the story in the mid to late nineteenth century. It was during the reign of Mwata Kazembe III "Ilunga" [approx. 1760 to 1805]. The Lunda capital was at Kasankila near Mofwe
Lagoon. At that time, the Lunda did not cultivate land, they did not even catch fish. This is why they had to be in the middle of the Luapula Valley, where the Shila people could bring them food.
Nachituti and Nkuba lived on Chisenga Island. Nkuba had many wives and among them he loved one greatly. One day he found his nephew, the son of Nachituti, with this most beloved wife. In a jealous rage, he killed his nephew, skinned him, carefully dried the skin, and kept it under his mat.

</file>

<file= AmE06_J39>

Production and Consumption: Establishing Priorities.
But the point of departure must be to make do with what we have - Bruno Leuschner, July 1947.
The best provisioning to the best worker - Die Versorgung, November 1947.
In the years immediately following the Second World War, Germany
was a site of destruction, chaos, and misery. Its major cities were in ruins; its transport network was severely damaged; its economy had collapsed. Its inhabitants, uprooted and traumatized, presented a vision of the despairing and the demoralized: "rubble women" and refugees,
returning soldiers and "displaced persons," the abject defeated and
their foreign occupiers. Having controlled nearly the entire European
continent at the apogee of its wartime power, Germany now found itself at the mercy of its wartime enemies.
For most Germans everyday life was now defined by the struggle to
survive in the face of chronic hunger, inadequate housing, a lack of
heating fuel and other basic commodities. To capture the desperate,
febrile quality of life in these years, historians have employed labels like
the "survival society," the "society in collapse," the "rations society," the
"society of provisioned classes.", With daily rations falling far below the
two thousand calories calculated by the League of Nations as the minimum requirement for a working adult, people had no choice but to steal, engage in furtive black market exchange, make repeated foraging trips to the countryside in overly packed trains, and scrounge for
heating fuel amid the twigs and tree branches of city parks.
Further heightening the sense of insecurity and confusion were the
many administratively launched upheavals. Throughout occupied Germany authorities made efforts to denazify and reeducate, to dismantle industrial plant and extract reparations, to reform patterns of landholding and reorganize business structures, to reestablish old political
parties and create new ones. And if we widen the lens further, we bring
into view the breakdown of the wartime alliance and the emergence of
the Cold War, the fault lines of which, running through German territory, introduced cause for even more profound uncertainty about the future.
In histories of the occupation period the year 1947 is commonly
viewed as the year of transition. Through a rapid succession of events it
signaled an alarming increase in tension between the Allies, a shift on
the part of occupiers from policies of punishment to those of reconstruction, and concomitantly a decisive shove forward along the path toward the division of Germany. The first half of the year alone witnessed the economic fusion of the American and British zones into the Bizone (January), the proclamation of the Truman Doctrine (March), the failure of the Moscow Foreign Ministers Conference (March-April), and the famous Harvard speech of U.S. Secretary of State George Marshall, announcing the plan that became most widely associated with his name (June).
On the eve of Marshall's speech, the Soviet Military Administration in Germany (SMAD) made its own contribution to the widening rift by creating the German Economic Commission (Deutsche Wirtschaftskommission or DWK). The DWK was to be the first centralized Soviet-zone  administration run by Germans and, as such, the first forum for developing the principles and policies of economic planning. No task faced by the apprentice planners of the DWK was more formidable than that of provisioning consumer goods under the difficult living conditions
of the immediate postwar years. Nor was their task eased by the vision
for reconstruction pursued by Soviet-zone authorities. Under close Soviet supervision, DWK planners drew a road map marked primarily by metallurgy, machine building, coal mining, and electrical engineering.
The path to a better future, as they conceived it, lay exclusively in the privacy of heavy industry. This emphasis on capital goods, however, necessarily came at the expense of consumption goods. And yet the consumption needs of Germans could not be entirely ignored. At the
most basic level, workers had to be fed, clothed, and housed if they
were to be productive. As the political confrontation between East and
West intensified, Soviet-zone authorities had to take steps to garner the support of the inhabitants in their zone. This chapter shows how those authorities began to try to accommodate the requirements of consumption within the larger imperative of production.
One thing DWK planners had working in their favor was the fact that the experience of hunger and food shortages was hardly novel. For nearly thirty years the lives of Germans had been punctuated by food crises; for many, these decades had consisted of recurrent bouts of austerity. Most adults in 1945 had personally lived through, or grown up hearing about, the terrible "turnip winter" of 1916-17. They remembered how slowly the food supply improved after the First World War,
only to be again thrown into chaos by the Great Inflation of 1923.
While the stabilization of the second half of the 1920s brought relief,
even prosperity, to some, low-income earners continued to struggle to
feed themselves and their families. Even during the "golden years" of
the Weimar Republic, millions remained unemployed--2 million in
1926, that is, 10 percent of the working-age population and 18 percent
of trade unionists. The onset of the Great Depression necessarily broadened and deepened their plight. In 1930 the number of unemployed reached 4.5 million; by January 1933, when Hitler became chancellor, it had grown to over 8 million. Although the Nazi regime managed to put people back
to work in the mid-to-late 1930s, there continued to be persistent complaints about shortages and rising prices for all kinds of basic commodities, as not only the reports of the German Social Democratic Party in Exile (SOPADE) but also those of the Gestapo clearly show.
For consumers the Nazi years were characterized by a combination of seductive promise and material frustration or, as one historian has recently put it, "enticement and deprivation." The regime's propaganda tantalized with images of mass access to new kinds of consumer experience, most notably its "strength through joy" vacations, radios, vacuum cleaners, refrigerators, cars, and early television sets. But with the exception of the cheap, standardized "people's radio," which appeared
to serve the propaganda needs of the regime, these promises were never, or only slightly, fulfilled during the Third Reich. With its economy geared toward preparing for war, its foreign currency reserves in desperately short supply, and its hope, never realized, of achieving a
state of economic self-sufficiency, the regime did what it could to suppress private consumption. To soak up excess purchasing power, it increased taxes and allowed prices to rise faster than wages. In the hopes of "guiding" consumption, it appealed to consumers to "buy
German" and to accept ersatz products in place of foreign or scarce commodities. It launched propaganda campaigns to encourage saving and to fight against all forms of waste. Consumers were less responsive to these propaganda efforts than Nazi authorities had hoped they
would be. Still, they faced shortages in textiles and foodstuffs. As early
as 1937, butter, margarine, and other fats had to be rationed. With the introduction of extensive rationing in August 1939, on the eve of the German invasion of Poland, there was much grumbling from the population. But the rationing system had been so painstakingly
well prepared by Nazi authorities that people more or less grew accustomed to it. The Nazi leadership had been determined to avoid what it regarded as one of the great mistakes of the previous war-that is, allowing the food supply to deteriorate to the point that it weakened
morale on the home front. Although there were complaints about the uniformity of rationed supplies, the regime managed in the first years of the war to feed its population sufficiently well, assuring "normal consumers" about twenty-three hundred calories daily. But as the tide of
the war turned in 1943, and it was no longer possible to supply Germans with the goods plundered from the territories conquered by the Wehrmacht, the orderly system of provisioning gave way increasingly to improvisation and shortages. By spring 1945 authorities had to extend
the validity of ration cards already distributed in February and March,
since it was impossible to allocate any new rations for the first week of
April. The ration cards themselves, meanwhile, had long since become worthless, as black markets flourished. In the end, as the military effort inexerably disintegrated, collapse on the home front could not be avoided. Still, the Nazi system of rationing had functioned well for much of
the war and, with its reputation largely intact, was continued in the Western occupation zones." In the Soviet zone, however, the allocation of rations followed a combination of Soviet and Nazi wartime practices. Here the population was divided into three main groups: normal consumers (Normnalverbraucher), partial self-providers (Teilselbstversorger), and self-providers (Selbstversorger). The normal consumers, who comprised the overwhelming majority of the zone's population, were then broken down into six smaller groups, each receiving a different ration
card based on occupation and age. In order of decreasing rations, cards were distributed to workers performing the heaviest manual labor (Card I), those performing heavy manual labor (Card II), workers (Card III), employees (Card IV), children (Card V), and the rest of the
population (Card VI). In the Western zones, too, there was a connection between heavy
manual labor and better rations. Workers performing heavy manual labor in Munich, for example, were "honored" in 1947 with 1,090 extra calories per day, which represented 83 percent of normal consumer rations. Similarly, miners in the Ruhr were rewarded according to a points system. But in the Soviet zone, Cards I and II were also given out to engineers and technicians, political leaders and administration officials, intellectuals and artists, and even some professionals. The inclusion of these groups in the highest ration categories, justified by "the great economic and social significance of their work," reflected distinctly Soviet practices in the allocation of scarce commodities, one aim of which was to win over and build up a reliable leadership cadre.
In direct contrast, the members of these groups in the Western zones were among those at the bottom of the rationing hierarchy. Moreover, in the Soviet zone heavy manual labor did not in itself guarantee receiving Card I or II. One had to work in either mining or heavy industry. This requirement, too, reflected a particularly Soviet emphasis that, as we shall see, was soon extended on a far greater scale than was ever done in the West.
In addition to the categories of occupation and age, the size (and thus political importance) of one's place of residence also played a role in the early stratification of provisioning. The population was divided into a four-tiered residential hierarchy in which the Soviet sector of
Berlin came first, followed by the two largest cities in Saxony, Dresden and Leipzig. These, in turn, were followed by the other sixteen large cities of the zone, which were finally followed by the smaller towns, rural communities, and villages. The differences between these categories, it should be added, were significant. For example, in January 1947 official daily rations ranged from nearly twenty-five hundred calories for a worker performing heavy manual labor in East Berlin to just over one thousand calories for a holder of Card V in one of the smaller locations of the zone. Very often, however, even these desperately inadequate rations could not be fulfilled, and the actual amounts given out fell well below the official levels.
It was with the help of trusted house and block representatives that zonal administrators distributed ration cards to the population. Upon presenting their work certificates to these representatives, individuals would receive their corresponding cards. The cards for most consumer goods retained their validity for a period of ten days, partly because officials wanted to limit attempts at speculation and partly because most food items would not keep for longer periods. Not only were consumers required to shop within the circumscribed time period, but they also had to shop at designated stores in their immediate neighborhoods, where they were registered. Individual stores received their deliveries only after preregistering their customers' cards. (In
East Berlin this system of binding consumers to particular stores lasted until 1951.)

</file>

<file= AmE06_J40>

Abstract.
This paper contributes to portfolio selection methodology using a Bayesian forecast of the
distribution of returns by stochastic approximation. New hierarchical priors on the mean vector
and covariance matrix of returns are derived and implemented. Comparison's between this
approach and other Bayesian methods are studied with simulations on 25 years of historical
data on global stock indices. It is demonstrated that a fully hierarchical Bayes procedure produces
promising results warranting more study. We carried out a numerical optimization procedure
to maximize expected utility using the MCMC (Monte Carlo Markov Chain) samples
from the posterior predictive distribution. This model resulted in an extra 1.5 percentage points
per year in additional portfolio performance (on top of the Hierarchical Bayes model to estimate
l and R and use the Markowitz model), which is quite a significant empirical result. This
approach applies to a large class of utility functions and models for market returns.
Introduction.
Portfolio theory is concerned with the allocation of an individual's wealth among
various available assets. The basic Markowitz version of the portfolio selection problem
is (Markowitz, 1952).
W = (w1,w2, . . .,wm)Rm is a column vector of proportions representing a
portfolio of assets, R and l are the covariance matrix and mean column vector of
asset returns y = (y1,y2, . . .,ym)0, k is the investor's risk-aversion parameter, and e
is a column unit vector. For no short sales restrictions, an additional constraint,
wi>0, can be added. This portfolio selection approach is termed the Mean - Variance
(MV) method because it ranks portfolio weights by their mean - variance pairs.
The set of optimal portfolios obtained as the level of risk-aversion, k, varies is termed
the Markowitz efficient frontier.
The Markowitz MV method can be viewed as maximizing expected utility. For
example, if the investor's current wealth is W0, his terminal wealth is
W = (1 + wy)W0.
According to Von Neumann and Morgenstern axioms, the investor determines w
by considering the expected value of a non-decreasing utility function of W. Using
the exponential utility function,
U(W)=-exp(W) and assuming y is distributed multivariate normal N(l,R), the maximization of
expected utility reduces to ranking MV portfolios using (l,R;k) in model (1).
Classical portfolio selection uses least-squares estimates of (l,R) in model (1).
However, MV portfolio selection based on estimates of population moments leads
to a problem of estimation risk that arises from the difference between the estimates
and the true parameter values. It has been well documented that the problem of estimation
risk is significant (Dickinson, 1974; Putnam and Quintana, 1991; Pari and
Chen, 1985; Frankfurter et al., 1971; Jobson and Korkie, 1980). Empirical studies
of estimation risk associated with least-squares estimates appear in Levy and Sarnat
(1970), Solnik (1982), Board and Sutcliffe (1992), Chopra et al. (1993), Chopra and
Ziemba (1993). All of these studies conclude that resulting portfolios involve either
extreme volatility or lack of diversification.
The use of Bayes and empirical Bayes estimators to estimate (l,R) have been
advocated by several researchers (Brown, 1976; Bawa et al., 1979; Jorion, 1986;
Frost and Savarino, 1986). Jorion (1986, 1991) employs Bayes modifications of
James - Stein shrinking formulas (James and Stein, 1960) to estimate l, while Frost
and Savarino (1986) employ empirical Bayes estimators of l, assuming R has intraclass
structure. They show through simulated and historical data that MV portfolios
using their respective Bayes estimates in model (1) dominate MV portfolios using
] classical least squares estimates. See also Kadiyala and Karlsson (1997), Kandel
et al. (1995), and Shaken (1987).
This paper examines a fully hierarchical Bayes model for (l,R). These models are
multivariate and thus can capture more complete information on the interdependence
between assets than previous models. Although these models are dependence
between assets than previous models. Although these models are cross-sectional,
one-step forward forecasts based on the posterior predictive distribution of returns
are available for ranking portfolios. The posterior predictive distribution has been
proposed for forecasting univariate ARMA models since Zellner (1971); also see
West and Harrison (1989). This paper will empirically demonstrate that Bayesian
forecasts are superior to moment estimates in portfolio ranking. Moreover, this approach
applies to any utility function.
Marriott et al. (1993) show how to obtain the predictive distribution for a vector
of future values via the Gibbs sampler and Monte Carlo integration. Kim et al.
(1998) exploit MCMC sampling methods to provide a practical likelihood based
framework for the analysis of stochastic volatility models. These methods are used
to compare the fit of stochastic volatility and GARCH models. Nakatsuma and Tsurumi
(1996) compare small-sample properties of Bayes estimation and maximum
likelihood estimation (MLE) of ARMA-GARCH models using MCMC sampling.
McCulloch and Tsay (1994) use the Gibbs sampler for Bayesian analysis of AR models.
This paper also exploits MCMC sampling methods to obtain a practical stochastic
approximation to the posterior predictive distribution and its moments.
This paper is structured as follows. The definition of posterior predictive distributions
is given in Section 2. Maximum expected utility is defined in Section 3. In Section
4, we describe data on eleven country-stock index funds provided by Morgan
Stanley Capital International. In addition, designs for comparing the different Bayesian
models are described. Bayesian data models including the fully hierarchical prior
are explained in Section 5. Sections 6 and 7 describe the results and conclusion,
respectively.
Bayes posterior predictive distributions.
Denote observed returns on m assets by y and future, or unobserved, returns by ~y.
Let 0Rp and 0Rq denote p parameters and q hyperparameters, respectively. The
parametric family of the joint likelihood of y and ~y will be denoted by
f(y;~y0)=f(y;y)and depends on the joint parameters only through the lowlevel
parameter h. Denote the prior distribution of (/,h) by p(/,h) = p(/)p(hj/).
Non-hierarchical models fix / and compute posterior distributions using the prior
p(hj/), while hierarchical models compute posterior distributions using the joint
prior p(/,h).
In the portfolio selection problem, h = (l,R) and / will represent a vector of
hyperparameters in the prior for (l,R). Portfolio selection using posterior predictive
distributions addresses two unknown quantities, ~y and (/,h), with the primary
goal being to gain information about ~y with (/,h) as nuisance parameters. The advantage of the hierarchical model, with priors instead of point estimates of
hyperparameters, is that the posterior distributions will reflect the appropriate
uncertainty in the hyperparameters. The disadvantage is that the posterior predictive
distribution will not be analytically tractable usually; however, the method
based on the MCMC sampler provides a stochastic approximation of the posterior
predictive distribution.
According to the likelihood principal all evidence about (~y; /; hÞ is contained in
the joint likelihood function f ðy; ~yj/; hÞ (for an overview see Bjørnstad, 1990). Based
on this likelihood, we wish to develop a posterior predictive distribution for ~y,
pð~yjyÞ, by eliminating (/,h) from the joint likelihood. The Bayes approach for this
problem is to integrate out (/,h) using the joint prior. The resulting predictive distribution
for ~y given the data, y, is the following.
A stochastic approximation of the posterior predictive distribution is generated by
simulation, using the MCMC sampler if necessary, using (h,/) ~ p(h,/jy) to generate
~y distributed with density f(yjh,/), and repeating these steps to obtain more simulated
observations.
Maximum expected utility.
For the reader's convenience, we repeat the basic notation. Let w 2 Rm denote
portfolio weights satisfying wiP 0, i =1,. . .,m; P
i¼1;...;mwi ¼ 1. The inner product
w0~y is the portfolio-return on future investment performance. An investor will choose
a utility for wealth. This utility is denoted by a monotonically increasing, concave
function U(W;k):W ! R, where k P 0 is a fixed parameter denoting risk-aversion.
The posterior expected utility of W ¼ ð1 þ w0~yÞW 0, where W0 is initial wealth, given
the data y is:
E(U(1+wy)W0)y)=fU(1+wy)W0)(yy)dy
This expectation exists under standard regularity conditions.
The direct utility (DU) optimal portfolio model is a solution to the following
model:
Maximize E(U(1+wy)W0y);
subject to we=1;
where wRm. Typically, model (6) must be solved with a non-linear optimization
algorithm. Many standard algorithms exist, such as sequential quadratic programming
(see Gill et al., 1981, p. 237; Schittowski, 1980, 1985) as implemented in MATLAB,
enabling solutions for any utility function that is twice-continuously
differentiable. In case the expected utility is not analytically tractable, it is necessary to contemplate samples from the posterior predictive distribution that can be used to
approximate the expected utility. This is an advantage since portfolio selection can
be carried out with a general utility function. Given a sample, ~y1; . . . ; ~yK, from the
predictive posterior distribution, the direct utility portfolio selection problem (6) is
approximated by the model:
Maximize K-1(U(1+wy)W0)
subject to we=1.
For the empirical study we the use exponential utility, U(W;k) = Ąexp[kW].
Empirical data analysis.
The daily stock market indices for 11 different countries over the period 1975 - 
2002 are used in this comparison of DU and MV using different data models. The
countries include US, UK, Canada, Belgium, Australia, France, Japan, Austria,
Spain, Germany, and Hong Kong. The indices are compiled and provided by Morgan
Stanley Capital International. Monthly returns were computed as the percentage
changes in the index between consecutive last days of the month. Morgan Stanley
Capital International provided two indices per country, one in local currency and
one in $US. Our study is based on the returns in $US.
The data are partitioned into five periods of five consecutive years. This allows a
comparison of the means, standard deviations, within-country serial correlations and
between-country correlations.
Comparison design.
To examine the performance of different models, 276 out-of-sample periods of
one months each, covering the period January 1980 - December 2002, are used. That
is, the first data set is from January 1975 - December 1979 (60 monthly observations)
and the first out of sample observation is for January 1980. Our last out-of-sample
portfolio is for December 2002. For a given model, we run the MCMC sampler independently
on 276 data sets, and use the individual posterior distributions to form the
portfolio. Our procedure can be summarized as follows:
1. Use 60 observations (initially those for months 1 - 60) to generate the joint posterior
distributions of the means and covariances (via the MCMC sampler) and, in
accordance with the decision theory rules, compute the posterior means of these
distributions.
2. For a given k, the risk-aversion parameter, find the investment proportions w.
3. Apply these proportions to the actual returns observed in the next month to
obtain the actual portfolio return for each model and value of k.
4. Roll the sample forward by one month, e.g. months 2 - 61, and repeat steps (1) - (3).
This resulted in 276 sample periods being used. A common value, k = 0.02, of the
risk-aversion parameter was used. Computational time prevented expanding the procedure
to a range of k's in this study. This is planned for future research.
Bayesian data models.
We will examine two hierarchical data models, and apply MCMC sampling to obtain
estimates of the posterior distribution of (l,R) and ~y. These estimates will then
be used to solve models (1) and (6) for MV and DU portfolios respectively.
All models, even non-Bayesian, can be specified within a hierarchical Bayesian
structure. In addition, the MCMC sampler can be used to solve even the simplest
model (while a closed-form analytical solution may exist, it may still be easier to
run the MCMC sampler to generate the posterior distributions). The following three
data models will be tested empirically.
The degrees of freedom parameter, v0, is unrestricted other than m0 P m, where m
is the number of asset, equal to 11 in this application. The values of e1 and e2 are
equal to 0.0001 allowing for a proper, diffuse hyperprior; and j0 is equal to 0.10 n
where n represents the sample size. In addition, P0 is a known correlation matrix
with structure.
In this application, the estimate of the correlation parameter q0 is 0.5.
Results.
We will employ a method of comparison based on portfolio performance. We use
the posterior means of lt and Rt as inputs to the MV framework (1); or the posterior
marginal predictive distribution as inputs to the direct utility framework (6).

</file>

<file= AmE06_J41>

In recent years events such as terror attacks, natural disasters and technological accidents have received growing attention from critical social scientists. A central question in this literature is whether such events exceed the mechanism of risk spreading through insurance that was so essential to a certain kind of security over the twentieth century. This problem of insurability has served as a lens for broader issues: do such risks exceed mechanisms of calculated mitigation altogether? How do they transform the apparatuses of collective
security that characterized industrial modernity?
The present article addresses these questions by shifting attention away from insurability and calculability per se and towards an analysis of alternative mechanisms for knowing and assessing risks. The knowledge form classically associated with insurance - which uses statistics to analyse an 'archive' of past events - is only one way through which uncertain threats can be known. In what follows, I contrast this archival - statistical knowledge with enactment-based knowledge produced by 'acting out' uncertain future threats in order to understand their impact. I will trace the genealogy of enactment through a series of historical moments in the United States: from civil defence preparedness to disaster insurance to budgetary rationalization in contemporary domestic security. In tracing this genealogy, I do not mean to suggest that enactment-based knowledge has superseded archival- statistical knowledge or that enactment is the paradigmatic knowledge form of contemporary collective security. It is, however, a significant new approach to producing knowledge about collective life, one that is increasingly important in the diverse emerging assemblages of risk, rationality, and security.
The article begins by examining the critical social science discussion concerning catastrophe risk and insurance. I focus in particular on a central point of reference in these discussions, Ulrich Beck's 'insurability thesis'.
Beck's thesis involves two interlinked claims. First, he argues that contemporary societies increasingly face 'catastrophe' risks such as industrial accidents or large-scale terrorism that cannot be covered by private insurance. Second, Beck argues that this insurance 'limit' marks the threshold of risk society, whose dominant techno-political dynamics are shaped by uncertain risks that are beyond rational assessment and calculated mitigation. Beck's claims have been criticized, both because they are empirically suspect (insurers do cover some catastrophe risk) and because they paint an overly epochal picture of the shift from the manageable risks of industrial modernity to the uncertain and thus unmanageable risks of risk society (Bougen, 2003; Ericson & Doyle, 2004a; O'Malley, 2004; Rose et al., 2006). These critiques are persuasive. Nonetheless, I want to insist that Beck offers crucial insights into contemporary mutations of risk and collective security. Following Beck's own suggestion in recent work (Beck & Lau, 2005), his epochal story can, with some modification, be turned into a set of useful propositions for inquiry. Beck argues that uncertain risks exceed the limits of calculative and instrumental rationality in general. It would be more precise and analytically productive to say that such risks push the limits of a specific form of calculative rationality based on archival - statistical knowledge. This distinction is important, since what we observe today is not the paralysis of frameworks for rational response to uncertain threats. Rather, we see the proliferation of such frameworks in multifarious emerging initiatives whose aim is to generate knowledge about uncertain future events and to link this knowledge to diverse mechanisms of mitigation. Thus, as O'Malley (2000) suggests, the central question concerns not the theoretical status of calculative rationality (or 'insurability') per se, but, rather, how risks judged 'uncertain' from one perspective are already being known and assessed using other approaches.
The remainder of the article examines one approach to assessing 'uncertain' risk that has emerged and become institutionally significant in the last fifty years: enactment. Enactment, it will be shown, is of a fundamentally different character from archival - statistical knowledge (see Table 1). Rather than drawing on an archive of past events, enactment uses as its basic data an inventory of elements at risk, information about the vulnerability of these elements and a model of the threat itself - an event model. And, rather than using statistical analysis, enactment 'acts out' uncertain future threats by juxtaposing these various forms of data. These elements of enactment have been noted in some critical scholarship on insurance (Bougen, 2003; Ericson & Doyle, 2004a). But they have generally been treated as 'idiosyncratic' or nonstandard techniques of risk assessment. What is more, since such discussions have been confined to insurance, they have not examined the diversity of domains in which enactment has been deployed.
The analysis that follows traces a genealogical progression through which enactment was articulated and linked to diverse mechanisms of mitigation: first, civil defence preparedness planning in response to the Soviet nuclear threat in the early 1950s; second, new approaches to natural hazard modelling, which linked enactment to insurance from the 1960s to the 1980s; third, and finally, the contemporary emergence of terrorism risk assessment based on enactment, and its proposed application to the rationalization of federal budgetary distributions. These moments in the genealogy of enactment affirm many propositions in Beck's risk society theses. In each, archival  -  statistical forms of knowledge and assessment encounter a 'limit'. In each, the problem of estimating the likelihood and consequence of uncertain future events is made an explicit object of expert reflection and technocratic response. The genealogy of enactment does not, however, confirm propositions concerning a generalized crisis of rationality in the face of uncertain threats or a 'structural break' (Beck & Lau, 2005, p. 532) in the status of expert knowledge. Rather, we find alternative forms of knowledge and assessment - enactment among them - with coherent genealogies, with bodies of sanctioned expertise that become authoritative in certain contexts (though they remain disputed in others), and with their own norms and modes of veridiction.  In specific sectors and in response to events we see moments of re-problematization in which existing forms are critiqued, redeployed and recombined. My analytic strategy is to trace these recombinations and the emerging forms of collective security to which they are giving rise.
Beck's 'insurability' thesis and styles of reasoning about risk In his now thoroughly debated work, Beck has characterized contemporary society as 'risk society'. The distinction that Beck marks with this term is between a society faced with threats whose 'risk' can be confidently expressed as a likelihood of future harm and a society faced with threats whose risk is uncertain. He argues that this distinction is made visible by the limit of a distinctive technology of collective security, namely, insurance and, in particular, private insurance. Given the centrality of these claims to contemporary discussions about risk, it bears rehearsing the outlines of Beck's 'insurability' thesis.
The voluminous critical scholarship on insurance has shown that insurance was applied to problems of collective security in response to a certain class of events - 'pathologies of the social' such as disease, workplace accidents and
poverty - in the nineteenth century (Ewald, 1991, 2002; Foucault, 2007;
Hacking, 1990, 2003; Rabinow, 1989; Rose, 1999).  Through an emerging practice of applied social science, these pathologies were mapped onto the regularities of collective life through new knowledge forms that were, in turn, linked to the insurential mechanism of risk spreading. The data associated with this 'social insurance', that is, insurance deployed as a mechanism of collective security, might be called archival (Collier & Lakoff, 2008a; Fearnley, 2005). It is a 'historical' record of past events - illnesses, crimes, incidents of poverty - in a
population. The analytic technique or technique of assessment in social insurance is statistics, used to understand the distribution of these events - and, thus, the distribution of risks  - over a population. This archival - statistical knowledge about the population, finally, was linked to the insurance form of risk spreading.
Actuaries could estimate the risks associated with an individual policy and with a total portfolio, and make decisions about what to insure, whom to insure and what premiums to charge.
For Beck - and, it should be noted, for many other critical social scientists - social insurance is paradigmatic of the modern 'security pact'. The 'systematic creation of consequences' by industrial modernity, Beck argues,
found a 'counter principle' in a 'social compact against industrially produced hazards and damages, stitched together out of public and private insurance agreements' (Beck, 1992b, p. 100). Through the mechanism of insurance 'the incalculable threats of pre-industrial society (plague, famine, natural catastrophes,
wars, but also magic, gods, demons) [were] transformed into calculable risks in the course of the development of instrumental rational control' (Beck, 1999, p. 76).
In his work on risk society, of course, Beck's primary concern is with the limits of this security pact. He associates this limit with the emergence of threats such as nuclear war or environmental catastrophes that do not meet the basic criteria of insurability based on archival - statistical knowledge. Such threats are also systematically produced by industrial modernity; indeed, as
Beck points out, they are the products of industrial modernity's very success, a point to which we return. But unlike 'pathologies of the social', these threats of risk society exceed industrial modernity's characteristic mechanism of security.
There is no archive of past events whose analysis might provide a guide to future events.11 As a consequence, Beck argues, 'standards of normality, measuring procedures and therefore the basis for calculating the hazards are abolished. . . . [I]ncomparable entities are compared and calculation turns into obfuscation' (Beck, 1992b, p. 102). Such risks, in short, exceed mechanisms of calculated, rational assessment.
Beck draws various implications from these observations, among them a core thesis concerning a new politics around the distribution of social 'bads'.
Here I want to focus on a narrower technical dimension of his argument. Beck posits that the 'limit' of private insurance marks the threshold of risk society.
The market economy's internal dynamic, he argues, 'reveals the boundary line
of what is tolerable with economic precision, through the refusal of private
insurance. Where the logic of private insurance disengages, where the
economic risks of insurance appear too large or too unpredictable to insurance
concerns, the boundary that separates ''predictable'' risks from uncontrollable
threats has obviously been breached' (Beck, 1992b, p. 130). The uninsurable
risk, for Beck, is the autonomic signalling mechanism of risk society.
Beck's 'insurability thesis' has been subject to deserved critique, first of all on
empirical grounds: private insurers do, in some cases, offer coverage for
catastrophe risk. This empirical qualification has conceptual implications. It
does not seem plausible to maintain that private insurance serves as the
boundary marker of risk society (Ericson & Doyle, 2004, p. 137); and we may
wonder more broadly whether epoch-marking terms like 'risk society' do the
analytical work required to trace contemporary shifts in contemporary risk and
security. These problems, I hold, can be traced to two conceptual slippages in
Beck's work. First, in his arguments about insurance Beck seems to conflate the
limits of a specific knowledge form - what I have called the archival - statistical

form of knowledge and assessment - with the limits of the insurential
mechanism of risk spreading. Second, Beck associates the limit of a specific style
of reasoning about risk with the limit of rational assessment in general.12 If these
slippages can be sorted out, we may be able to proceed with Beck's categories,
understanding that they offer provocative orientations for inquiry rather than
an epochal diagnosis of the present.
First, the empirical question concerning the insurability of catastrophe risk:
discussions of this problem can be traced back to at least the 1960s and 1970s
when a combination of new economic formalizations and practical problems -
specifically, mounting losses from natural disasters - drew the attention of
insurance professionals and policy-makers. Beginning in the 1970s, a specialist
literature of economists and insurance experts began to argue that catastrophe
risk could be managed by private insurance companies (Anderson, 1976; Jaffee
& Russell, 1997). Over time, tools for assessing such risks became increasingly
widespread, and have recently been extended to new kinds of catastrophe risks,
such as terrorism. This development has been examined in critical studies of
insurance that have asked how actuaries and underwriters respond to
catastrophe risks (Bougen, 2003; Ericson & Doyle, 2004a; O'Malley, 2003).

</file>

<file= AmE06_J42>

The U.S. Court of International Trade ("CIT" or the "court") reviews determinations of the U.S. International Trade Commission (the "Commission" or "ITC") pursuant to 28 U.S.C. § 1581 (c).' In general, an
"interested party" who appeared in a underlying antidumping or countervailing duty proceeding may appeal a determination of the Commission to the court. The most commonly appealed cases arise out of negative preliminary injury determinations in original investigations, final injury determinations in original investigations, and sunset review determinations.
During 2007, the CIT issued nine written opinions involving appeals from the ITC. The court remanded two negative preliminary determinations in original investigations, affirmed a negative final determination on remand in an original investigation, affirmed two affirmative sunset review determinations, and remanded an affirmative sunset review determination. The court also issued written opinions granting an application for preliminary injunction to enjoin liquidation of entries pending appeal, denying another such application for injunctive relief pending appeal, and denying a motion to strike attachments to a reply brief filed with the court. The opinions having the greatest impact on how the ITC decides cases involved the application of the Bratsk "replacement/benefit" test in an original investigation, the interpretation of the "likely" standard in a sunset review, and the application of the "clear and convincing evidence" standard in preliminary original investigations. The other five CIT decisions from 2007 deserve careful attention as well and are discussed separately below. For the most part, these decisions did not involve major legal issues, and they broke little new ground.
The CIT's decision in Mittal Steel Point Lisas Limited. v. United States, however, is an important case in a series of decisions relating to the application of a principle announced two years ago by the U.S. Court of Appeals for the Federal Circuit in Bratsk. Under this principle, a domestic industry is not to receive relief from unfair trade even if the evidence relating to the statutory injury factors would otherwise compel an affirmative determination if (1) the case involves a "commodity product," (2) imports from non-subject countries are a "significant factor" in the market, and (3) there is reason to believe that nonsubject imports would have replaced the subject imports had those imports not been present in the market or would do so if antidumping/ countervailing duty orders against subject imports are imposed. The ITC has referred to this as the "replacement/benefit test" and has vehemently argued that it is contrary to law. Mittal Steel reflects perhaps the starkest manifestation of the ITC's opposition to Bratsk, and it is given special attention in the discussion below for that reason.
In addition, the CIT reviewed two Commission decisions involving negative determinations in preliminary investigations. The court's opinions in these cases suggest that the "arbitrary and capricious" standard of review applicable in such appeals may not be an impossible burden for losing petitioners to overcome in such cases.

Mittal Steel Point Lisas Ltd. v. United States arose from a long-running appeal of an affirmative material injury determination by the Commission in an antidumping duty investigation of steel wire rod from the Republic of Trinidad and Tobago ("RTT"). In an earlier segment of the proceeding, the CIT sustained the Commission's affirmative material injury determination.'  The Federal Circuit later vacated that result and remanded to the Commission."
On remand, Commissioners Shara L. Aranoff and Jennifer A. Hillman found material injury and threat of material injury by reason of imports of subject merchandise from RT. Commissioners Stephen Koplan and Charlotte R. Lane also made an affirmative material injury determination, though they did not make a threat determination in their separate and dissenting views. Taken together, this majority of the Commission found a significant volume of subject imports, negative price effects, and a significant adverse impact on the domestic industry. In contrast, Commissioner Deanna Tanner Okun readopted her negative material injury and threat of material injury findings. Commissioner Daniel L. Pearson did not participate in the remand determination. Even though Commissioners Aranoff, Hillman, Lane, and Koplan all found material injury by reason of subject imports, the remand resulted in a negative determination because
Commissioners Aranoff and Hillman complied with what they found to be a compulsory mandate by the court to apply the Bratsk "replacement/ benefit" test, even in the context of assessing the threat of material injury.  This extra step of analysis led Commissioners Aranoff and
Hillman to make a negative determination, even though they found that application of the statutory factors resulted in affirmative findings for both material injury and threat of material injury.  Commissioners Aranoff and Hillman felt compelled by the remand order to apply the "replacement/benefit" test in a threat context, though the Commission
has never found Bratsk to require this test.
The Federal Circuit's remand was largely based on a claim of legal error by foreign producers that the Commission "did not evaluate the effect of RTT's imports in light of other LTFV [less than fair value] imports, and its findings did not discuss the effect of fairly-traded imports."  On remand, the Federal Circuit mandated that the Commission make a specific causation determination and in that connection
... directly address whether [other LTFV imports and/or fairly traded imports] would have replaced [Trinidad and Tobago's] imports without any beneficial effect on domestic producers. 
The "replacement/benefit" test is "triggered" in an antidumping duty investigation if (1) the subject merchandise is a commodity product and (2) price-competitive non-subject imports are a significant factor in the market.  Commissioners Aranoff and Hillman found that these "triggering factors" were present in this case.  Commissioners Koplan and Lane, however, dissented and found that subject and non-subject imports were not commodity products.  Because they concluded that
"this threshold Bratsk triggering factor [was] not met," Commissioners Koplan and Lane did not perform the "replacement/benefit" test and maintained their affirmative material injury determination.  In contrast,
Commissioners Aranoff and Hillman found the presence of a commodity product because they felt "constrained to interpret the Court's ruling broadly for purposes of satisfying the Court's remand in
this case .... Commissioners Aranoff and Hillman  noted that their negative remand result occurred
solely as a consequence of our application of the additional "replacement/benefit" analysis set forth by the Federal Circuit in Caribbean Ispat and Bratsk. Had the Commission applied only
what we believe to be the proper standard for "material injury by reason of subject imports,"... we would have arrived at an affirmative determination.
Senior Judge Aquilino also recounted that Commissioners Aranoff and Hillman raised other fundamental concerns with Bratsk: namely, that the decision creates a presumption in favor of finding that replacement of subject imports by non-subject imports will occur upon revocation of
the order at issue.  Specifically, Commissioners Aranoff and Hillman cautioned that the "replacement/benefit ' test appears to require the
agency to render a negative determination if the triggering factors are satisfied, unless the record contains substantial evidence that either non-subject imports would not replace the subject imports or that such replacement would nonetheless benefit the domestic industry.
Commissioners Aranoff and Hillman also suggested that "the data needed to rebut such a presumption" might be withheld by non-subject producers "from countries not under investigation.., with no incentive to provide the data needed." As a result, they reasoned that this presumption may well prove unassailable and that the Bratsk analysis will require the Commission to make negative determinations where the record would (and perhaps should, in their view) otherwise lead to
a contrary result.  This is exactly what happened in Mittal Steel, where the application of the "replacement/benefit" test by Commissioners Aranoff and Hillman was not predicated on any positive evidence.
Instead, Commissioners Aranoff and Hillman merely noted an "absence" or "lack" of data on the record to rebut the presumptions for both the "replacement" and "benefit" prongs.  The domestic industry, in contrast, argued that it is less than clear that the court's decision actually requires any such presumptions.
The Commission's reversal on remand is also noteworthy because Commissioners Aranoff and Hillman stated that the Bratsk test is unclear and engenders ambiguities [that] arise because the requirement imposed by the Bratsk panel,., is not among the statutory factors Congress has required the Commission to consider. Indeed, such a test misconstrues the purpose of the statute, which is not to bar subject imports from the U.S. market or award subject import market share to U.S. producers, but is meant instead to "level[] competitive conditions" by imposing a duty on subject imports and thus enabling the industry to compete against fairly traded imports. The statutory scheme in fact contemplates that subject imports may remain in the U.S. market after an order is imposed and even that the industry afterwards may continue to suffer material injury. Indeed, the dumping of subject imports may have no impact on respective market shares, but may affect the domestic industry's selling price and profitability alone.
Therefore, the Commission is required under Bratsk to determine whether non-subject imports would fill the void created by the "elimination" of subject imports despite the fact that there may be no such void created by an order.
Despite the Commission's strong challenge to Bratsk and the legality of the "replacement/benefit" test, Senior Judge Aquilino noted that "this
court cannot conclude that the commissioners failed to address the questions imposed by" the Federal Circuit's remand. As a result, the
CIT affirmed the Commission's application of the "replacement/benefit" test which led to the negative material injury determination on remand. 
Intervenor-defendant Gerdau Ameristeel Corp. argued to the CIT
that Commissioners Aranoff and Hillman adopted an "extreme interpretation" of Bratsk, failed to make their own findings as to whether or not the subject merchandise was a commodity product, relied on a rebuttable presumption that replacement of imports automatically negated benefits to the domestic industry, and extended the "replacement/ benefit" test to the threat context. SeniorJudge Aquilino chose
not to address these arguments, but rather left them for the Federal
Circuit to resolve.
The fact is that the rebuttable presumptions that Commissioners
Aranoff and Hillman applied are nowhere to be found in the Bratsk decision itself, and the extension of those presumptions to the
"replacement/benefit" test would make Bratsk even more problematic for petitioners in future cases. Likewise, the "replacement/benefit" test
had never previously been applied in a threat context, and the Federal
Circuit will have to determine whether it should be.
In sum, Mittal Steel shows that Bratsk and its "replacement/benefit" test will remain a source of vigorous judicial review of ITC injury determinations involving commodity products for the foreseeable future.
The case reflects the ongoing struggle between the Commission and the Federal Circuit with regard to a judicial test that appears nowhere in the statute or the legislative history, and that turns otherwise affirmative determinations into negative determinations for reasons that the Commission believes are contrary to law. Interven or defendant Gerdau Ameristeel Corp. appealed the decision to the
Federal Circuit, and oral argument took place on June 3, 2008.
JUDICIAL REVIEW OF SUNSET REVIEWS BY THE COMMISSION
An Unsuccessful Attempt to Toughen The "Likely" Standard
In Wieland-WerkeAG v. United States, Judge Wallach heard a reprisal of another frequent source of judicial review-the scope of the "likely" standard used by the Commission in sunset reviews. The case arose from an affirmative determination by the Commission in the second sunset review of antidumping duty and countervailing duty orders on brass sheet and strip ("BSS") from various countries, including Germany.
The Commission determined that "revocation of the orders would likely lead to continuation or recurrence of material injury to the domestic BSS industry within a reasonably foreseeable time."
German producers, exporters, and a U.S. subsidiary appealed.
Judge Wallach focused on plaintiffs' argument to "broaden the scope of the 'likely' standard pursuant to which the Commission conducts sunset reviews. " As noted by Judge Wallach,
Plaintiffs contend that because the "likely" standard requires a probability-and not merely a possibility-that volume, price effects, and adverse impact will occur and increase, the evidence must show a "rational economic option" to increase exports in order to support an affirmative determination.

Plaintiffs argued that their proposed "rational economic option" test should apply both to the Commission's analysis of discernible adverse impact in addressing cumulation and to its analysis of the likelihood of continuance or recurrence of material injury.

</file>

<file= AmE06_J43>

Historical Temptations
WHO NEEDS HISTORY?
Subject positions, the topic of this book, do not always have names, but
like any aspect of culture they always have a history. They come into being
at a certain period of time, which shapes them, and they also change
through time as long as they persist. Subject positions also always contain
spatial scales within them (N. Brenner 1998; Harvey 2000). To be a "Yale
student" has a different spatial scale than to be a "New Yorker" or "Japanese." The various subject positions through which one lives at any point in time may not have isomorphic spatial scales: one's sense of self as a youth could be global, as a man local, and as a laborer national, all at the same time. Or to be a youth could be both local and global at the same time, intersecting. Thus, three crucial issues in the ethnographic investigation of subject positions are (1) their historicity (that is, the way they are shaped by their embedded notions of their own history and what counts as history); (2) their spatial scales; and (3) how they intersect with other subject positions and the histories and spatial scales of those other subject positions. This chapter focuses on the first of these issues.
Only since the 1970s or so have people in Indonesia called themselves
gay or lesbi, yet many Westerners seek a clear temporal trajectory connecting gay and lesbi with "indigenous" homosexualities. This deep-seated desire for unbroken history has many precedents in the Western tradition, most notably the Old Testament chains of "begats" that establish legitimacy through a patriline. While on rare occasions I have encountered
gay and lesbi Indonesians who share this concern with a clear temporal
trajectory, what demands explanation is that most do not. It is not a meaningful connection; for gay and lesbi Indonesians, belonging, recognition, and authenticity are legitimated not through history but by the performance of good deeds (prestasi) in the present. Since this chapter concerns the historicity of the gay and lesbi subject positions, it is built around an empty center, a McGuffin-Hitchcock's term for a plot element that is of intense interest (to Westerners, in this case) but has no content. The point is not to unearth the hidden past of gay and lesbi subjectivities, but to explore contingent contexts of homosexual desire in the archipelagoand why such "history" matters to the Western reader, where notions of the "archive" as scene of legitimation remain powerful, and archives are often mined for their content without attention to the affective assumptions embedded in their form (Derrida 1996; Stoler 2002a).
"Indonesia," after all, is a self-consciously novel concept. The postcolonial nation came into being with the proklamasi (proclamation) of Sukarno on August 17, 1945; nationalism dates back only another forty
years or so. The term "Indonesia" was coined by George W. Earl in 1850
and first used by his colleague James R. Logan that same year, but it was
not used as a political term by "natives" of the archipelago until April
1917 (Ave 1989:220; R. Jones 1973:100-103; Nagazumi 1978:28). It is
quite certain that no one in the archipelago called themselves gay or lesbi
in the year 900, 1400, 1900, or probably even 1960. Yet by the early
1980s gay and lesbi existed in the archipelago as nationally distributed
subject positions. These subject positions challenge narrativizing; their
"history" seems to be all change and no continuity. This threatens the
dualism of change (modernity) and continuity (tradition) that has been a
motif of Indonesianist historiography (Benda 1972).
This problem-"can there be a history of sexuality?"-is not unique
to Indonesia; it has been the topic of debate in scholarship on Western
sexualities. But it takes on new urgency when globalization and postcoloniality are brought into the discussion. The most careful scholarship on Western homosexualities takes continuity into consideration while foregrounding "the irreducible cultural and historical specificities of the present" (Halperin 2002:17). In a postcolonial context, it can appear that without an unbroken historical timeline one must view gay and lesbian non-Westerners as derivative, converging on a single global conception of
homosexuality.
One response to the problem of radical change accepts the premise that
an unbroken historical timeline is needed to establish authenticity. Since
the nation-state form is deeply bound up with conceptions of modernity,
the idea of tradition is a central paradox of national thinking: nationstates are young, but they imagine themselves as of great antiquity (Anderson 1983:5). Tradition is the shadow modernity casts back in time to see itself whole. Often it is postcolonial nation-states that display a particular concern with an unbroken historical timeline because this appears to bracket the colonial encounter. Although one does not assume that a Western man born in 1980 is first shaped by conceptions of homosexuality dominant in the 1920s before calling himself "gay" in 2001, this response suggests that gay and lesbian non-Westerners are first and foremost products of indigenous locality-as if the history of a person repeats, in miniature, the ostensible history of a society. This developmentalist perspective assumes that non-Western "homosexualities" like gay and lesbi originate in homosexualities and transgenderisms of the past. It makes it difficult to understand how a lesbi woman in northern Bali could say "I don't know of any cases in the past where there were actually women having sex with each other" and not find this a cause for concern.
A more theoretically informed response to the problem of radical
change questions the need for "the narrative continuity of history and
identity" (Clifford 1988:341). Can there be a subject position without a
direct historical predecessor? Is such a subject position necessarily less
authentic? How can one think historically about the gay and lesbi subject
positions without assuming that what came before is the foundation of
what comes after? Concerns for "discovering gay and lesbian history"
sometimes participate in the widespread assumption that such history always already exists; that its real or apparent absence is inevitably problematic; that its existence by definition has validating effects; and that these effects are necessary to the sexualities in question. This tempting chain of reasoning creates the desire for narrative continuity and delegitimates sexualities for whom such continuity really does not exist (and is not only waiting to be uncovered).
I am not saying that historical research on nonnormative sexualities
and genders is misguided, but that tradition is not the same thing as history and that proper historiography requires being open to a variety of
causal relationships between the past and present, including no relationship at all. There may be no "perfect path" between past and present, just as there may be no genealogical lineage between "gay" and "lesbian" sexualities in the West and non-West (Boellstorff 1999). A less reproductive and heteronormative metaphor is needed in place of the genealogical grid. One such metaphor I develop in this book is that of the archipelago.
Forging histories of nonnormative sexualities and genders outside the
West presents methodological as well as theoretical challenges. While the
barriers to historical research on homosexuality in the United States may
only "appear, at first glance, to be unusually daunting" (Chauncey
1994:365), outside the West much more than appearances are involved.
The available documentation is often so minimal that one is forced to
make do with limited sources and craft the best narrative possible (Jackson 1999a:369). Written documents by persons from the archipelago now
called "Indonesia" are primarily limited to courtly texts. These sometimes
address homosexuality and transgenderism, but not in a sustained manner; it is unclear to what extent they reflect everyday life outside royal
circles. One can often read between the lines of colonial documents to
find data concerning homosexuality and transgenderism (e.g., Proschan
2002). In Indonesia, the remarkable lack of colonial documentation on
male (and female) homosexuality underscores how Dutch civil law paid
little attention to sodomy until the end of the three-century presence of
the Dutch in the archipelago. Stoler (1995:96) notes that her own "silence on this issue and the prominent place I give to heterosexuality reflects my
long-term and failed efforts to identify any sources that do more than
assume or obliquely allude to this 'evil,' thereby making the other 'lesser' evils of concubinage and prostitution acceptable."1
This chapter examines past nonnormative sexualities and genders in
what is now called Indonesia without assuming that this past contains the
present in embryonic form. Gay and lesbi subjectivities can represent an
innovation, even a radical break, with understandings of sexuality in Indonesia and still be authentic if they are conceptualized in terms of conjunctural, "dubbed" histories of homosexual desire.
RITUAL AND DRAMA
Scholars of sexuality in what is now called Indonesia have tended to focus
on what might be called "indigenous" homosexualities and transgenderisms. The best known is probably the bissu subject position, associated
with Bugis culture in southern Sulawesi. Bissus are linked to pre-Islamic
traditions and first entered the Western written record with the visit of
Antonio de Paiva to Sulawesi in 1545. They appear in sources from the
1600s, as well as the travelogue of the "white raja" James Brooke in 1840
(Andaya 2000:41; Bleys 1995:117; Pelras 1996:56).
At present it is typically assumed that bissus are male transvestites
(Hamzah 1978:6; Pelras 1996:165-167), but female bissus appear in Bugis mythology (Pelras 1996:83), and historically the majority of bissus were apparently women (in the I La Galigo myth cycle, for instance, thirty-two of the forty original bissus were women, including We Tenriabeng, twin sister of the cycle's hero Sawerigading).2 To the present day there are women bissus (known by the terms bissu makkunrai or corecore) whose presence is required for certain rituals (Lathief 2004:48-49).
Although refraining from sex has long been a way for bissus to protect
and increase their power, and despite the fact that bissus sometimes married women, since at least the sixteenth century most bissus have been male transvestites who engage in sex with men (Pelras 1996:83).3 Following what is usually at least three years of training (Lathief 2004:43), bissus
historically engaged in a lifelong profession of guarding royal regalia and
conducting rituals for nobles, particularly for life events like childbirth
and weddings, as well as rituals for the fertility of the rice fields. In performing these activities, bissus would dress in an androgynous fashion, combining men's and women's clothing. One of the best-known bissu rituals involves trance proven through maggiri, where bissus attempt to
stab themselves with ceremonial knives (krises); if the bissus' bodies are
truly possessed by gods (dewatas), the knives will not be able to enter (Graham 2003). At present, however, this ritual is not performed by all bissu groups (for instance, it is not performed in the Bone region; Lathief 2004:75).
For several hundred years, bissu rituals coexisted with the Islamic faith
now followed by virtually all Bugis. This changed radically with the rise
of the Islamic fundamentalist movement of Kahar Muzakar in South Sulawesi in the mid-1960s. One element of this movement, "Operasi Tobat" (Operation Repent), took aim at practices considered un-Islamic, particularly bissu practices. It was also claimed that bissus were in league with the Communist Party of Indonesia, which was in the process of being eliminated by Soeharto's New Order government. Sacred regalia were burned or thrown in the sea, rituals forbidden, and bissus offered the choice of death or leaving the bissu profession, dressing and working like
"normal" men (Lathief 2004:79-80). As a warning, the head bissu of the Bone region, Sanro Makgangke, was decapitated and his head publicly displayed; many other bissus were killed as well.
Since the late 1990s there have been attempts to revitalize bissu practices. In part this reflects the refetishization of adat (traditional custom) across the archipelago in the wake of Soeharto's fall in 1998, supported by new government policies of regional autonomy. This has included the production of at least two documentary films by Westerners on bissus.
However, it appears that these attempts to revitalize bissu practices are failing.  Bissu rituals have been radically simplified (for instance, the Mappalili agrarian ritual in the Segeri region that once took forty days is now conducted in a single night), many rituals are now performed only for tourists, and the rice fields that were once given to the bissu community for income have been taken away and sold (Lathief 2004:69, 87-89, 83 -85).

</file>

<file= AmE06_J44>

Philosophers tend to believe that to punish and to take revenge
are vastly different activities. Consider the ancient view put
forth by Protagoras, in Plato's Protagoras:
No one punishes wrongdoers putting his mind on what they did and for the
sake of this - that they did wrong not unless he is taking mindless vengeance,
like a savage brute. One who undertakes to punish rationally does not do so
for the sake of the wrongdoing, which is now in the past
- but for the sake of the future, that the wrongdoing shall not be repeated, either by him or by
others who see him or by others who see him punished.
Nowadays, this continues to be the predominant view amongst
philosophers of law regarding the relationship between punish
ment and revenge. It is probably impossible to present an
exhaustive list of authors who hold that punishment and revenge
are different, and any partial, non-exhaustive list is prone to be
helplessly idiosyncratic. Just to get a taste, I will mention a
handful of influential authors who embrace this view. Joel
Feinberg puts vengeance in close company with "lust for blood"
and "Schadenfreude" and claims that whereas retributive
punishment is "righteous," vengeance is "angry". Likewise, philosophers such as John Elster, Antony Flew, Ted Honderich,
John Kleinig, Nicola Lacey, and C. L. Ten, amongst many oth
ers, have held that punishment and revenge are different.
Arguments which allegedly show that punishment and
revenge are two different phenomena abound in the literature
on punishment. Among the most popular of these arguments
are the views that unlike punishment, revenge can be carried
out for mere slights, or by unauthorized agents, or by agents
who display an inappropriate emotional response to what
they perceive was an instance of wrongdoing, or by agents
who are unconcerned with the generality of universality of
the moral principles behind their action, i.e., agents who
would not treat like cases alike, or by agents unconcerned
with the proportionality of their response and the perceived
wrongdoing to which they react. In spite of this argumen
tative manifoldness, I shall argue that all these arguments
tend to revolve around one basic idea: revenge is "mindless,"
or barbaric, irrational, etc., and thus unjustifiable, whereas
punishment is "rational" or civilized, enlightened, etc., and
thus justifiable.
In opposition to this orthodoxy of sorts, I shall argue that
punishment and revenge are not at all easy to distinguish. I thus
hope to cast doubt on the plausibility of the above arguments. I
hope to show as well that the main role this distinction has
played throughout history is not really analytic but merely
rhetorical. It is rhetorically powerful to claim that such and
such action is not punishment but mere revenge, in spite of the
fact that there may be no good analytic grounds for making
such a distinction. In most cases 'punishment' and 'revenge' are
terms used to conveniently describe one single phenomenon.
A sketch of an account of punishment should suffice here to
shed light on the sort of phenomenon that I have in mind: A
punishes B when A inflicts (what she believes to be) suffering upon B as a reaction to (what she believes was) B's
wrongdoing. This is, in principle, also true of the avenger: the
avenger also re-acts to what she believes was someone's
wrongdoing and seeks to inflict suffering upon her. I shall
discuss below a narrowly circumscribed sort of case in which,
perhaps, a principled difference between punishment and re
venge might be recognized. Yet, even if there are cases in which
some acts of revenge contain elements not found in paradig
matic acts of punishment, this way of distinguishing between
punishment and revenge is not only dim, and not only does it
operate (if it operates at all) in a miniscule number of cases, but
it is different from any of the standard distinctions found in the
literature. In any event, rather than seeing acts of revenge as
wholly different from acts of punishment, I see the conceptual
connection which might obtain between punishment and re
venge as follows. Either there is no analytical difference at all
(true in the vast majority of cases), or, at most, it could be
conceded that revenge is a form of punishment. Either way,
there is no vast difference between the "two" phenomena.
The term 'action' (or activity, or act -
I do not here distin
guish between these terms) which I used in the preceding par
agraph to refer to punishment and to revenge is itself
important. Punishment can be seen in different ways: as an
institution, or as a reason supporting the setting up of these or
those institutions, or as a motive for which actions are done, or
as an action. Something similar happens with revenge. In what
follows I shall concern myself with the act of punishing and the
act of revenging. My interest lies with the alleged differences
between a case in which a person is said to punish and a case in
which a person is said to take revenge. Perhaps some of the
conclusions that I draw regarding the relationship between
punishment and revenge, understood as activities, might hold
too if one sees punishment and revenge as motives, or as
institutions, and even in cases in which one sees punishment
and revenge as different sorts of phenomena, say, punishment
as an institution, and revenge as a motive for which an action is 
done. But I am not interested in extending the scope of the
present article. Sometimes we punish and sometimes we avenge,
so goes the official story; philosophers overwhelmingly assert
that these are two essentially different activities, and I shall
argue that they are not.
One obvious way to attempt to refute the thesis that
punishing and taking revenge are different activities would be
to point to the numerous fictional examples where the "two
phenomena" seem perennially intertwined. Does Othello
punish Desdemona or does he take revenge upon her for what
he thought was her betrayal? Is the Count of Monte Cristo
punishing or taking revenge on the malefactors who attempted
to ruin his life? Does Lazarillo de Tormes really take revenge
(as he claims) on the blind man who had cruelly abused him for
so long, or is he in fact punishing him? Does Michael Kohlhaas
seek to take revenge or to punish as he roamed Saxony in
pursuit of those who did him wrong? These questions are not
easy to answer. Sometimes it seems to be the very point of these
stories to leave these questions unanswered; that they are not
easy to answer constitutes a reason why these literary works are
so good and compelling. Most emphatically, however, I am
not appealing to literature as the ultimate judge in anything.
Yet, that in literature there are many cases where it is difficult
to distinguish punishment from revenge surely has some
significance.
The view that punishment and revenge are not easily distin
guishable finds support elsewhere as well. Oft-quoted biblical
passages, such as Romans 12:19, "Vengeance is mine; I will repay,
saith the Lord" or Romans 13:1, "[a ruler ordained by God] is the
minister of God, a revenger to execute wrath upon him that doeth
evil," seem, too, to be cases in which 'vengeance' and 'punish
ment' are not easily distinguishable. When God says that ven
geance is his, surely it is not absurd to assume that he uses
'vengeance' as a synonym for 'punishment'. For it is otherwise
hard to accept that what (a benevolent) God has in mind is that he
is the only one allowed to engage in barbaric, unjust, irrational
behavior (can God behave in such a way?), and it is precisely
barbarism, brutality and so on, which are customarily adduced as
evidence showing that revenge is different from punishment.
The best dictionary of the English language also seems to assume
that punishment and revenge are very similar. One of the meanings
the Oxford English Dictionary gives for "revenge" is "punishment".
In most of the other meanings of "revenge" listed in the Oxford
English Dictionary, the element of "being a response to a wrong,"
clearly something that is also true of punishment, is present. The
Oxford English Dictionary provides the following as the first defi
nition of "vengeance": "The act of avenging oneself or another;
retributive infliction of injury or punishment; hurt or harm done
from vindictive motives". For the noun form of 'avenge', the Ox
ford English Dictionary gives one single meaning (though it admits
is obsolete): "Execution of vengeance; retributive punishment,
retaliation either upon an offender, or on account of a wrong".
The view that punishment and revenge are not so easily
distinguishable is supported in yet further ways. In an inter
esting article, Gregory Vlastos discusses the complicated rela
tionship between punishment and revenge in ancient Greece.
Vlastos points out that early in Greek history, the distinction
between punishment and vengeance was not clear-cut. The
linguistic connection between punishment and vengeance, for
instance, was then so strong that Vlastos dubs it "positively
tyrannical". As Vlastos points out:
Down to the last third of the fifth century [BCE], Tifi pia, whose original
and always primary sense is "vengeance," is the word for "punishment".
The specialized word for the latter, ko?gc?eiv ("chastening," "disciplining,"
- with no collateral use for "taking vengeance") does not acquire currency
until we reach the prose of Thucydides and Antiphon.
All of this: literature, the Bible, the Oxford English Dictionary,
ancient Greek philology, is grist for my mill, insofar as it
supports the main thesis that I wish to defend in this paper,
namely, that punishment and revenge, qua activities, are not
that different. But, of course, I am not going to simply present
literary references and dictionary entries. I present this evidence
simply to highlight the fact that, unlike the predominant view
amongst contemporary philosophers, the view that punishment
and revenge are clearly different is not met with general
acquiescence in other fields. In what follows I shall dissect the
specific arguments which those who believe that there is a clear
distinction between revenge and punishment endorse, and I
shall argue why I believe that they are unconvincing.
PUNISHMENT SIMPLICITER AND RETRIBUTIVE PUNISHMENT.
Vlastos claims that "the distinction of punishment from revenge
[in Plato's Protagoras] must be regarded as one of the most
momentous discoveries ever made by humanity". Not surpris
ingly, what Vlastos considers momentous about this discovery is
that revenge is a sign of "barbaric tribalism" and that punishment is a sign of a more civilized concern for justice. Vlastos
bemoans the conflation of punishment and revenge in the archaic
period of Greek history, as he comments on the fact that
earlier, as for example in Herodotus, language traps one into using "vengeance", even when "punishment" is exactly what one means.
But how does Vlastos actually figure out what "exactly" pun
ishment is and how "exactly" does it differ from revenge?
When it comes down to articulating the allegedly diaphanous
difference between punishment and revenge, Vlastos, by and
large, follows the lead of one contemporary author whose
distinctions between revenge and retributive punishment have
become a mainstay of the contemporary discussion: Robert
Nozick. Vlastos' own contrasts between punishment and re
venge are admittedly corollaries to Nozick's views.
Vlastos is by no means alone in following Nozick. The
influence that Nozick's contrasts exert in the contemporary
discussion of the relationship between punishment and revenge
is immense. Such an immense influence is not only the result of
Nozick's creativity, but also of the fact that Nozick's contrasts
constitute a sort of compendium, comprising many of the differences which have been intuitively assumed to hold be
tween punishment and revenge. (It is likely that if we ask
people who have never read Nozick, or indeed who have never
studied the philosophy of law, about the differences between
punishment and revenge, we will get answers of the same tenor
as Nozick's contrasts.)
Thus, the general discussion of the al
leged differences between punishment and revenge overlaps to
a great extent with the particular discussion of Nozick's own
arguments.

</file>

<file= AmE06_J45>

Introduction.
SLUMMING: EROS AND ALTRUISM IN VICTORIAN LONDON.
FOR THE BETTER part of the century preceding World War II,
Britons went slumming to see for themselves how the poor lived.
FThey insisted that firsthand experience among the metropolitan
poor was essential for all who claimed to speak authoritatively about social problems. To a remarkable degree, the men and women who governed church and state in late-nineteenth- and twentieth-century Britain
and dominated social welfare bureaucracies and the emerging profession
of social work felt compelled to visit, live, or work in the London slums
at some point in their careers of public service. Even the fiery Welsh radical Lloyd George, champion of popular rights against aristocratic privileges, sought out a friend to take him on a tour of the East London slums
soon after he arrived in London in 1890 to assume his seat in parliament.' Lloyd George may have been intent to witness the scenes of
human misery and sexual degradation made famous the world over by
the serial murderer Jack the Ripper, but he also embarked on a journey
routed for him by thousands of well-to-do men and women. By the
1890s, London guidebooks such as Baedeker's not only directed visitors
to shops, theatres, monuments, and churches, but also mapped excursions to world renowned philanthropic institutions located in notorious
slum districts such as Whitechapel and Shoreditch.
We will never know precisely how many men and women went slumming, but the fact that slums became tourist sites suggests it was a very
widespread phenomenon. At any given time there were hundreds of private charitable institutions and agencies in the metropolitan slumrs, each
visited regularly by scores of donors, trustees, and volunteer and paid
workers. No doubt slumming was merely an evening's entertainment for
many well-to-do Londoners, but for many others, the slums of London
exercised powerful and tenacious claims over their minds and hearts,
drastically altering the course of their lives.
One such man was James Granville Adderley. Adderley was far too
iconoclastic to be representative of anything, but his life providles one
point of entry into the world of the women and men whose philanthropic labors are the subject of this book. Even those who disliked
Adderley's radical ideas liked the man himself. He bristled with righteous indignation about the world's injustices, but he also radiated an inner
calm and a joyful enthusiasm that drew people of all sorts and conditions
to him. Well-born, charming in conversation, blessed with even-featured
good looks, and bright without being ostentatiously intellectual, Adderley seemed destined for a lucrative career in law and politics. However,
within a short time of leaving Oxford in the mid-1880s, he found himself
the toast of philanthropic London as head of one of the metropolis's
newest institutions for translating vague ideals about cross-class brotherly love into concrete form: the Oxford House in Bethnal Green. A residential colony of idealistic university men planted in a slum district, it
was devoted to constructing bridges of personal friendship between rich
and poor through Christian work and wholesome "rational" recreation.
There was something absurd about Adderley's instant celebrity as an expert on social questions, and he knew it better than anyone else. He cannily recognized that his contemporaries saw him not as he actually was
but rather as an embodiment of a new type of man: the "'ecclesiastical
young man,"' called upon to "address all kinds of meetings, and looked
upon as a sort of freak-the fellow who might live in luxury in Belgravia
but preferred [the poverty of] Bethnal Green."
Impatient with the unending stream of visitors, reporters, and transient do-gooders to Oxford House, Adderley took his clerical vows and
moved farther east into ever less glamorous slum districts. He joined the
Catholic prelate Cardinal Manning and the trade unionist Ben Tillett in
championing the cause of London's grossly exploited dock laborers in
their world-famous strike in 1889; he defended the rights of laboring
men against puritanical attempts to deny them the pleasures of the stage
and music hall; he threw his heart and soul into club work with the
"rough lads" in his adopted neighborhood of Poplar and invited large
numbers of them for holidays on the grounds of his ancestral home,
Hams Hall. He helped form a new religious community within the
Church of England that was founded on the rules of St. Francis: The Society of Divine Compassion. Adderley and his brothers in poverty exalted the beautiful while despising the exuberant materialism of late Victorian London. Jolly fellowship among men went hand in hand with
severe austerity. "There was no carpet on the floors, a fire only in the
common room, and the brothers did their own crude cooking," one visitor recalled. A bare plank served as his only bed. Adderley felt that even
this self-denying regimen kept him too far removed from the gritty struggles of the homeless poor. He spent weeks at a time disguised as a tramp,
often sleeping rough on the streets. The depth of his compassion was
matched by the breadth of his tolerance. He extended his hand not only
to social outcasts but also to sexual outlaws like the celebrated playwright Oscar Wilde convicted in 1895 for committing same-sex acts of gross indecency. Living in East London placed Adderley far from the
starched-collar respectability and top-hatty conventions of bourgeois domesticity and freed him to develop distinctly heterodox ideas about class
relations, male sexual celibacy, and social purity. When Adderley died
in 1942, it was another man, Arthur Shearly Cripps, his "comrade in
tramping, dossing, and in preaching the gospel," who memorialized their
loving friendship in a tender poem of chaste but sensual couplets: "He to
whose lips the taste of old wine clings/ Asks no new wine. Ah me! My
friend's loss brings/ No wish for some new friend to fill his place."
Why did Adderley renounce the privileges of aristocratic birth and the
comforts of family to live for six decades in voluntary poverty and sexual
celibacy among the London poor as a bachelor slum priest? His only biographer discouraged readers from seeking the psychological roots of
Adderley's singular devotion because he was "a man of simple ways and
thoughts and friendships" who never worried about himself and instead
did God's work as a parish priest. We need not posthumously coerce
Adderley onto the psychoanalyst's couch to suggest that the private and
public, sexual and social forces shaping his life choices may not have
been as "simple" as his "ways."
This book tries to make sense of the ideas and movements, institutions
and practices that made the slums of London and "slumming" seem so
necessary to Adderley and thousands of members of the "comfortable
classes." It examines the complex historical and cultural circumstances
in which such women and men found themselves and to which they importantly contributed. I attempt to save them from the misguided goodwill of those who would make them into saints and the smugness of those who would dismiss them as marginal cranks, or worse yet, as hypocrites. They were none of these. Instead, I try to recapture the altogether
messier mingling of good intentions and blinkered prejudices that informed their vision of the poor and of themselves. While exploring deep
structures of thought and feeling in nineteenth- and early-twentiethcentury British culture, I attend to individuals' particularities. I portray
slum reformers and workers not as mere tools of social or discursive
forces outside their control-though such forces did influence their agendas-but as human beings who confronted ethical dilemmas and made
difficult choices. I examine the interplay of sexual and social politics
both at the micro-level of how women and men came to express and understand who they were and at the macro-level of public debates about
poverty and welfare, gender, and sexuality. By so doing, I work within,
but also reorient, a tradition of scholarship linking private conscience
and public duty in Victorian culture and society.
The intimate, turbulent, and often surprising relationship between
benevolence and sex, rich and poor, in Victorian London is my subject. I
came to this topic circuitously through the history of elite men's and
women's philanthropic endeavors to bring "sweetness and light" to the
dark spaces and dirty inhabitants of the metropolis. As I immersed myself deeply in the sources, I found it impossible to keep sex, sexual desire,
and sexuality out of their story. So what began as an inquiry into classbridging institutions and social welfare programs took on a life of its
own, propelled by several insights. First, it became clear that debates
about "social" questions such as homelessness, social hygiene, childhood
poverty, and women's work were often sparked by and tapped into anxieties about sex, sexuality, and gender roles. To understand how elite
men and women thought about the poor required me to reckon with
how they thought about sex, gender, and themselves. Second, I discovered that the widely shared imperative among well-to-do men and women
to traverse class boundaries and befriend their outcast brothers and sisters in the slums was somehow bound up in their insistent eroticization
of poverty and their quest to understand their own sexual subjectivities.
But how and why were these movements, both literal and imaginative,
connected? And what were the consequences of such linkages for the histories of class, gender, sexuality, and welfare? An inquiry into the set of
social practices and relations that Britons called slumming promised a
means to untangle and knit together in a new way the history of sexual
and social politics. Once I started looking for slumming, it was hard not
to find it everywhere.
The Oxford-educated journalist Henry Wood Nevinson, who lived
with his talented wife Margaret and their growing family in an insectinfested slum flat in the 1880s, astutely observed that slumming expressed both "shamed sympathy" with the poor and an irresistible "attraction of repulsion" for them. Nevinson's paradoxical formulation
points to the double optic through which elites viewed the slums of London. Men and women like the Nevinsons knew only too well that slums
were real places of monotonous material deprivation and quiet human
suffering which both rightly elicited their sympathy and called them to
action. At the same time, when elites wrote about slums, they tended to
romanticize and exoticize them as sites of spectacular brutality and sexual degradation to which they were compulsively drawn." Slums were
anarchic, distant outposts of empire peopled by violent and primitive
races; but they were also conveniently close, only a short stroll from the
Bank of England and St. Paul's, inhabited by Christian brothers and sisters. They were prosaically dull and dangerously carnivalesque.
The metropolitan slums provided well-to-do philanthropic men and
women with an actual and imagined location where, with the approval
of society, they could challenge prevailing norms about class and gender
relations and sexuality.' These men and women may well have needed
the freedom the slums offered them more than the poor in their adopted
neighborhoods benefited from their benevolent labors. Such claims capture the complex social dynamics of philanthropic encounters between
rich and poor, as well as my own ambivalence about them. Reformers'
creativity and passion, their sincerely felt and lived ethos of service, inspire admiration. At the same time, many were deeply invested in the titillating squalor of the slums, which they used as stages upon which they
enacted emancipatory experiments in reimagining themselves. Synonymous with squalid tenements and soiled lives, the slums of London ironically functioned as sites of personal liberation and self-realization - social, spiritual, and sexual - for several generations of educated men
and women. Upper-class men and women had long ventured into the low haunts of
London in pursuit of illicit pleasure. In 1670, the Queen and the
Duchesses of Richmond and Buckingham caused a public uproar when
they disguised themselves as "country lasses" at Bartholomew Fair to
mingle undetected with the common people. "They had all so over done
it in their disguise," Sir Henry Ingilby reported in his diary, that they
quickly drew the attention of the mob, which angrily pursued them all
the way to the Court gate.

</file>

<file= AmE06_J46>

Additives
Additives are a substance or combination of substances that are mixed with food products as a result of production, processing, packaging, or storage. The total market for food additives was estimated at $4 billion in 2006 in the United States and grows at an approximate rate of 4.8 percent annually. Additives are included among ingredients for any number of reasons, ranging from improving nutritional value to enhancing quality or consumer acceptability. There are many categories of food additives, including preservatives, processing aids, coloring agents, flavorings, and nutritional supplements. There are also growing concerns over the increasing use of additives and their relation to allergic-like reactions, cancer, and other illnesses. Some additives have been banned outright due to insufficient testing or dangerous problems related to their use.
History of Additives and Legal Background
As with many other elements used in food processing, additives originate very early in human history. For example, people learned in prehistoric times that adding salt to meat would preserve it. Likewise, smoke, which also acts as a preservative, might be considered an early food additive. Over time, additives have come to thoroughly influence our eating habits, our taste preferences, and our sociocultural development.
The earliest legislation controlling the use of food additives took place in Britain in the 19th century, following the work of Frederick Accum, though its original impetus was the prevention of food adulteration. In the United States, the Food and Drug Administration (FDA) oversees the regulation of ingredients that can be added to food. It was not until 1958 that legislation was adopted requiring food and chemical manufacturers to test their additives before they were submitted to the FDA. Before that law, the FDA itself was responsible for testing the submitted additives. Thereafter, Congress established a Generally Recognized as Safe (GRAS) list. This list recognized that many substances that had been added to food for a long time were commonly seen as safe by qualified scientists, which exempted them from premarket clearance. This list was revised in 1969 and as of 1980 contained 415 substances that were originally included in the 1958 project. Today, manufacturers are responsible for demonstrating their GRAS status and providing evidence (such as scientific literature) to support it. Approximately 100 new substances are presented to the FDA for GRAS certification every year.
Also included in 1958 law was an amendment called the Delaney Act; it stipulated that "no additives may be permitted in any amount if the tests show that it produces cancer when fed to man or animals or by other appropriate test." Many manufacturers, as well as some FDA commissioners, have criticized this amendment as being unenforceable. Instead, in 2003, the FDA adopted a "no residue" clause; this clause provided that the FDA could approve an animal feed additive or drug that induces cancer if there is "no residue" of the additive found after slaughter. The FDA maintains that the risk is slight or nonexistent if humans consume meat or drink milk from an animal with a minuscule amount of a carcinogen present.
Uses and Categories of Food Additives
Additives are used in foods for a wide array of purposes. Some of these include improving nutritional value, enhancing taste or consumer acceptability, improving storage potential, making the food more readily available, and facilitating its preparation. Food additives can be categorized by function; the main categories explored here are preservatives, appearance additives, and nutritional supplements.
Preservatives are added to food substances to slow spoilage and deterioration. Some sources estimate that there are about 100 common preservatives being used by the food industry. Preservatives are specific to the food being preserved; antioxidants are used in fatty products, for example, while breads often have "mold inhibitors." Other common multipurpose preservatives are familiar ingredients such as salt, sugar, and vinegar. All of these extend the shelf life of food products and reduce the risk of food-related disease. Some preservation techniques, such as irradiated food, aseptic processing, and microwave pasteurization, reduce the need for conventional food additives used to reduce spoilage, but their safety is sometimes debated.
Appearance additives include texturizers, coloring agents, and flavorings. These additives enhance the look, mouthfeel, or taste of foods without changing other characteristics. Texturizers are put into food products to maintain a desired texture, and sodium nitrate is used to develop and stabilize the pink color in meats. Coloring agents are used to increase attractiveness of food to consumers; foods that are frequently colored include candy, soft drinks, and dairy products such as butter, cheese, and cream. Flavoring additives are the most common type of additive approved for human consumption. Of the roughly 3,000 additives currently approved, about 2,000 are flavorings used to replace the flavors lost during processing. Hence this category of additives is used mainly to appeal to user tastes and to enhance purchasing of the target food product.
Nutritional supplements are another major type of additive. Breakfast cereals, which often make extensive nutritional claims, rely heavily on such additives. Many of the additives used are vitamins (such as vitamin C) and are intended to make the product a better source of nutrition. Other uses include moisture control, thickening, and processing aids.
To regulate all these additives, and to help consumers sort through the complex and confusing terminology, each additive is assigned a unique number. This codification (also called E numbers in Europe) is recognized internationally and is managed by the Codex Alimentarius Commission. The Food and Agriculture Organization and the World Health Organization created this commission in 1963. Its mission is to develop food standards and guidelines for identification worldwide. For example, aspartame, a popular artificial sweetener, is coded E951, while monosodium glutamate (MSG) is coded E621.
This classification identifies all existing additives, regardless of whether they are approved for use or not. Hence, an additive might be approved for use in one country but banned in another. This is why universal classification is required. In some cases, a product that is approved in some countries but not in Europe will have a unique number but no corresponding E number. For example, acetic acid, which is approved in Australia, is additive 103, but does not have an E103 number.
Additives can be either synthetic (i.e., chemically produced, artificial) or natural in origin. A 2006 study found that participants significantly preferred foods with natural additives over those with synthetic ones.
Critics and Dangers Related to Additives
Criticisms directed toward food additives are abundant. The most common criticisms focus on allergic-like reactions, carcinogenic properties, and a causal relationship to attention deficit hyperactivity disorder (ADHD). Growing concerns have also been related to additive overuse.
Over the years, many people have reported adverse reactions to certain food additives, including aspartame and MSG. To date, no consistent pattern of symptoms can be attributed to the use of aspartame, and carefully controlled clinical studies have shown that it is not an allergen. MSG, sulfites (used as colorants), and FD&C yellow #5 also cause "allergic-like" reactions but have not been given allergen status. For now, agencies require producers using these ingredients to carefully label their products while they continue monitoring the situation.
There are also concerns about food additives and cancer. A few additives have been linked to certain forms of cancer, but some scientists believe that the amount needed to cause harm is so excessive that there is no real danger; most additives that involve risk are allowed in foods only at levels less than a hundredth those at which the risk is still known to be zero. Hence, in these situations, the benefits outweigh the risks. Once the risk is too high, these additives are usually banned from use in food products.
Finally, some people think that certain food additives, especially artificial colors, can cause hyperactivity in children or may exacerbate ADHD. At the moment, there is not enough scientific evidence to support a connection between these additives and hyperactivity or ADHD in children. Research is ongoing.
Although the use of single food additives at their regulated concentrations is believed to be relatively safe, their effects when combined remain unclear. Hence, there are concerns that cytotoxicity (toxicity to the cells) might be amplified when the number of different additives consumed increase. As the number of approved additives grows, the risks of combined cytotoxicity also increases.
Moreover, some believe that if adverse reactions to additives seem statistically rare, it is only because they are underdiagnosed in part due to a low index of suspicion. But once a specific additive is identified as causing an adverse reaction, basically all its forms are avoided and consequently banned.
Advertising
More than in any other nation, American foodways have been formed and nurtured by advertising and media. The birth of the hype machine can be traced to the mid-19th century, when revolutions in transportation, settlement patterns, and cheap newsprint led to enormous changes in eating patterns. In the post-Civil War era, railroads had begun to send oranges from Florida and grain from the Midwest to the urban centers of the northern United States even as those very cities were becoming engorged from an influx of ex-farmers and immigrants. Mass-market magazines were at the ready to instruct the new urban middle classes in ways of preparing the brand-new, industrially produced foods that flooded the market.
The invention of modern advertising can largely be credited to patent medicine sellers of the Reconstruction era. They came up with all sorts of spurious and even dangerous cures for such ill-defined diseases as neuralgia and dyspepsia, which seemed epidemic in that unsettled time. To promote their nostrums, the hucksters scared their customers with "facts" almost guaranteed to induce psychosomatic symptoms, printed advertisements offering miraculous cures (showman P. T. Barnum was a one-time copywriter), enlisted celebrities as spokesmen, and sponsored traveling medicine shows where quack doctors and their accomplices testified to the efficacy of their potions. Early soft drinks were sold and marketed as patent medicines. An 1892 advertisement for Coca-Cola was typical of the genre: the carbonated potion was recommended as "the Ideal Brain Tonic for Headache & Exhaustion." Coffee substitutes were originally promoted in much the same way. As late as 1951, the Federal Trade Commission was investigating the Post Company for running ads for its Postum beverage claiming that drinking coffee discourages marriage or that it results in "divorces, business failures, factory accidents, juvenile delinquency, traffic accidents, fire or home foreclosures.''
Postum, created in 1895 by Charles W. Post, was just one of many new foods concocted around the turn of the 20th century. The most notable of these were breakfast cereals like Post's own Grape-Nuts, Kellogg's Corn Flakes, and the products of the Shredded Wheat Company. Advertising was the heart and soul of the cereal business from the beginning. The pioneer in this regard was Henry Crowell of the Quaker Oats Company. He was the first to register a trademark for a cereal, in 1877, and he promoted his brand by plastering signs on barns, trains, and even the white cliffs of Dover. Crowell came up with the idea of giving away bowls and saucers as a premium. In imitation of the patent medicine sellers, his ads made various unsubstantiated medical claims. Before Crowell's campaign, oatmeal had been relatively rare in the United States, but now who could turn down a food that promised to "supply what brains and bodies need  ...  with more proteids [sic], more phosphorus, more lecithin than any other food''? A generation later, when Kellogg's Corn Flakes hit the market, the advertising onslaught only intensified. In 1912, Kellogg's erected a 50- by 106-foot sign in New York's Times Square - at that time the largest billboard in the world. In Chicago, a moving electric sign was installed atop a building at State and Adams streets. By 1942, Kellogg Company figures showed that they had spent approximately $100 million on advertising. For most of the 19th century, there was no such thing as a breakfast cereal; by the mid-20th century, 50 percent of Americans would be eating an ounce or so of it every day.

</file>

<file= AmE06_J47>

A CRUCIAL METHODOLOGICAL INNOVATION
We live in an era of high-tech everyday life. Computer chips can be found in automobiles, toys, washing machines and toasters. The study of language use and acquisition is no exception. Among the techniques currently in fashion are computerized analysis of language samples (MacWhinney, 2000), split video monitors with a soundtrack that matches one image for the study of language comprehension (Hirsh-Pasek & Golinkoff, 1996), evoked-potential recording to trace the division of labor between the two hemispheres in language processing (Mills, Coffey-Corina & Neville, 1997), magnetic resonance imaging (MRI) to identify the brain locus of specific aspects of language processing (Raichle, 1994), and molecular genetic techniques to identify and characterize the influence of specific genes on language (Plomin & Dale, 2000; Meaburn, Dale, Craig & Plomin, 2002). In contrast to these technologically complex methods, this chapter is based largely on the revival and improvement of a very old and low-tech approach to studying language; one that is not only practical and cost effective, but is also, for certain purposes, simply footer than the alternatives. It is parent report: the systematic utilization of the extensive and representative experience of parents (and potentially other caregivers) with their children. Parent report, in the form of diary studies, is in fact the oldest form of child language research. A long series of "baby biographies," often examining language, began with Tiedemann's 1787 diary of infant behavior and
includes such distinguished examples as Darwin's 1877 study (both excerpted in Bar-Adon & Leopold, 1971). The tradition continues fruitfully, as in Dromi (1987) and Tomasello (1992). Despite the scientific significance of these studies, there has been an understandable reluctance to use parent report more generally for purposes of language assessment or substantive investigation. Unlike Dromi and Tomasello, most parents do not have specialized training in language development and may not be sensitive to subtle aspects of structure and use. Furthermore, a natural pride in their own child and a failure to critically test their impressions may cause a parent to overestimate developmental level;
conversely, frustration in the case of delayed language may lead to underestimates. One of the most striking developments in the study of child language, and indeed developmental psychology more generally, over the past 20 years is the revival of parent report as a trustworthy, and trusted, research
technique. The work of Elizabeth Bates and her colleagues in Italy and the United States has been central to this revival.2 Indeed, the single most widely used language assessment instrument for young children included in published research today is a product of Bates's research program: The
MacArthur Communicative Development Inventories (MCDI; Fenson, Dale, Reznick, Bates, Thai, & Pethick, 1994).
The MacArthur Communicative Development Inventories had their genesis in free-form interviews with parents (Bates et al., 1975). They continued to evolve through structured interviews using a standard set of openended probes (Bates, Benigni, Bretherton, Camaioni & Volterra, 1979), to
a highly structured checklist administered orally, to a self-administered checklist format, the Early Language Inventory (never published, but widely used in research). Gradually a range of aspects of early language and communication was incorporated in the research program, including preverbal
communication and symbolic skills, morphology, and syntax. A more complete account of this development, including the emergence of an impressive body of validation evidence for these measures, may be found in Fenson et al. (1994) and Dale (1996). By 1988, there was clear evidence for the research and clinical utility of this type of measure, together with a body of detailed, item-by-item information necessary to develop scales with adequate psychometric properties. With funding from the MacArthur Foundation Network on Early Childhood Transitions, a final, substantial revision

was undertaken, along with a very large (N = 1,803) norming project, and several validation studies.
Following the principle that parents can best evaluate emerging aspects of language, two forms of the Inventory were developed, one for children between 8-16 months (MacArthur Communicative Development Inventory: Words and Gestures, or MCDLWG) and one for children between 16 and 30 months (MacArthur Communicative Development Inventory: Words and Sentences, or MCDLWS). Both instruments are often used with somewhat older children whose communicative development is delayed. The MCDLWG includes two major components. The first is a 396-item vocabulary checklist organized into 19 semantic categories. Parents indicate for each word if the child understands the word, or if the child understands and says the word. The second component is a list of 63 communicative, social, and symbolic gestures. The MCDLWS also has two major components. The first is a vocabulary
checklist of 680 words organized in 22 semantic categories. Parents are asked only to indicate if they have heard their child say the word. The second major component utilizes an innovative sentence pair format to assess grammar, e.g., to judge whether "kitty sleep" or "kitty sleeping" sounds
most like the way their child is talking. (See Marchman & Thai, this volume, for more discussion of the grammar component.) The central place of vocabulary in the MCDI is itself notable. Vocabulary
development was relatively neglected in research on early language development during the 1960s and 1970s. One source of this neglect was the difficulty of studying vocabulary in young children, for whom structured testing is often unfeasible. Probably more important, however, was the intense focus on syntax that stemmed from Chomsky's (1957) emphasis on the complexity, creativity, and abstractness of syntactic structure, and the exciting discovery of some of this complexity and creativity in the language of
quite young children (Brown, 1973). The linguistics and psycholinguistics of the era emphasized the distinction between lexicon and grammar. On first consideration, lexical development appeared to be less creative, less combinatorial, and less pattern-governed than syntax. Nevertheless, by the late 1970s interest in vocabulary development also revived. Several sources contributed to this revival. One was interest in the cognitive foundations of lexical development (Bloom, 1973; Gopnik & Meltzoff, 1986). Another was interest in individual differences (Nelson, 1973). But we suggest that the main reason was a renewed appreciation of the fundamental challenge of learning a word. Consider what would appear to be the simplest condition of word learning. A fluent speaker of the language, such as a parent, undertakes
to teach a learner a word by uttering the word, say, "gavagai" and pointing in the direction of a small furry white object, recognizably a rabbit (borrowing from Quine, 1960). What does "gavagai" mean? Does it mean 'rabbit'? Perhaps, but it might also plausibly mean 'white' or 'animal' or 'food' or 'tail' or 'my pet rabbit'. Each type of reference is a plausible use of the word. And even if we were correct in believing that "gavagai" meant 'rabbit', how would we know the range of animals to which this label might be correctly applied? That is, every word other than proper names is a label for a category, not a single object, action or quality. Learning the category boundaries for each word is a specific "problem of induction." Children are
placed in exactly this situation, and must solve the problem for tens of thousands of words; the fact that they do so successfully means that some general principles of acquisition must be involved (Clark, 1995; Markman, 1994). While the MCDI provides measures of gesture and grammatical development, it is largely devoted to measuring vocabulary, and that will be the focus of the present chapter.
Advances in observational and measurement techniques have often directly stimulated theoretical advances, because they do not simply lead to more precise measurement of what is already studied, but to the observation and/or measurement of new entities or quantities. The development
of the microscope and of the telescope are good examples of this, leading to the discovery of new forms of life and new objects in the sky, respectively. The same process occurs in the study of language development. As Bates and Carnevale (1993) wrote, "the field of child language has reached a new
level of precision and sophistication in the way that we code and quantify linguistic data. Single-case studies, qualitative descriptions and compelling anecdotes will continue to play an important role. But they cannot and will not be forced to bear the full weight of theory-building in our field." The development of the MCDI made possible a measure of overall vocabulary size which was both comprehensive and cost-effective. Unlike structured tests of vocabulary, it was not biased toward nouns and other words which are easily pictured; unlike language samples, it was not biased toward highfrequency words. Furthermore, it was cost-effective enough to be used longitudinally with a substantial sample. Thus it provides a perspective on early language development, especially vocabulary development, that was not
previously possible. In particular, it made possible the establishment of a database on vocabulary development which was could be used to characterize both commonality and individual differences in vocabulary development. The ability to draw on both commonality and individual differences
provides a foundation for theorizing at the level of specific acquisition processes which is far better grounded than relying on either alone. The authors of this chapter have been the beneficiaries, both personally and professionally, of the collaborative program of research which led to the MCDI. Liz Bates's collegiality, collaborative enthusiasm, and passionate commitment to addressing the "big questions" of child language development with multiple innovative techniques including parent report are responsible for the major themes of our own research. Her generous friendship and copious humor have made it a delight. In the remainder of this chapter we discuss some of the fruits of our own and related research,
much of it done in collaboration with Bates. Three projects receive special emphasis in this far-from-comprehensive review. The first is the norming study for the MCDI (Fenson et al., 1994). At the time of its publication, it was a uniquely large-sample study of child language development, and both
the design of the study and the consistency of the results did much to gain acceptance for parent report. It was a cross-sectional study, however, and two other studies have added important longitudinal perspectives. The first
is the San Diego Longitudinal Study, conducted by Goodman in collaboration with Bates, which followed a group of 28 children longitudinally from 8 to 30 months with monthly assessment utilizing both the MCDI and laboratory measures of comprehension, production, and word learning. The second is the Twins' Early Development Study (TEDS), directed in the U.K. by Robert Plomin, in which Dale is a collaborator. Measures of language development at ages 2, 3, and 4 years were adapted from the MCDI. In addition to a very large sample size for a longitudinal study - more than 8,000 for most analyses - the twin design made possible the evaluation of genetic and environmental factors in early language development.
WHAT WE HAVE LEARNED ABOUT VOCABULARY
GROWTH FROM THE MCDI
Vocabulary Size as Antecedent and Consequent
Does size matter? Does the number of words learned by a young child reflect important aspects of the present or past environment, or of significant organismic factors? Does it have any continuing, i.e., predictive,
significance? Which "Size?" Before addressing the question of vocabulary size directly,
we must consider an issue which is begged by the use of the singular noun "size." Almost all parents and other observers of young children have an impression that they understand more words than they produce. Every pediatrician has had worried inquiries about children who have barely started to talk, even though they appear to understand much of the speech addressed to them. That impression is confirmed by more systematic research,
which was greatly stimulated by Nelson's (1973) careful comparison of very early receptive and expressive vocabularies (see Bates, Dale & Thai, 1995 for a more complete review). Nevertheless, it is possible that even though receptive vocabulary exceeds expressive, the two might be sufficiently well correlated that a single number would characterize both developments. Here is a first payoff from the increased sample size that is made possible by the use of parent report. 
As Fenson et al. (1994) document, there is enormous variability in the development of both receptive and expressive vocabulary. But more surprising is the developmental asynchrony between the two.

</file>

<file= AmE06_J48>

Longitudinal data from a study of kindergarten through 5th graders were used to estimate a structural model in which chronic peer exclusion and chronic peer abuse were hypothesized to mediate the link between children's early peer rejection, later classroom engagement, and achievement. Peer exclusion and abuse were expected to predict changes in 2 forms of school engagement (classroom participation and school avoidance), and changes in both forms of engagement were expected to predict changes in achievement. The model fit the data well and lent support to the premise that distinct forms of peer maltreatment and classroom engagement mediate the link between early peer rejection and changes in children's achievement. Early peer rejection was associated with declining classroom participation and increasing school avoidance, but different forms of chronic peer maltreatment mediated these relations.
Whereas chronic peer exclusion principally mediated the link between peer rejection and classroom
participation, chronic peer abuse primarily mediated the link between rejection and school avoidance. Children's reduced classroom participation, more than gains in school avoidance, anteceded decrements in children's achievement.
The premise that peer group acceptance and rejection influences
children's development and adjustment has been a compelling
impetus for research on children's peer relationships since the
1930s (see Ladd, 2003). Since that time, a sizable body of empirical
findings has been assembled, much of which is consistent with
the inference that peer group rejection is a cause of children's
adjustment difficulties (see Ladd, 1999; MacDougall, Hymel,
Vaillancourt, & Mercer, 2001; Parker & Asher, 1987). Contributing
to this corpus of evidence were early investigations in which
investigators found that peer group rejection was concurrently
linked with child maladjustment (e.g., Koch, 1933; Northway,
1944) and recent longitudinal studies in which researchers reported
that childhood peer group rejection not only anteceded early- and
later emerging adjustment problems (Boivin, Hymel, & Bukowski,
1995; Ladd, 1999) but also predicted these problems independently
of other potential risk factors (e.g., children's behavioral dispositions, and the like; Coie, Lochman, Terry, & Hyman, 1992;
Ladd & Burgess, 2001).
Among the most convincing evidence gathered to date are
findings that implicate peer group acceptance or rejection as an
antecedent of children's school adjustment problems (see Ladd,
2003; Parker & Asher, 1987). The construct of peer acceptance/
rejection has been defined at the level of the peer group (e.g., in
contrast to friendship, which is defined at the level of the dyad),
and it has been construed as an attitudinal variable that reflects the
collective valence of group members' sentiments (i.e., liking,
disliking) toward individuals in the group (Buhs & Ladd, 2001).
Thus, when administered in classrooms, measures of peer group
acceptance/rejection yield information about how well liked versus
disliked a child is, on average, by classmates. Low classroom peer
acceptance has been consistently linked with indicators of school
disengagement (e.g., negative school attitudes, school avoidance;
Ladd, 1990; Ladd, Kochenderfer, & Coleman, 1997), and, relative
to other types of peer relationships, peer group rejection appears to
be one of the strongest predictors of academic readiness and
achievement (Buhs & Ladd, 2001; Ladd, Birch, & Buhs, 1999;
Ladd et al., 1997; Vandell & Hembree, 1994).
Although these findings support the interpretation that peer
group rejection negatively impacts children's engagement and
achievement in the school environment, insufficient effort has
been devoted to understanding how this linkage is established or
develops over time. In part, this limitation stems from a lack of
theory about the processes through which peer group rejection may
affect children's involvement in learning opportunities and
achievement in classrooms. Only recently have investigators begun
to question how peer sentiments, themselves not directly
observable, can affect children's adjustment (Boivin & Hymel, 1997; Buhs & Ladd, 2001; Bukowski, Hoza, & Boivin, 1993;
Coie, 1990; Wentzel & Caldwell, 1997).
Toward this end, researchers have begun to formulate and
evaluate frameworks that identify specific mediating processes
that account for the association between peer group rejection and
children's school engagement and achievement. On the basis of
propositions advanced by Coie (1990), Buhs and Ladd (2001)
proposed a model in which it was hypothesized that the effects of
peer rejection on children's achievement is mediated through two
processes: (a) the negative behavioral treatment that rejected children
receive from peers and (b) resulting changes that such treatment
causes in children's classroom participation (see Figure 1).
The specific premises upon which this model was constructed can
be summarized as follows (see Buhs & Ladd, 2001): First, peers
express the dislike they feel toward rejected children by treating
them more negatively than other classmates, and, once manifested,
these negative behaviors serve as visible markers of rejection for
both the larger peer group and for rejected children. Second, once
children are marked by maltreatment, or behavioral manifestations
of rejection, they become marginalized from classroom peer activities.
Marginalization occurs because, as peers become aware of
children who are often targeted for maltreatment, they tend not to
associate with these children or include them in classroom activities.
Moreover, rejected children disengage from classroom activities
as a way of avoiding further abuse. Third, disengagement
from classroom activities negatively impacts children's learning,
which ultimately leads to lower levels of achievement.
Preliminary empirical support for this model was obtained by
Buhs and Ladd (2001) with a sample of young children who were
identified at school entrance and followed from fall to spring of
their kindergarten school year. Results showed that children who
were rejected by their classmates in the fall of the school year were
more likely to be maltreated by classmates and that exposure to
these rejecting behaviors partially mediated the negative association
found between peer rejection in the fall of kindergarten and
changes in classroom participation over the course of the school
year. Furthermore, these findings also revealed that early peer
rejection was negatively related to later achievement and that this
association was partially mediated through peer maltreatment and
declining classroom participation, respectively.
Although illuminating, the Buhs and Ladd (2001) results were
limited in three important ways. First, because the construct of
peer maltreatment was evaluated as a latent variable, the findings
did not generate a high degree of specificity about the types of peer
maltreatment that may be most closely associated with children's classroom disengagement. The latent peer maltreatment variable
was constructed from three indicators of rejecting behaviors, including
being ignored or rebuffed by peers when attempting to
interact or enter activities (via a measure of unilateral entry bids),
being actively excluded from peer activities (peer exclusion), and
being verbally or physically harassed (peer abuse or victimization).
Measurement model results revealed that, although the latent negative
treatment variable contained information about all three
forms of maltreatment, exclusion had the highest lambda (loading)
of the three indicators. For this reason, the investigators speculated
that, as children began school, exclusion might be a more powerful
mediator of the link between peer rejection, declining classroom
participation, and underachievement.
Whether exclusion is the principal mediator between rejection
and classroom disengagement, and whether this inference generalizes
beyond the first year of formal schooling - and to other
forms of school disengagement - remains to be evaluated. Note
that Buhs and Ladd (2001) hypothesized that classroom disengagement
was a consequence not only of peers' exclusionary behaviors
but also of children's propensity to avoid contexts in which they
are likely to be abused by peers. This logic implies that there are
two pathways to classroom disengagement that emanate from
peers' rejecting behaviors: First, children become less active participants
in classroom activities because their opportunities to do
so are increasingly restricted as a result of peer exclusion. Second,
children who are harassed by peers seek to avoid classrooms (or
the school context in general) as a means of escaping further abuse.
Whereas exclusion by peers is construed as a cause of reduced
participation in classroom activities, abuse is seen as increasing
children's motivation to avoid the classroom or school context.
Notice that these two pathways imply that different forms of
classroom disengagement evolve from each form of peer maltreatment.
This observation illustrates a second limitation of the Buhs
and Ladd (2001) study: Only one form of classroom engagement
(i.e., classroom participation) was examined as a mediator between
peer maltreatment and children's achievement. A better test of the
Buhs and Ladd model would entail an empirical evaluation of the
following predictions: (a) decrements in children's classroom participation
are better predicted by peer exclusion than by peer
abuse, and increments in children's school avoidance are better
predicted by peer abuse than by peer exclusion; and (b) the link
between peer exclusion and achievement is mediated through
classroom participation, and the link between peer abuse and
achievement is mediated through school avoidance.
Third, the Buhs and Ladd (2001) results were limited by features
of their longitudinal design and assessment plan. Although it was
proposed that peer rejection incites peer maltreatment, the measures
of these constructs were obtained concurrently. A more
rigorous test of this premise requires a design and assessment plan
in which the former variable is assessed prior to the latter, thereby
establishing temporal precedence for peer rejection. Furthermore,
because Buhs and Ladd (2001) conducted their study within a
single school year (kindergarten), it remains to be determined
whether the forms of peer maltreatment that originate from early
peer rejection endure over time (e.g., across grade levels) and, if
so, whether children who are exposed to sustained (chronic) rather
than transient maltreatment are at greater risk for classroom disengagement
and delayed scholastic progress. On the basis of logic
that originates within theories of psychological risk, stress, and
support (Dohrenwend & Dohrenwend, 1981; Johnson, 1988; Ladd
& Troop-Gordon, 2003), it was hypothesized that prolonged peer
maltreatment increases the probability that children will disengage
from classrooms (or the school context) and that increasing disengagement
impairs children's achievement. Thus, it was predicted
that longer rather than shorter histories of peer maltreatment,
after controlling for contemporary exclusion or abuse, would
mediate the link between early peer rejection and later classroom
disengagement.
Thus, the principal purpose of this investigation was to address
prior limitations by implementing the proposed innovations and
gathering data that would reflect on the aforementioned hypotheses
(see Figure 1). In contrast to the Buhs and Ladd (2001)
short-term longitudinal design, we conducted a 6-year prospective
longitudinal study that spanned the school years of kindergarten
through fifth grade. The assessment plan was expanded to include
measures of peer group acceptance/rejection, peer exclusion, peer
abuse, classroom participation, school avoidance, and achievement.
Peer group acceptance/rejection was defined as the extent to
which individuals were liked/disliked by classroom peers and
indexed with averaged sociometric ratings that were obtained from
classmates during children's first year in grade school (kindergarten). 
One form of peer maltreatment - peer exclusion - was defined
as the extent to which children were the target of peers'
nonaggressive rejecting behaviors, including behaviors such as
ignoring, avoiding, or refusing to associate with them in the
classroom context. The other form of peer maltreatment - peer
abuse - was defined as the extent to which children were recipients
of classmate's aggressive and harassing behaviors (i.e., the
target of confrontive aggressive acts such as verbal and physical
aggression). Measures of the two peer maltreatment constructs
were obtained in kindergarten and every year thereafter until grade
5, and separate indicators of chronic versus current peer maltreatment
were created so that the predictive efficacy of chronic peer
maltreatment could be estimated while controlling for current peer
maltreatment. The constructs used to represent aspects of classroom
disengagement were termed classroom participation and
school avoidance. Classroom participation encompassed two aspects
of children's classroom behavior that have been termed
autonomous and cooperative participation (see Buhs & Ladd,
2001; Ladd et al., 1999; Ladd, Buhs, & Seid, 2000). Both forms of
participation have been shown to be indicators of classroom engagement
and predictors of achievement. Autonomous participation
refers to classroom behaviors that are characterized by initiative
or self-directedness (e.g., starting activities, working
independently, seeking challenges), and cooperative participation
refers to classroom behaviors that are conducted in a socially
responsible manner (i.e., adhering to classroom rules and role
expectations; see Ford, 1985; Wentzel, 1991). School avoidance
was defined as the degree to which children expressed a desire to
avoid school and engaged in school-avoidant behaviors.

</file>

<file= AmE06_J49>

In the early 1970s when andragogy and the concept that adults and children learn differently was first introduced in the United States by Malcolm Knowles, the idea was groundbreaking and sparked much subsequent research and controversy. Since the earli- est days, adult educators have debated what andragogy really is. Spurred in large part by the need for a defining theory within the field of adult education, andragogy has been extensively analyzed and critiqued. It has been alternately described as a set of guidelines (Merriam, 1993), a philosophy (Pratt, 1993), a set of assumptions (Brookfield, 1986), and a theory (Knowles, 1989). The disparity of these positions is indicative of the perplexing nature of the field of adult learning; but regardless of what it is called, &#8220;it is an honest attempt to focus on the learner. In this sense, it does provide an alter- native to the methodology-centered instructional design perspective  (Feur and Gerber, 1988). Merriam, in explaining the complexity and present condition of adult learning theory, offers the following: 
It is doubtful that a phenomenon as complex as adult learning will ever be explained by a single theory, model or set of principles. Instead, we have a case of the proverbial elephant being described differently depending on who is talking and on which part of the animal is examined. In the first half of this century, psychologists took the lead in explaining learning behavior; from the 1960s onward, adult educators began formulating their own ideas about adult learning and, in particular, about how it might differ from learning in childhood. Both of these approaches are still operative. Where we are headed, it seems, is toward a multi- faceted understanding of adult learning, reflecting the inherent richness and complexity of the phenomenon.
Despite years of critique, debate, and challenge, the core principles of adult learning advanced by andragogy have endured (Davenport and Davenport, 1985; Hartree, 1984; Pratt, 1988), and few adult learning scholars would disagree with the observation that Knowles ideas sparked a revolution in adult education and training (Feur and Gerber, 1988). Brookfield (1986), positing a similar view, asserts that andragogy is the single most popular idea in the education and training of adults.  Adult educators, particularly beginning ones, find these core principles invaluable in shaping the learning process to be more conducive to adults. It is beyond the scope of this introductory book to address the many dimensions of the theoretical debate raised in academic circles. Our position is that andragogy presents core principles of adult learning that in turn enable those designing and conducting adult learning to build more effective learning processes for adults. It is a transac- tional model in that it speaks to the characteristics of the learning transaction, not to the goals and aims of that transaction. As such, it is applicable to any adult learning transaction, from community education to human resource development in organizations.

Care must be taken to avoid confusing core principles of the adult learning transaction with the goals and purposes for which the learn- ing event is being conducted. They are conceptually distinct, though as a practical matter may overlap considerably. Critiques of andra- gogy point to missing elements that keep it from being a defining the- ory of the discipline of adult education (Davenport and Davenport, 1985; Grace, 1996; Hartree, 1984), not of adult learning . Grace, for example, criticizes andragogy for focusing solely on the individual and not operating from a critical social agenda or debating the rela- tionship of adult education to society. This criticism reflects the goals and purposes of adult education. Human resource developers in organizations will have a different set of goals and purposes, which andragogy does not embrace either. Community health educators may have yet another set of goals and purposes that are not embraced. Therein lies the strength of andragogy: It is a set of core adult learning principles that apply to all adult learning situations. The goals and purposes for which the learning is offered are a separate issue. Adult education (AE) professionals should develop and debate models of adult learning separately from models of the goals and purposes of their respective fields that use adult learning. Human resource development (HRD), for example, embraces organizational performance as one of its core goals, whereas adult education may focus more on individual growth. Having said that, these core principles are also incomplete in terms of learning decisions. Figure 1-1 graphically shows that andragogy is a core set of adult learning principles. The six principles of andra- gogy are (1) the learners need to know, (2) self-concept of the learner, (3) prior experience of the learner, (4) readiness to learn, (5) orientation to learning, and (6) motivation to learn. These principles are listed in the center of the model. As you shall see in this and sub- sequent chapters, there are a variety of other factors that affect adult learning in any particular situation and may cause adults to behave more or less closely to the core principles. These include individual learner and, situational differences , and goals and purposes of learning , shown in the two outer rings of the model. Andragogy works best in practice when it is adapted to fit the uniqueness of the learners and the learning situation. We see this not as a weakness of the principles, but as a strength. Their strength is that these core principles apply to all adult learning situations, as long they are considered in concert with other factors that are present in the situation. This sixth edition of The Adult Learner provides a journey from theory to practice in adult learning. Figure 1-1 provides a snapshot summary of the journey in displaying the six core adult learning principles surrounded by the context of individual and situational differences, and the goals and purposes of learning. The following chapters will reveal the substance and subtleties of this holistic model of andragogy in practice.
The first part of the book The Roots of Andragogy  presents the core principles of adult learning: andragogy. It traces the development of the theory and focuses on the core unique characteristics of adults as learners. Part 2, Advances in Adult Learning addresses the two outer rings. Chapter 7 discusses in detail the Andragogy in Practice model introduced in this chapter and discusses how to apply it in different settings. Chapter 8 discusses adult learning as practiced within human resource development. Chapter 9 focuses on new thinking about andragogy and elaborates on applying the core prin- ciples to different learners. Chapter 10 discusses new advancements in the understanding of adult learning that enable facilitators to further adapt application of the core principles. Chapter 11 summarizes these two sections by looking at the future of andragogy in the areas of research and practice. Part 3, Practice in Adult Learning, presents selected readings that elaborate on specific aspects of andragogy in practice. These include strategies to implement the core assumptions, to tailor learning to individual differences, and to implement adult learning in organizations. Of special interest are two self-assessment instruments, the Core Competency Diagnostic and Planning Guide (Chapter 16) and the Personal Adult Learning Style Inventory (Chapter 17), that enable the reader to begin a personal development journey in adult learning.
 Reflection Questions
What are your general thoughts on how humans learn? 
Based on personal experience, what key factors are related to adult learning? 
 If you understood more about how adults learn, how would you use this information? 
This is a good question. Perhaps you shouldnt. If you have no questions about the quality of learning in your organization, if you are sure its the best it can be, we suggest that you cancel your order for this book and get a refund. However, if you re a policy-level leader, a change agent, a learning specialist, or a consultant, you should seriously consider exploring learning theory. Doing so will increase your understanding of various theories and your chances for achieving your desired results. Policy-level leader may have such questions as: Are our HRD interventions based on assumptions about human nature and organizational life that are congruent with the assumptions on which our management policies are based? Is our HRD program contributing to long-run gains in our human capital, or only short-run cost reduction? Why do our HRD personnel make the decisions they do concerning priorities, activities, methods and techniques, materials, and the use of outside resources (consultants, package programs, hard- ware, software, and university courses)? Are these the best decisions? How can I assess whether or not, or to what degree, the program is producing the results I want? Managers may have all of these questions plus others, such as: Which learning theory is most appropriate for which kind of learning, or should our entire program be faithful to a single learning theory? How do I find out what learning theories are being followed by  the various consultants, package programs, and other outside resources available to us? What difference might their theoretical orientation make in our program? What are the implications of the various learning theories for our program development, selection and training of instructional personnel, administrative policies and practices, facilities, and program evaluation? Learning specialists (instructors, curriculum builders, and meth- ods, materials, and media developers) may have some of those questions in addition to the following: How can I increase my effectiveness as a learning specialist? Which techniques will be most effective for particular situations? Which learning theories are most congruent with my own view of human nature and the purpose of education? What are the implications of the various learning theories for my own role and performance? Consultants (change agents, experts, and advocates) may have some of these questions plus others, such as: Which learning theory should I advocate under what circumstances? How shall I explain the nature and consequences of the various learning theories to my clients? What are the implications of the various learning theories for total organizational development? Which learning theory is most consistent with my conception of the role of consultant? A good theory should provide explanations of phenomena as well as guidelines for action. But theories about human behavior also carry with them assumptions about human nature, the purpose of education, and desirable values. Understandably, then, a better understanding of the various learning theories will result in better decisions regarding learning experiences and more desirable outcomes.
What ia A Theory?
It seems that most writers in this field dont expressly define the term theory , but expect their readers to derive its meaning from their use of the term. Torraco (1997) informs us that a theory simply explains what a phenomenon is and how it works (p. 115). Websters Seventh New Intercollegiate Dictionary gives five definitions: (1) the analysis of a set of facts in their relation to one another; (2) the general or abstract principles of a body of fact, a science, or an art; (3) a plausible or scientifically acceptable general principle or body of principles offered to explain phenomena; (4) a hypothesis assumed for the sake of argument or investigation; (5) abstract thought. Learning theorists use all five of these definitions in one way or another, but with wide variations in their usage: Here, for example, are some definitions by usage in context. The research worker needs a set of assumptions as a starting point to guide what he/she does, to be tested by experiment, or to serve as a check on observations and insights. Without any theory, researcher activities may be as aimless and as wasteful as the early wanderings of the explorers in North America . . . knowledge of theory always aids practice. (Kidd, 1959, pp. 134) A scientist, with the desire to satisfy his/her curiosity about the facts of nature, has a predilection for ordering his/her facts into systems of laws and theories. He/she is interested not only in verified facts and relationships, but in neat and parsimonious ways of summarizing these facts. (Hilgard and Bower, 1966, pp. 1-2) Every managerial act rests on assumptions, generalizations, and hypotheses; that is to say, on theory.

</file>

<file= AmE06_J50>

Character education 
The field of character education is much more difficult to chart. The term has been mostly used in the USA, but for a longer time than the term moral education. There was a large interest in educating for character development starting at the end of the nineteenth century and continuing through the first four decades of the twentieth century, essentially interrupted by World War II (McClellan, 1999). It was mostly a traditional approach, focusing on the inculcation of desirable habits. Even John Dewey (1922) defined character as the 'interpenetration of habits' (p. 38) and the effect of consequences of actions upon such habits. This behavioural orientation has an important legacy for the development of the field.
After World War II, there was a vacuum in US education concerning educating for positive youth development. But with the social and political upheaval of the 1960s, a resurgent interest in this age-old challenge arose. In response to societal questioning of moral values and policies, numerous approaches to socializing youth were proposed. One was moral education, as described above. Another was values clarification, a relatively short-lived but nonetheless highly influential approach that focused on promoting self-discovery but with an implicit philosophy of moral relativism. Yet another was the rediscovery of character education, often with a strong Aristotelian justification. In fact, most of the character education work blurred the line between Aristotelian virtues ethics and psychological behaviourism, in part because of the somewhat overlapping reliance on the acquisition of habits and in part because of the under-emphasis on theory in the more practice-oriented field of character education. An exception was the study group The Foundations for Moral Education and Character Development which operated in the first half of the 1980s and resulted in a three-volume set of essays - Knowles & McClean, 1992; McClean, Ellrod, Schindler & Mann, 1986; Ryan & McClean, 1986. Despite this extensive body of work, these volumes unfortunately remain obscure and are rarely cited. Recently, Lapsley and Narvaez (2006) have published a broad theoretical review of the conceptual roots of character education.
In the early 1990s, however, the field of character education changed. A pair of attempts to create a national agenda for character education brought together some strange bedfellows under the 'big tent' philosophy of banding together to serve the USA through promoting the positive development of youth, predominantly in schools. The watershed was reached in 1992 when, within a few months of each other, the Josephson Institute launched what later become Character Counts and the first planning meetings of the eventual Character Education Partnership were held. At both meetings were experts with quite diverse perspectives (including representatives of the moral education approach). Through these and other events and initiatives, character education was broadened to include a wider range of pedagogical and philosophical perspectives. However, it remained largely a-theoretical and non-scientific, at least as compared with the moral education movement. More recently, however, a sizeable body of school-based outcome studies has generated an empirical data-base for demonstrating the potential effectiveness of character education (Berkowitz & Bier, 2005).
Character education remains a phenomenon difficult to define, as it includes a very wide range of outcome goals, pedagogical strategies and philosophical orientations. There is substantial overlap between the character education and moral education 'camps', evidenced by a large number of North American members of the Association for Moral Education who also belong to the Character Education Partnership. In fact, numerous character educators have incorporated moral development into their character education models. Lickona (1991) emphasizes the promotion of moral reasoning through peer discussion. Berkowitz and Bier (2005) report that effective character education programs frequently target moral development and implement moral discussion in classrooms. Lickona and Davidson (2005) include critical thinking as one of their targeted '8 strengths of character'. Most notably, the Character Education Partnership's flagship document Eleven principles sourcebook (Beland, 2003), offers a lengthy discussion of moral development and education as part of its focus on the cognitive side of character. Nonetheless there is an uneasy tension between those who hold to a theory-based approach to promoting moral cognitive capacities and those who largely a-theoretically want to promote self-motivated competent moral agents.
Moral vs. character education 
There are a few key differences between moral education (defined here as cognitive-developmental approaches to moral education) and character education, most notably that:
* Moral education tends to be theory-based and character education tends to be a-theoretical. Moral education is heavily influenced by and derived from cognitive-structural models of stages of moral reasoning development (Piaget, 1965; Damon, 1976; Kohlberg, 1976). 
* Moral education, in its current guise dates to the 1960s, remaining relatively stable over time. Character education has been evolving repeatedly for well over a century (McClellan, 1999). 
* Moral education typically has a narrow focus (the development of moral reasoning structures). The exception to this is the infrequently implemented Just Community School model which attempts to expand the narrow moral reasoning development focus to incorporate the development of moral behaviour, values and emotions (Kohlberg & Hersh, 1977), although moral reasoning development remains the primary outcome variable even in Just Community studies. Character education, on the other hand, has a very comprehensive and diverse set of targeted outcomes. Similarly, moral education relies on a very narrow range of pedagogical strategies and character education has a very broad and variable range of strategies (Berkowitz & Bier, 2005). 
* Moral education, as the term suggests, is more focally a moral domain than is character education. Moral education focuses on the development of justice reasoning, and, insofar as it incorporates more recent work on relational morality, reasoning about interpersonal care. Character education, because it takes a very broad approach, often blurs the line between moral concepts and other non-moral but related concepts. Berkowitz (1997) attempted to delineate the latter as 'foundational characteristics', suggesting they are not specifically moral (e.g. perseverance, loyalty, courage) but serve to support moral agency. They derive their moral status, if they acquire any, from the moral or immoral ends to which they are applied (i.e. do you persevere to serve good or evil? Are you loyal to a moral or immoral cause?). More recently, Lickona and Davidson (2005) have offered a new model that identifies two sides of character (moral character and performance character), that approximate the distinction made by Berkowitz. However, Lickona and Davidson argue that they are equal in status, rather than one being derivative of the other, and need to be equivalent goals of character education. For many moral educators, the incorporation of some of these non-moral concepts is a salient reason for distancing themselves from the character education field, while for many character educators this inclusive duality connects them more centrally to the academic side of schools and the central mission of schooling (i.e. educating and developing the whole child). 
* Moral education comes from a liberal, social science tradition and character education comes predominantly from a classic, traditional and at times philosophical tradition. This is an overly simplistic dichotomy, however, for (1), as noted above, theory has a stronger place in moral education than in character education; (2) character education has become a very eclectic field and actually incorporates some of moral education; and (3) character education at times is a-theoretical and at times relies on varying combinations of virtue ethics and psychological behaviourism. Nonetheless, as a trend, this still remains accurate. Kohlberg (1981) wrote extensively about the liberal tradition upon which his work was based. The progressive education movement, and Dewey in particular, strongly influenced his applications to education. Moral education methods tend toward liberal, democratic forms (e.g. the Just Community Schools were radical experiments in democratizing schools, and moral dilemma discussions are decidedly empowering and democratic in nature). Traditional character education, on the other hand, has relied more on conservative and hierarchical methods (e.g. adult advocacy, direct teaching, presentations of inspirational cultural artefacts). Perhaps this is a reason why some important educational models based more on progressive roots (e.g. the Child Development Project and the Collaborative for Academic, Social and Emotional Learning) have avoided defining themselves as 'character education' programs (Watson et al., 1989; http://www.casel.org). 
* Moral education, in part because it has strong roots in social science, tends to have a much stronger empirical base and orientation. More recently, in part because of funding by and legislation from the USA federal government, the empirical base of character education has grown. It is worth noting, however, that when attempts are made to review this research (Beland, 2003; Berkowitz & Bier, 2005) they frequently incorporate moral education research. In fact, in their comprehensive review of the character education outcome research, Berkowitz and Bier report that the most commonly supported outcome of character education is socio-moral reasoning development. This is a direct product of the extensive research on Kohlbergian moral dilemma discussions. Furthermore, two of the 33 supported character education programs they identify are moral dilemma discussions and the Just Community Schools program, both moral development models. 
Citizenship and civic education 
Citizenship 
In one sense, citizenship is a legal category, identifying one's legal status, entitled rights and responsibilities, as a member of a society. In a deeper sense, however, it is much more than that. In democratic societies, fair procedures (the rule of law) and constitutional rights are designed to maintain equality among citizens but also to ensure the active role of citizens in contributing to society, partly through participation in democratic decision-making. Thus, a key aspect of democratic citizenship is the capacity to 'move beyond one's individual self-interest and to be committed to the well-being of some larger group of which one is a member' (Sherrod, Flanagan, & Youniss, 2002, p. 265), which includes the ability (and the motivation) to follow debates on current public policy issues and to participate in local and national politics. 'For the purpose of clarity, we use the term civic competence to refer to an understanding of how government functions, and the acquisition of behaviours that allow citizens to participate in government and permit individuals to meet, discuss, and collaborate to promote their interests within a framework of democratic principles' (Youniss et al., 2002, p.124). This conception is consistent with Dewey's view that democracy is not only a form of government (which would require educated voters) but also a mode of living together (which requires citizens prepared to solve differences in mutual deliberation in a respectful way and to engage responsibly in the common interest). Informed and effective citizenship in a democracy must be nurtured. There is a tension between a narrow definition of citizenship as a legal status, a middle definition of citizenship as knowledge of and skills for participation in the political sphere, and a broader definition of citizenship as a combination of knowledge about society, skills for participation in society and dispositions to engage constructively in public efforts to promote the common good.
Citizenship education 
For any society, the question must arise as to how to instil citizenship in each subsequent generation, however citizenship is defined. Here we enter the field of citizenship education. Not surprisingly, there is variation in what this endeavour is called (e.g. citizenship education, civic education, political socialization, democratic education). For the purposes of this discussion we will call it citizenship education, although we will make a distinction below between citizenship education and civic education. We will not attempt to give a full account of approaches to citizenship education in this article (see Parker, 2003, for one such attempt), but rather will highlight some controversial standpoints and suggest a possible consensus.
It is important to note that there is a common and often unreflective complex interplay between the outcomes of citizenship education (i.e. citizenship, differentially defined as knowledge, skills and dispositions) and the methods of citizenship education (e.g. direct instruction, direct experience with politics, indirect experience with public service, etc.).
On the conceptual level, there has been a longstanding tension in educational theory and research between approaches directed towards knowledge delivery and approaches directed toward practice, active participation and experiential learning as the means to competent citizenship.

</file>

<file= AmE06_J51>

Before delving into definitions of the basic elements of critical thinking, a brief overview of the structure of these elements will perhaps be helpful. The discus- sion offered in this book will focus primarily on the argument , which itself con- sists of at least two propositions . All arguments are either deductive or inductive , and an understanding of this distinction is required for criticism. The success of deductive arguments is evaluated in terms of what philosophers call validity and soundness , while inductive arguments are rated along a spectrum from weak to strong. Many of these words are familiar to most, but it is likely that they are associated with a number of different possible meanings. To remove these potential ambiguities, precise definitions will be introduced. The importance of a thorough familiarity with these concepts cannot be overstated.
Propositions 
The precise nature of propositions is a matter of some philosophical debate, but for present purposes, it will suffice to define a proposition as a claim or asser- tion that affirms or denies that something is the case . All propositions are either true or false, and no proposition can be both true and false. Furthermore, they are the only sort of thing that can properly be called true or false. Put simply, propositions are the sole bearers of truth and falsehood, and as will become clear shortly, this feature is of crucial importance for identifying them in ordinary language. Here are some examples of propositions.
All triangles have three sides. 
Either George W. Bush won the U.S. election, or John Kerry won it. 
People ought not to lie.
If today is Wednesday, then tomorrow is Thursday. 
All circles are squares.
The majority of propositions that one encounters come in the form of a declarative sentence, but it is important to note that a proposition is not identi- cal to the sentence that expresses it. A proposition is that to which a declarative sentence refers. For this reason, multiple sentences may express or refer to the same proposition.
George W. Bush won the U.S. election. 
The U.S. election was won by George W. Bush. 
George W. Bush was the winner of the U.S. election.
It is perhaps helpful to think of a declarative sentence as pointing to a kind of abstract object. It is this sort of object that philosophers have termed a proposition. 
Non-propositional language: exclamations, commands, and questions 
There are many uses of language that do not express propositions. As noted above, all propositions are either true or false, so whether a particular phrase or sentence can be considered to be true or false will determine if it indeed expresses a proposition. Consider the following.
What are we doing here?
To decide whether this question expresses a proposition, simply ask whether it makes sense to say that, what are we doing here? is either true or false. Clearly it does not, and just as clearly this sentence does not express a proposition. Another tactic is to add, It is the case that in front of the phrase in question and consider whether this new construction makes sense. If it does, then it is very likely that the phrase expresses a proposition. If it does not, then it is likely a non-propositional use of language. Consider.
It is the case that, 'what are we doing here'.
This is, of course, nonsensical. By contrast, consider the following propositions.
It is the case that all triangles have three sides.
It is the case that, either George W. Bush won the U.S. election, or John Kerry won it.
It is the case that, people ought not to lie.
It is the case that, if today is Wednesday, then tomorrow is Thursday.
The concern here is not whether the constructed sentence is true or false but whether it makes sense. Consider a false proposition.
It is the case that, all circles are squares.
Obviously this claim is false, but it does make sense, and thus the phrase all circles are squares expresses a proposition. Other non-propositional uses of language include commands and exclamations. Go to bed. Oh, no! A quick consideration of each of these constructions reveals that they are neither true nor false and thus do not express propositions. Note that the following do not make sense. It is the case that, go to bed. It is the case that, oh, no! This provides the basis for a general rule. Questions, commands, and exclama- tions do not express propositions . They are non-propositional uses of language. Questions of interpretation: rhetorical questions as assertions and commands as normative claims It should not come as a surprise that there are exceptions to this rule. In some cases certain questions or commands can be interpreted as actually expressing propositional language. A rhetorical question is a question that is meant to have an obvious answer and can be interpreted and reformulated as a declarative sentence that expresses a proposition . Consider the meaning of the question who won the 2007 election?  in the following.
Indeed, there are many propositions for which this is the case. The third is also a difficult claim to prove. What kinds of evidence and information would be required to support it? Clearly, it would be extremely difficult to remove all doubt, but it might very well be possible to offer strong supporting evidence for or against it. Such evidence would likely include historical documents, eyewitness accounts of Trumans decision-making process, etc. Coming to at least a cursory understanding of the burden of proof that a pro- ponent of a claim must meet is absolutely fundamental to a good critical analysis. As the discussion moves from propositions to arguments, it will be very helpful to keep this in mind. For the sake of concision, propositions will often be represented as capital letters throughout this text. P is the default designation for a proposition, Q is a proposition that is not P, R is a proposition that is neither P nor Q, and so on. To express the negation of a proposition, a not will simply be placed in front of the letter. If P is the cat is on the mat; then Not P is simply the cat is not on the mat or it is not the case that the cat is on the mat. Keep in mind that the negation of a denial is a possibility. If P is people ought not to lie; then Not P would be it is not the case that people ought not to lie or simply people ought to lie. Finally, the negation of a particular proposition always bears the opposite truth value of that proposition, so if P is true, then Not Pis false and vice versa. 
Arguments and Inferences 
The English word argument has many meanings, and when most people hear
it, they think of a (perhaps heated) disagreement. It is very likely that they do not immediately think of philosophy or critical thinking. Perhaps they should however, because the argument, or rather a particular kind of argument, is the philosophers bread and butter. In philosophy, an argument is a group of two or more propositions that express an inference. An inference , in turn, is a mental
process of linking propositions by offering support to one proposition on the basis of one more other propositions . The conclusion of an argument is that single proposition which is supported by other propositions , and a premise is a proposition that provides a basis of support for the conclusion . Consider the following argument. 
P1   : If you look into the abyss, then the abyss will change you. 
P 2 : The abyss did not change you. 
C: Therefore, you did not look in to the abyss. 
In this example, P 1 and P 2 are premises, and C is the conclusion. All three propo- sitions taken together are the argument. The inference, however, does not lie here on the screen. It is a mental activity, and as such, it occurs in the mind. Indeed, the inference is an instance of the reasoning activity for which human beings are so famous. Recognizing the differences between good inferences, those in which the premises provide adequate support for the conclusion, and bad ones, those in which the premises are inadequate to this task, is the essence of critical thinking and the primary focus of logic.
Deductive and Inductive Reasoning 
A deductive argument is one in which it is claimed that the premises provide a guarantee of the truth of the conclusion. In a deductive argument, the premises are intended to provide support for the conclusion that is so certain that, if the premises are true, it would be impossible for the conclusion to be false. Because successful deductive arguments are those in which the conclusion is com- pletely guaranteed by the premises, the conclusion must be contained within the premises. The conclusion cannot go beyond what the premises implicitly assert. For this reason, deductive arguments are usually found in inferences that follow from definitions, mathematics and rules of formal logic. The following are examples of deductive arguments.
An inductive argument is an argument in which it is claimed that the premises provide reasons supporting the probable truth of the conclusion . In an inductive argument, the premises are intended only to be so strong that, if they are true, then it is unlikely that the conclusion is false. They can appeal to any consideration that might be thought relevant to the probability of the truth of the conclusion. Inductive arguments, therefore, can take very wide ranging forms, including arguments dealing with statistical data, generalisations from past experience, appeals to signs, evidence or authority, and causal relationships. Please note that while causal reasoning is a complex and important part of any robust discussion of induction, there is simply no way to do it justice within the scope of this book. 
The real difference between the deductive and inductive arguments comes from the sort of relation the author or expositor of the argument takes there to be between the premises and the conclusion. If the author of the argument believes that the truth of the premises definitely establishes the truth of the conclusion due to definition, logical entailment or mathematical necessity, then the argument is deductive . If the author of the argument does not think that the truth of the premises definitely establishes the truth of the conclusion, but nonetheless believes that their truth provides good reason to believe that the conclusion is true, then the argument is inductive . It is usually fairly easy to decide whether an argument is deductive or inductive, but because it requires insight into what the arguer is thinking, it can occasionally be difficult or controversial.
Form versus Content
Put simply, the form of an argument is its logical structure or the manner in which the premises offer support for the conclusion. Since the form describes the relationship between the premises and the conclusion, it cannot be true or false (remember that only propositions can be true or false). The content of an argument is the group of actual propositions that comprise the argument. It is with respect to content alone that one may consider truth and falsehood. Consider the following arguments. 
Notice what is similar and what is different about these four arguments. What is similar about these arguments is their form . What is different about these argu- ments is their content . In the last example, the actual propositions have been replaced with capital letters. This makes the point that it is possible to consider the form of argument without any reference to actual propositions or content. This is important because the form of an argument is the most direct expression of the inference. Generally speaking, the study of logic primarily concerns itself with evaluations of the form of arguments and the reasoning process that they involve.
Recall that a deductive argument is one in which it is claimed that the premises guarantee the conclusion with absolute certainty.

</file>

<file= AmE06_J52>

Philosophy as traditionally understood may be defined as the branch of intellec - tual inquiry that deals with the most general and fundamental questions about the nature of reality and human life insofar as those problems lie beyond the compe- tence of the special sciences to raise or resolve. Defined in this way, many people do not want there be such a thing as philosophy, or at any rate want as little of it as possible.   However, despite the concerted efforts of many movements in twentieth century philosophy, from logical positivism to post-modernism, this traditional conception of philosophy has proven remarkably resilient and philosophers continue to discuss the big questionswith ever less embarrassment as the mid-twentieth century slips farther below the horizon of living memory. Among the questions that have exercised philosophers of the last sixty years, that of the existence of God has been one of the most hotly contested. It is the debate over that question that will be subject of this book.
Philosophy of Religion is the branch of philosophy that deals with that ques- tion, as well as related questions concerning the nature of God, the meaning of religious language and the immortality of the soul, among others, insofar as these questions are amenable to discussion from the point of view of neutral, impartial rational inquiry. Unlike other &#8220;second-order&#8221; philosophical pursuits, however, the philosophy of religion does not take religion as such, i.e. actual religious belief and practice, as its subject-matter. Instead, philosophy of religion is con- centrated primarily on the metaphysical truth-claims implied or presupposed by religious belief and practice properly so-called and attempts to bring these to the surface, analyze their central concepts, clarify their implications and attempt to evaluate their truth or falsity from the rational point of view. Since most reli- gions make some attempt to do this from within the perspective of belief in the form of dogmatic theology, it is religion as theologically articulated, rather than as lived belief and practice, that is primarily of interest to philosophers of reli- gion, regardless of whether or not they are sympathetic to religion.
The religious tradition of the West, which for our purposes here includes the Middle East and Persia, is known as monotheism , which is centrally character- ized by the thesis, shared by Christians, Jews and Muslims, that there exists one supreme being, the personal and providential creator of the universe who reveals Himself through the prophets and whose mighty deeds are recorded in canoni- cal scriptures such as the Bible or the Holy Qu&#8217;ran. Although these religions diverge with regard to which prophets and scriptures they accept and differ with regard to their distinctive religious teachings, all of them trace themselves to the fundamental revelation of God to Abraham and claim to worship the same God. Furthermore, as theologically articulated, they largely agree in their overall metaphysical conception of God. It is this conception of God, known as theism , which is the primary object of analysis in the philosophy of religion and which is thought to express the fundamental metaphysical truth-claims common to all the Western monotheistic faiths. The theological consensus among monotheists goes deeper than this, however; the mainstream conception of the theistic God, shared by the major figures of all three traditions, is nowadays referred to as Classical Theism or Perfect Being theology.   According to this view, God is characterized as the &#8220;being a greater than which cannot be conceived&#8221;, i.e. as the most perfect being in prin- ciple, unsurpassable even by Himself. Reflecting to some extent an element of Platonic inheritance, God is thought of as a being so fully and completely real- ized in every respect as to be immutable, impassable and utterly self-sufficient in such a way as to require nothing outside of Himself in order to exist. At the same time, God is a personal being, possessing intellect and will and thus capable of rational agency. One way in which God has exercised this rational agency is in the creation of the observable universe from nothing pre-existent, whether this is conceived of as eternally existing matter or the very substance of God Himself. Rather, God created the world ex nihilo simply through willing that it should exist; further, given that the observable universe is created from nothing, it requires not only to be created but to be conserved by God at every moment at which it exists. Thus, despite being wholly independent of and transcendent to the observable universe He has created, God is immanent in presence and power to every point in space and time. In the context of the divine perfection, we must therefore attribute omnipotence, omniscience and perfect goodness to God, which attributes are reflecting in and testified to by what He has made. In addition, most Classical Theists include necessary existence as among the divine attributes, maintaining that God, being independent and self-sufficient, contains within Himself the cause, reason or explanation for His existence. In the context of the divine perfection, Classical Theists deny that God is subject either to coming-to-be or passing-away. Being incapable of ever having come into existence or of ever going out of existence, God thus possesses infinity and eternity. However, Classical Theists are divided over whether this eternity should be understood as existence for an infinite period of time in both directions or as constituted by a unique, non-temporal mode of existence outside of time. In addition, some modern exponents of necessary existence as one of the central divine attributes maintain that it ought to be understood as logically necessary existence and thus as entailing that Gods non-existence, properly understood, is logically impossible or self-contradictory. The classical ontological argument for Gods existence, to found in Anselm, Descartes and moderns such as Norman Malcolm and Alvin Plantinga, is an expression of this tendency, though not all theistic philosophers, not even all Classical Theists, endorse this idea.   Much work has been done to clarify and defend the Classical Theist concep- tion of God, as well as to evaluate it, especially in comparison to other, alterna- tive conceptions of God on offer in contemporary philosophy of religion.
We cannot go into all this here; however, we will focus on the central question of the philosophy of religion, namely, the question of Gods existence, from the philo- sophical point of view. For our purposes here, we may further divide this topic into two more specific issues. First, there is the question of whether or not there are any rationally compelling (or at least intellectually respectable) arguments, evidence or grounds for believing that God exists. Theistic philosophers of religion generally defend an affirmative answer to this question, whereas atheistic philosophers of religion critique those defences and conclude that there is no good reason for believing that God exists. Secondly, there is the problem of evil, which examines the main piece of evidence for positive atheism, i.e. the affirmative judgment that there is no God. Since the mid-twentieth century, the problem of evil has been one of the most written on philosophical topics in the English-speaking world. No account of contemporary philosophy of religion can fail to deal with this issue at least in broad outline.
Three Competing Paradigms in Contemporary Philosophy of Religion I will survey contemporary philosophy of religion in accordance with the histor ical thesis I advanced in my earlier, more comprehensive treatment of this mate- rial called Analytic Philosophy of Religion: Its History since 1955 .   According to that treatment, contemporary philosophy of religion is best characterized as reflecting the interaction between three overlapping but successive paradigms in the philosophy of religion called Deductivism, Inductivism and post-Deductivism respectively. While each of these approaches to the questions of Gods existence and problem of evil has adherents today, each also enjoyed a period in which it was the prime focus of attention within the discipline. Deductivism, which was initiated by the neo-Thomists and which drew the fire of the analytic atheists, was dominant in the crucial decade of the 1950s in which contemporary philosophy of religion emerged, emphasized the construction and evaluation of deductive proofs of Gods existence, and gave rise to the deductive Argument from Evil intended to prove Gods non-existence. In response to theistic critiques of the Argument from Evil, the discussion of Gods existence slowly shifted from the evaluation of deductive proofs for Gods existence to the question of whether or not Gods existence was reasonable to believe on the basis of our total evidence; this initiated the Inductivist paradigm that culminated in Richard Swinburnes Bayesian argument for the existence of God. Although what I call the post- Deductivist paradigm emerged about the same time as Inductivism, it did not enjoy its heyday until the 1990s, when Alvin Plantinga proposed his Reformed Epistemology; as the core of a new kind of;Christian philosophy.
Given the constraints of the Insights series, I will only be able to barely sum- marize the stages and movements that characterize contemporary philosophy of religion, though I hope to do enough to pique the readers interest in consult- ing the aforementioned text as well as the many fine books referenced here and listed in the bibliography. We will begin by looking at the historical background of contemporary philosophy of religion and the emergence of the Deductivist paradigm. 
Prior to the emergence of modern analytic philosophy, British and American philosophy was dominated by the tradition of German Idealist thought inspired by Hegel. This school conceived of the ultimate reality as an impersonal, self- developing absolute mind or spirit identical with but not exhausted by the observ- able universe. Given this conception of things, it is not surprising that thinkers in this school were largely opposed to traditional monotheism and rejected tra- ditional Christianity as anything more than a kind of mythical expression of the truth that finds its full and literal expression only in the categories of Hegelian metaphysics.   Although Christianity did manage to find defenders even within the Idealist school, what was then known as philosophy of religion was largely just the project of reinterpreting traditional religious ideas within the framework of Hegelian philosophy, where those ideas ceased to have anything like their traditional significance.
By the turn of the twentieth century, the inevitable reaction against Idealism and Hegelianism in particular had begun to set in. Under the broad-based banner of neo-Realism, philosophers in Europe, the United Kingdom and the United States began to urge a return to a less arcane philosophical perspective, one more amenable to common sense. One of the contending new realist posi- tions was the philosophical and theological system known as neo-Thomism; another, which grew out of British neo-Realism, gave rise to what became known as Analytic philosophy. It is in the confrontation of these two schools that contemporary philosophy of religion and its characteristic problems and issues emerged. 
Neo-Thomism was a movement that began in the Catholic Church and remained largely within its confines. Although a Thomistic revival was already well under - way by the early decades of the nineteenth century, the movement took its primary inspiration from two events that occurred later in the century. The First Vatican Council (1870-71) is remembered now primarily for its definition of the dogma of papal infallibility; in fact, however, that decree was only part of the realization of its more central aim, which was to re-affirm counter-Reformation Catholic theology in the context of the challenges of the modern world. Decrees were also issued against modernism, rationalism, semi-rationalism and evolutionary naturalism.   At the same time, fideism and traditionalism were also con - demned and among the positive decrees of the Council was the thesis that Gods existence was capable of rational proof independently of the claims of faith. In 1879, Pope Leo XIII issued his first encyclical, Aeterni Patris , in which he directed that all Catholic seminaries and universities should base their philosophy and theology curriculum on the teachings of St. Thomas Aquinas.   This encyclical had the effect of making Aquinas the official theologian of the Catholic Church and spawned the neo-Thomist movement that would dominate the Catholic philosophy and theology for the next eighty years.

</file>

<file= AmE06_J53>

I am going to try to make some progress toward a model of our sociality that
focuses on both interpersonal structures at a time and characteristic forms of
construction of our sociality over time. I begin with some ideas about shared
agency that I have defended elsewhere; I then turn to the dynamics of our
sociality.This leads me to a complex view of the relation between shared intention
and obligations of each to each not to opt out. While such obligations are not
essential to shared intention, they are a common and important aspect of standard
ways in which we construct our sociality over time in response to systematic
problems of stability and depth, problems that arise as we seek shared agency
and shared deliberation in the face of our different reasons and our different
judgments.
SHARED INTENTION AND SHARED VALUING.
Suppose that we are building a house together. Our interrelated activities are
structured and organized by relevant intentional structures.This is why it is true, in
the relevant sense, that we are building the house together. These intentional
structures normally involve something like a shared intention to build the house, as
well as related forms of shared valuing - in favor, for example, of earthquake
preparedness. Our shared intention and our related shared valuings help coordinate
and organize not only our interconnected activities, but also our associated
planning, bargaining, and shared deliberation.
What is shared intention? As I see it, in a basic case you and I share an
intention to J when, in a public context, we each intend that we J by way of each of
our intentions that we J and their meshing subplans, and the continued persistence
of our intentions involves a recognized interdependence.
The idea is to understand shared intention to act in two stages.We first appeal
to a background theory of the intentions of individuals - as I see it, this will be what
I have called the planning theory. The planning theory sees intentions as playing
basic roles in forms of planning central to our abilities to achieve complex goals
across time and interpersonally. It aims at understanding intention primarily in
terms of regularities and norms that are associated with these coordinating, organizing
roles. In the second stage we then apply to intentions so understood a theory
of intention sharing, a theory that appeals to certain characteristic contents and
interrelations among such intentions in a public context. Central ideas for this
theory of sharing will include ideas of intentions of an individual in favor of a joint
activity, intentions that subplans come to mesh, semantically interlocking intentions
(i.e., intentions that are about each other's roles in thought and action),
recognized interdependence of intentions, and public accessibility.The theory tries
to see characteristic forms of functioning of shared intentions in cross-temporal
and social organization of action and planning as following from the basic theory
of intention, together with appeal to the special contents, interrelations, and contexts
that characterize intention sharing.
It is important that this theory goes beyond a simple appeal to public accessibility,
or some version of common knowledge or the like, and appeals to semantic,
causal, epistemic, and explanatory interconnections across the relevant intentions
of the participants. This helps it explain forms of rational mutual responsiveness, over time, on the part of each to developments in the intentions of other participants.
So, for example, my intention that we J by way of your analogous intention
and meshing subplans imposes rational pressure on me, as time goes by, to fill in
my subplans in ways that fit with and support yours as you fill in your subplans;
and vice versa. This pressure derives from the basic rational pressure on me for
means-end coherent and consistent plans, given the ways in which your intentions
enter into the content of my intention.
Turn now to shared valuing - for example, our shared valuing, in the context
of our shared house building, of earthquake preparedness. My proposal here
has been that this is, in a basic case, a shared policy about relevant practical
reasoning - where policies are intentions that are appropriately general in their
contents. For us to value X is, in a basic case, for us to have a shared policy to treat
X as justifying in shared deliberation in which we intend to engage and by which
we intend to be guided.We say in what such a shared policy consists by extending
the account of shared intention to the case in which the content of the relevant
policy-like intentions of the participants is our treating X in this way. So, in a basic
case, you and I have a shared policy of treating X as justifying in relevant shared
deliberation when, in a public context, we each have a policy (i.e., a general
intention) that favors that treatment by us by way of meshing subplans, these
policies are interlocking, and their persistence is appropriately interdependent and
recognized as such.
Such shared valuing is different from, and has a complex relation to commonality
in value judgment.There are two general considerations that support this
point.The first concerns individual agency. Here the basic point is that even in the
case of an individual agent there is a distinction between judging something good
or valuable, and valuing it. As Gilbert Harman notes, I might judge listening to
Mozart to be valuable but not value it. Further, if I do value X the significance or
weight that my valuing accords to X may be quite different from what I judge to be
its value. Finally, I may reasonably hold my value judgments to certain constraints
of intersubjective rational convergence. Given my recognition of thoughtful and
rational divergence across different evaluators, this may block certain value judgments
on my part. Nevertheless, in the course of living my own life, I may value
certain things in certain ways while recognizing the absence of interpersonal rational
convergence on this. So my valuings go beyond my value judgments.
What conception of the valuings of an individual best coheres with these
observations? This is a large issue. Here let me just say that, as I see it, for an
individual to value X is, at least in a basic case, for that individual to have a policy
(i.e., an intention that is appropriately general) of giving X relevant weight, or
other kinds of justifying significance, in her motivationally effective practical reasoning.
And the present point is that such a policy is not the same as, and is related
in complex ways to, a judgment that X is good or valuable.
The second promised consideration concerns the wedge between shared
valuing and shared value judgments that is introduced by the structure of sharing.
Of course, given potential divergences between the valuings and the value judgments
of the individual participants, we already have reason to expect that shared
valuings may not involve commonality of value judgment. But there are also
further pressures for such divergences once we turn to phenomena of sharing.You
and I may each judge that earthquake preparedness is a good thing, and we may
each personally value earthquake preparedness, and yet we may not participate in
a shared valuing of earthquake preparedness with respect to the shared deliberation
associated with our house-building activities. It is one thing for each of us to
think earthquake preparedness a good, another for us to be committed to giving it
weight in these shared deliberations. Further, even if we both judge that earthquake
preparedness is a good thing and we both participate in a shared valuing of
earthquake preparedness, the weight given to it in our shared valuing may diverge
significantly, in either direction, from the extent to which each of us judges it good.
Finally, we might have a shared policy of giving significant weight to earthquake
preparedness in our shared deliberations even if I believe there is no significant
earthquake danger and so no significant value to earthquake preparedness in
house building, and even if I do not myself value earthquake preparedness in my
own personal deliberations. I may still participate in our shared valuing because
earthquake preparedness matters so much to you, and because it is part of a
package of shared commitments that makes overall sense to me, as well as to you.
Our sociality is, in this way, a package deal.
There is a general point that lies behind this distinction between shared
valuing and commonality of value judgment. Shared valuing involves a relativity to
certain shared contexts, especially contexts of shared deliberation. Commitments
to participate in giving weight to X in such contexts are context-relative. But
judgments about the value of X are not context-relative in this way. So we have interrelated models of shared intention and shared valuing,
models that exploit the two-stage strategy noted earlier. I now want to emphasize
a basic feature of the underlying account of sharing: Such sharing does not require
commonality in each agent's reasons for participating in the sharing.You and I can
have a shared intention to garden together even though I participate because I
need the exercise and you participate because you need the food.Though we have
different background reasons for which we participate, our shared intention nevertheless
establishes a common framework. Again, suppose we are on a college
admissions committee and we have a shared policy of giving weight to legacy
considerations in relevant shared deliberation. In this sense we value such considerations.
Nevertheless, I might participate in this because I think giving such
weight in admissions decisions is an effective fund-raising tool; whereas you participate
in it because you think our institution has made an implicit promise to its
alumni to provide this benefit to their children.We participate for different reasons,
but our shared valuing nevertheless establishes a common framework.
I think it is a significant virtue of this theory of sharing that it helps us model
such partial social unity and commonality of framework in the face of different
background reasons for participation. After all, much of our sociality is partial in
this way. Nevertheless, it is also true that when our shared intention or shared
valuing is grounded in different reasons the absence of certain forms of social unity
can pose important practical problems. In particular, there can be problems of
stability and problems of depth.The potential for these problems might be seen as
an objection to the model of sharing. But I believe, in contrast, that it is a virtue of
the model that it helps us articulate both these problems and also standard ways in
which a group can, over time, reasonably respond. Or so I will argue.
PROBLEMS OF STABILITY AND OF DEPTH.
Begin with instability.
There is a potential for instability of the shared intention or
shared valuing if the different reasons for which participants engage in the sharing
were to come to point in different directions: Just consider what is likely to happen
to our joint gardening if you were no longer to need this source of food even
though I continue to need the exercise. Of course, there will many times be, in the
context of the shared activity, various kinds of salient consequences associated with
sticking with one's part in the shared activity. Issues about reputation effects and
the like belong here. But these are contingent matters.
That there are these possibilities of instability, given the model, might be seen
as a ground for rejecting the model as insufficiently rich. After all, it may seem that
sharing brings with it a further bond between the parties, a bond that goes beyond
the interlocking and interdependence I have highlighted, and a bond that supports
stability in a way that we have not yet captured.
Our model of sharing does have further resources here, resources grounded
in the planning theory that is in the background. Central to the planning theory is
the idea that there are norms for stability of intention. To some extent, these are
norms of nonreconsideration of prior intentions; to some extent these are norms
that give prior intentions a (defeasible) default status even in the face of reconsideration.

</file>

<file= AmE06_J54>

CHAPTER 1.
Uniting the kingdom.
Three Acts of Union - from the sixteenth to the very start of the nineteenth century - cemented the legal, political and economic relationships between dominant England and the 
so-called Celtic fringe of Wales, Scotland and Ireland. These relationships, which by the nineteenth century saw all these regions directly ruled from the Westminster parliament, are often dubbed 'internal colonialism'. Bringing Wales, Scotland and Ireland within a broader British realm represents some of England's earliest forays into colonial rule, for though the formal statutes linking these countries take us through to the nineteenth century, English interest in and often coercion of these neighbouring regions has a very much longer history. The first of these lands to come directly under English control was Wales, brought formally into the English fold by the 1536 Act of Union that created 27 Welsh parliamentary constituencies. For at least a hundred years prior to this, conflicts between the Welsh and the English were common and, in the borderlands between the two especially, the English imposed discriminatory regulations and practices on the Welsh as they gradually gained the upper hand. The Welsh lacked many of the rights that the English enjoyed, a pattern of inequality and prejudice that would only grow after formal annexation. Scotland's associations with England were more complicated, and its annexation more drawn out. It was under Stuart rule in 1603 that the union of the Scottish and English crowns was formalized, with the seat of government firmly located in England. Little more than a hundred years later, in 1707, another Act of Union robbed Scotland of its own parliament: 45 seats in the Westminster House of Commons and 16 in the House of Lords were established to represent the new Scottish constituencies.
Unlike Wales, however, and largely because it had been an identifiable sovereign state before unification, Scotland maintained even after union with England its own judicial system, its own national church (Presbyterian, unlike England's Episcopal Church of England) and a separate education system. And unlike the 1536 Act that had yoked Wales to England, the 1707 Act was a product of negotiation and not brute force. Scotland was in a position to sever its ties with England and the purpose of the 1707 legislation was to prevent that. The Act thus gave the Scots considerably more latitude than the 1536 Act had allowed the Welsh. Indeed, outside the wealthier segments of society, it probably made little difference to most Scottish men and women. As we shall see, the Jacobite uprisings in the eighteenth century would change that, but before then the Union was reasonably harmonious and not wholly disruptive for Scotland. The same could not be said for the Act of Union with Ireland, which took effect on the first day of the new century, January 1801. A number of critical political factors prompted this union, largely related to Britain's vulnerability, perceived or real, to foreign invasion. Ireland's incorporation came at a time when Britain was almost constantly at war with the Catholic power of France, and sometimes with Catholic Spain, and shortly after a popular uprising in Ireland had been subdued. Ireland was, of course, a predominantly Catholic land. Wolfe Tone's 1798 Irish rebellion was a violent one, with almost 30,000 fatalities. The fact that the rebellion had been aided by the French (despite the fact that Tone was himself a Protestant) fuelled English fears, especially during the threatening years of Napoleon's rule when Britain's political and military strengths were severely tested. With a far larger population than either Scotland or Wales, the 1800 Act granted Ireland 100 seats in the House of Commons and 32 in the House of Lords to represent its 5 million inhabitants. As with Scotland, this representation was premised on the dissolution of a separate Irish parliament, and though that all-Protestant body had long been under the English thumb, the symbolism of its abandonment was nonetheless potent in shaping the future of Irish politics. English intrusions into Ireland date back to the twelfth century, and the Irish had experienced considerable erosion of their liberties over the years; by the sixteenth century England was actively engaged in a political, economic and religious subjugation of this neighbouring island. The dates are anything but coincidental, for the bloodshed over religion so characteristic of the Tudor years in England had deep and dangerous consequences for Catholic Ireland. By the late sixteenth century, there was some urgency to the policy of de-Catholicizing Ireland through migration and plantation, as England became definitively Anglican. A greater and greater Protestant presence was imposed from the 1560s and for the next hundred years, with a corresponding dispossession and pauperization of the Catholic Irish. When the Act of Union was passed in 1800 one promise upon which the old Irish parliament insisted before they would dissolve was that Catholics be allowed to vote and to hold public office. It was a promise that would remain unfulfilled for a further three decades. Early in 1801, it was his inability to keep that controversial promise that prompted the prime minister, William Pitt, to resign his office. After 1801, the kingdom 'now consisting of Scotland, Wales, Ireland and England' was governed solely and as one from London. The state was more deeply centralized than ever before; the term Great Britain, already in use, was now the official designation of the nation. Some historians have argued that such centralization was effective by 1640. This might be so in an informal sense, but we can certainly point to 1801, the dawn of the nineteenth century, as a time when such centralizing tendencies were fully and formally in place and when internal colonialism was complete. Although Scotland retained a number of important local institutions, Britain had one parliament and a state religion secured by a Protestant succession to the monarchy. It was fear of destabilizing this Protestant sovereignty that prompted much of Britain's internal colonialism. Ireland as a predominantly Catholic region was the principal flashpoint for these religious debates within imperial policy, but religion was politically significant in Wales and Scotland too. In Scotland, Catholicism had largely given way by 1690 to a Calvinist-inspired Presbyterianism, under the influence of John Knox. It was distinctively different in character from English Episcopalian Christianity. The influence of Presbyterians among Protestant Irish groups, mostly centred in northern Ireland, was of considerable concern to the English authorities in the eighteenth century. Presbyterianism in Scotland was acceptable, but its presence in Ireland added a complicating addi- tional layer of religious discontent and dissent that could only add to England's problems there; the Protestant Irish strand was the origin of the Orange Movement, which quickly became deeply involved in political protests in Ireland. Meanwhile, in Wales the insistence by the Anglican Church that services be conducted solely in English secured sturdy support for non-Anglican forms of Protestantism. The successful spread especially of Methodism and of Baptism owed much to the fact that they conducted services in the local language. Likewise the translation of the Bible into Welsh, effected during the reign of Elizabeth I, ensured that virtually every household in Wales would continue to be exposed to the Welsh language even at a time when that language was under attack. Welsh remained alive in large part because of the actions of a monarch determined to impose Anglican conformity. Non-Anglican Protestants and Catholics throughout the kingdom were barred from public office, local or national, by the Corporation Act of 1661 and the Test Acts of 1673 and 1678. These laws were part of the legal and political mechanism designed to protect a Protestant succession to the throne and to resist reinstatement of the Catholic Stuart dynasty. They also reflected a popular and growing sentiment that parliament 'although hardly a representative institution at this time' was a specifically Protestant expression of liberty, what historian Linda Colley has called the 'Protestant inheritance'.  This celebration of Protestantism gave voice to a profound and long-standing anti-Catholicism in England, bolstered throughout the eighteenth century by the animosity between England and its most significant imperial and commercial rival, France. Ironically, after the defeat of France in the Seven Years War in 1763, Britain's new colonial acquisitions made the Empire a far less identifiably Protestant one than it had formerly been. Catholics and non-Christians figured largely among those now subject to British imperial rule. National stability and success depended on more, of course, than merely a continued adherence to Protestant Christianity. The needs of merchants and of bankers were vital, and the mercantilist economic principles of England in the seventeenth and eighteenth centuries meant that the state took a close interest in trade and commerce, actively regulating economic life as a way to pay the expenses of government. In the early eighteenth century, for example, rarely a year passed in which a new law designed to regulate colonial trade in some fashion, or to control customs revenue (and its enemy, piracy), did not come before parliament. England kept a tight rein on colonial trade. Colonial goods were governed by strict regulations that gave a strong advantage to the English economy. Scots merchants and traders gained access to these lucrative colonial markets as a condition of the 1707 Act of Union. This meant that they were no longer required to route goods through England and pay duties on them, but could trade directly with other colonies within the British Empire. For Ireland, the wait was longer. An Act of 1696 had ruled that goods from the American plantations could not be landed in Ireland, a law that hindered the logical trade that might otherwise have flourished between America and Ireland, given their geographical proximity. Though this restriction was eliminated in 1731, Irish trade continued throughout the eighteenth century to be primarily with England, a situation that gave England as the dominant trading partner a considerable advantage. Wales, like the often neglected Celtic region of Cornwall on England's south-west tip, was the site of active smuggling, both across the English-Welsh border and at the ports. The loss of government revenue that efficient smuggling represented made the control of contraband and of piracy a major component in England's desire to fold these 'peripheral' areas into the polity. The prospect of economic order in border areas - meaning a crackdown on smuggling and an organized customs agency pulling in significant revenue - was a key motive for internal colonialism. This interest in potential revenues from the Celtic periphery was seldom balanced by a corresponding commitment to investment in those regions. The Irish plantation schemes of the sixteenth and seventeenth centuries were more concerned with peopling the country with loyal Protestants than in giving economic aid to, or partnering with, the Irish. Centralization, as it would do at a later stage of empire and in more distant lands, often worked to the advantage of the dominant and not the peripheral power. In the arena of banking, for example, London became more and more powerful at the expense of provincial centres, and the establishment of the Bank of England in London in 1694 was some indication of the power lines in the fiscal world. Whereas Scottish banks remained robust until the banking reorganization of the 1840s, those in Wales and Ireland simply could not compete with their English rivals. Economic competitiveness was crucial to political survival in this era, and here again it was English practices and activities that came to dominate, particularly from the eighteenth century onwards. The landholding practices common in Celtic cultures were often incompatible with English ideas about inheritance, wealth creation and economic efficiency. The dominance of gavelkind , in which land was shared by all male heirs, contrasted with the typical practice of primogeniture in England, which concentrated wealth in eldest sons. In an era when the English economy was focusing increasingly on consolidating large tracts of land for more profitable and efficient cash-crop farming, the tendency in gavelkind for land plots to shrink to accommodate every son was regarded as a backward-looking and inefficient system.

</file>

<file= AmE06_J55>

Women are taking the lead and making a huge contribution to defining the international agenda in terms of human rights, macroeconomics, conflict/peace, and sustainable development. We have a valuable and unique perspective on these issues as women and as human beings. We recognize that feminism in one country is not sustainable-we need feminism on a global scale. [Women in Development Europe, 1995]
Central to women's experiences-and to analyses informed by socialist-feminism or feminist political economy-has been the sexual division of labor, the separation of societies into public and private spheres, the organization of economies into the spheres of production and reproduction, and the ways that these have articulated with gender and class, as well as with ethnicity and race.
Feminist analyses, and women's activism, have been concerned with the broad implications of the identification of women with the private sphere of the family, including their exclusion from the public sphere, their varied experiences of marginalization, exploitation, and integration in the sphere of production, and their undervalued location within the sphere of reproduction, whether as
paid or unpaid labor.1 Although these experiences and analyses have preoccupied women and feminists throughout the world, it may be said that prior to the mid-1980s the world's women had not yet developed a collective identity, a collective sense of injustice, or common forms of organizing. The year 1985 was, in many ways, a watershed. The third United Nations world conference
on women, which took place in Nairobi, Kenya, and consisted of both an intergovernmental conference and a forum of nongovernmental organizations, brought together women from across the globe who shared their experiences with and criticisms of new economic policies, conservative governments, and cultural-political movements that they deemed inimical to women's interests.
Although the term globalization would not be used until years later, the emerging global state of affairs proved to be an impetus for novel forms of women's organizing and mobilizing.
This book examines transnational feminist networks (TFNs), their relationship to the globalization process, the ways that they engage with public policy at the national and international levels, and their organizational dynamics.
The study is situated at the nexus of three bodies of literature and is intended as
a contribution to all three. The first is the now extensive literature on globalization, which analyzes the complex and multidimensional processes that are said to have begun in the late twentieth century. This literature addresses globalization from different disciplinary, conceptual, and political vantage points, but it rarely investigates the gender dimension of globalization. Much of the literature emphasizes globalization as an economic process; some studies examine globalization in terms of the changing nature of the post-Cold War world order; other studies emphasize the cultural aspects and effects of globalization.2 The literature does not, however, consider globalization as a gendered process.
A related body of studies has appeared on the growth of nongovernmental organizations, civil society and citizenship, global civil society, transnational advocacy networks, and global social movements. This, too, is a rich literature, but attention to the women's movement or to women's organizations is limited to only a few studies. For example, the well-known volume by Keck and Sikkink examined transnational advocacy networks (TANs) organized around human rights, the environment, and violence against women. O'Brien, Goetz, Scholte, and Williams studied the ways in which global social movements-women's movements, unions, and environmental organizations-have engaged with multilateral economic institutions such as the World Bank, the International
Monetary Fund, and the World Trade Organization. And the collection by Cohen and Rai on global social movements includes essays on feminist networking for conflict resolution and the advancement of women's human rights.4
A third literature focuses precisely on women's movements and women's organizations.5 Much feminist theorizing of women's movements has focused on national-level factors such as the growth of the population of educated women with grievances about their second-class citizenship. More recent studies have begun to connect women's movements and organizations to international or global processes such as the role of international organizations
or the United Nations Decade on Women, and they examine the ways that women's organizations have engaged with the world of public policy. For the most part, however, this literature has not explained the worldwide social movement of women in terms of globalization processes such as the feminization of labor, growing social inequalities, and increased access to the new information and computer technologies (ICTs) by educated and politically active women-which is the argument I make in this book. At the same time, we have seen increasing numbers of feminist studies on globalization and gender, some of which draw on postmodernist and postcolonialist thought and some
of which may be described as Marxist-feminist or as constituting a materialist, structuralist, or feminist political economy approach.
This book offers an analysis of globalization as a multidimensional and gendered process (the subject of chapter 2) and of the transnational women's movement as a product of sociodemographic, economic, political, and cultural changes (discussed in chapters 3 and 4). I also describe the research, advocacy, and lobbying efforts of TFNs, their methods of organizing global networks of women, and the ways that they engage with states, international organizations, and global civil society to influence public policy and raise international awareness about women and gender issues (chapters 5, 6, and 7). In discussing Islamic fundamentalism and neoliberal capitalism as two
key features of globalization that have resulted in transnational feminist responses, I bring together topics that are usually discussed separately. Conceptually, I draw on world-systems, Marxist, and feminist frameworks to analyze contemporary globalization processes and explain the emergence of a transnational women's movement in the late twentieth century. My explanation
incorporates the concepts of gender, class, capital, and the state, all of which
operate within a hierarchical world-system divided into core, periphery, and semiperiphery.6 This book illuminates the links between globalization, inequalities, and women's movements and brings neglected feminist research and activism into clearer view.
The capitalist world-system has often produced antisystemic movements that cross borders and boundaries, while national-level class conflicts and political contradictions have similarly generated forms of collective action and social protest, including social movements and revolutions. But a key characteristic of the era of late capitalism, or globalization, is the proliferation of transnational social movements, including the transnational women's movement and its organizational expression, the transnational feminist network.
What exactly are transnational feminist networks? They are structures organized above the national level that unite women from three or more countries around a common agenda, such as women's human rights, reproductive health and rights, violence against women, peace and antimilitarism, or feminist economics. They are part of the family of political change organizations
operating above and across national borders that have been variously described as global civil society organizations, transnational advocacy networks, and transnational social movement organizations-and which, along with international nongovernmental organizations, constitute the making of a transnational public sphere. As three scholars of social movements have noted:
"Globalization has in fact brought social movements together across borders in a 'transnational public sphere,' a real as well as conceptual space in which
movement organizations interact, contest each other, and learn from each other. Transnational feminist networks work with each other and with transnational human rights, labor, social justice, and environmental organizations to draw attention to the negative aspects of globalization, to try to influence policy-making, and to insert a feminist perspective in transnational advocacy and
activism. Since 1985, when a number of now well-known TFNs emerged, feminist networks have proliferated around broad agendas or specific issues and
campaigns. The focus in this book is on those TFNs that address themselves primarily to women's human rights and to economic policy. In particular, I examine three feminist networks that formed in opposition to structural adjustment and neoliberal economic policy, developing a feminist critique and an alternative feminist economics framework (chapter 5); and I examine three
feminist networks that promote women's human rights, especially in Muslim countries where fundamentalism emerged and the legal status of women became compromised (chapters 6 and 7).
Why the focus on feminist economics and the human rights of women in
the Muslim world? I can offer three reasons. First, some of the best-known and well-organized transnational feminist networks are precisely those orientedprincipally toward economic policy (notably Development Alternatives with
Women for a New Era, or DAWN) and the human rights of women in the Muslim world (in particular, the network Women Living under Muslim Laws, or
WLUML). Second, I argue that neoliberal capitalism and Islamic fundamentalism are among the defining features and consequences of global restructuring and globalization in the late twentieth century; hence it is logical to examine those feminist networks that focus on these issues. It is noteworthy that the Gloria Declaration-issued by the Eighth International Women and Health
Meeting in Rio de Janeiro in March 1997-identified two major systemic obstacles to achieving women's health and rights: globalization of the market economy and religious fundamentalism. That declaration emphasized Christian and especially Catholic restrictions on women's sexual autonomy, as well as Islamic fundamentalism, and criticized the role of the Vatican at venues such as
the 1994 International Conference on Population and Development and the
1995 Fourth World Conference on Women. Third, I decided to give prominence to the feminist critique of neoliberal economic policy because I felt it important to correct a certain misunderstanding concerning the women's movement, which is that feminists are either concerned exclusively with personal and reproductive rights or oriented primarily around violence-against women issues. My decision to include three TFNs that engage with economics is meant to show that global feminism is taking on some of the major policies and institutions of our time-including neoliberal capitalism, the World Bank, the International Monetary Fund (IMF), and the World Trade Organization (WTO)-and inserting a distinctly feminist perspective in global discussions about economic justice.
Female Labor and Women's Mobilizations in an Era of Globalization Why did the transnational feminist networks emerge in the mid-1980s and not earlier? What was the context?
The women's movement of the second wave, which began in North America and Europe in the 1960s, consisted of feminist groups that emerged within national borders and addressed themselves to their own nation-states, governments, employers, and male colleagues and kin. As women's groups expanded internationally, they remained primarily nationally based and nationally oriented. Feminist groups encompassed liberal, radical, Marxist, and socialist ideologies, and these political differences constituted one form of division within feminism. Another division took the form of North-South, or First WorldThird World differences in terms of priority feminist issues; many First World feminists saw legal equality and reproductive rights as key feminist demands
and goals, while many Third World feminists emphasized underdevelopment, colonialism, and imperialism as obstacles to women's advancement. The Cold War also cast a shadow on feminist solidarity, in the form of the East-West divide. Disagreements over what constituted priority feminist issues came to the fore at the beginning of the United Nations' Decade for Women, and especially at its first and second world conferences on women, which took place in
Mexico City in 1975 and in Copenhagen in 1980, respectively. The disagreements at the Mexico City and Copenhagen conferences pitted women activists from the North and from the South, and revolved around prioritizing equality and sexuality issues versus economic and political issues. A shift in the nature and orientation of international feminism began to take place in the mid-1980s, during preparations for the third UN world conference on women, which was held in Nairobi, Kenya, in 1985. The shift took the form of bridge building and consensus making across regional and ideological divides, and the emergence of a women's organization of a new type.
What enabled this important change in the women's movement to take place
were three critical economic and political developments within states and regions, and at the level of the world-system: the transition from Keynesian to neoliberal economics, along with a new international division of labor that relied heavily on (cheap) female labor; the decline of the welfarist, developmentalist state; and the emergence of various forms of fundamentalist movements. These developments led to new thinking and new forms of organizing on the part of activist women in both developing and developed countries.

</file>

<file= AmE06_J56>

PART ONE.
Contesting Vietnam.
CHAPTER I.
Visions of Indochina and the World.
The Second World War ended one epoch of Vietnamese history and
launched another. For half a century France had dominated the territory
collectively known as Indochina-the Vietnamese provinces of
Cochinchina, Annam, and Tonkin, plus neighboring Cambodia and
Laos. Vietnamese nationalists had periodically challenged colonial rule,
but French authorities had squelched demands for change in every case,
reaffirming their nation's supremacy through a mix of armed repression, economic subjugation, and cultural domination that had characterized colonial rule in Indochina since its beginnings in the nineteenth century. Nationalist agitation aggravated the colonial administration
but did not threaten its control. Nor was French rule much challenged
from beyond Indochina's borders. The Western powers, respectful of
French claims and excluded by formidable trade barriers, took little
account of the area. China, for centuries the main Asian aspirant to
power in the region, remained too weak and divided during the decades
of French rule to interfere. For its part, Japan steadily emerged as a
regional power during the early twentieth century but posed no direct
threat to European dominance in Southeast Asia until 1940.
The events of that year changed everything. In May Nazi armies
launched a crushing attack against France and, a month later, forced the
French government into a humiliating armistice that shattered the country's pretensions as a global power. The defeat had immediate consequences in the Far East, where the Japanese government eagerly exploited French weakness. Seeking to bolster its war effort against
China, Tokyo demanded that the colonial administration close the
Chinese-Tonkin border, thus sealing off an important channel of Western supplies for the beleaguered Chinese army. Severed from Paris,hopeless of challenging Japanese military power, and doubtful of receiving support from Western nations, French governor-general Georges
Catroux capitulated. Over the next eighteen months, the trickle of
French concessions grew to a flood as Catroux's Vichy-appointed successor, Admiral Jean Decoux, struggled to appease Tokyo and avert an
outright Japanese takeover. First Japan demanded airfields in northern
Indochina and the right to transport troops across Tonkin to fight in
China. Then in summer 1941, as the Japanese military prepared to
attack southward toward the Dutch East Indies and Singapore, Tokyo
insisted on establishing bases in southern Indochina. By the time of
Japan's assault on Pearl Harbor, the French administration had become
a virtual accessory to the Japanese war effort.
In a sense, Catroux's strategy was successful. In return for French
cooperation Tokyo permitted the colonial administration to remain in
place at a time when Japanese forces were uprooting Western regimes in
Malaya, Singapore, Burma, the Philippines, and the East Indies. Even in
its hour of humiliation France nominally remained master of its Southeast Asian empire, a privilege that the colonial administration struggled
to protect over the following years. But this arrangement-really an
expedient that served Tokyo's interests by relieving it of administrative
and military burdens-could not mask the underlying reality of French
impotence.
To Vietnamese nationalists the crumbling of French power signaled
an unprecedented opportunity to overturn the colonial order. They
understood that their goal would not be realized quickly. There
remained not only the matter of ending what was left of French control
but also that of evicting the Japanese. Still, the rapidly changing international situation generated hope. Amid surging optimism Ho Chi
Minh and other Vietnamese nationalist leaders gathered in a damp
mountainside cave near the Chinese border in northern Tonkin to begin
laying the groundwork for revolution. The delegates agreed to submerge
clashing agendas within a broad patriotic coalition-the Viet Nam Doc
Lap Dong Minh Hoi, or Viet Minh-and chose guerrilla warfare as the
principal method of struggle against foreign occupiers. With those
matters settled, Ho Chi Minh appealed to Vietnamese patriots to join
the fight. "The hour has struck!" he proclaimed. "Raise aloft the
insurrectionary banner and guide the people throughout the country to
overthrow the Japanese and the French!"
The Viet Minh's unalloyed enthusiasm contrasted sharply with the
deep anxiety that prevailed among Western policymakers concerned
with Indochina's future. Events that inspired optimism and boldness in
the Tonkin mountains generated fear and discord among Free French,
British, and American officials charged with settling Indochina's destiny
following Japan's defeat. Above all, the evisceration of French control
raised the vexing question of how-and, at least in Washington,
whether-colonial rule should be reestablished. In the early months of
the Pacific war, Allied deliberations amounted to little more than academic exercises since Japan totally dominated the region. As the tide of
war turned toward the allies in 1943 and 1944, however, Indochina's
postwar status became a steadily more pressing issue. Small clusters of
French, British, and U.S. policymakers focused on a full slate of questions: Who should govern Indochina after the war? How should the
aspirations of local nationalists be taken into account? What would be
Indochina's place in the postwar international order? In attempting to
answer these questions, the three Western governments laid down patterns of thinking and debate that would underlie policymaking for the
next half decade.
FREE FRANCE AND THE RECOVERY OF INDOCHINA.
As Allied victory grew more certain in 1944, the Free French organization under General Charles de Gaulle became increasingly anxious
about Indochina. To be sure, the matter ranked below the most pressing
national concerns-the reestablishment of the French state, economic
rehabilitation, and the war against Germany. Consumed by these challenges, ordinary citizens, the Free French media, and the renascent political parties showed little interest in the fate of a territory on the other
side of the world. For the small leadership group concerned with recovering France's traditional role as a global power, however, the issue did
not lag far behind the nation's top priorities. These men-bureaucrats,
diplomats, politicians, and military officers-shared a conviction that
their country's long-term prospects rested on its ability to preserve the
empire, not least Indochina. Francois de Langlade, a one-time rubber
planter who became one of de Gaulle's chief delegates for Indochinese
affairs, succinctly stated the group's thinking in early 1945. "Without
Indochina," he wrote, "France is no longer a world power."
The equation of French grandeur with imperial prowess ran deep in
French history. Since the early nineteenth century, French colonial ambition had swelled during times of national crisis as leaders strove to compensate for setbacks in Europe with victories overseas. It was no
accident that the most active phase of French conquest followed the
humiliations of the Franco-Prussian War in I870-1871 or that French
determination to maintain the empire soared during the Second World
War. Indeed, the necessity of preserving the empire was as plain to Vichy
officials as to Gaullists. Both regimes struggled desperately to preserve a
glimmer of French independence following the debacle of 1940. "The
colonial peoples represented the best reason for France to believe and to
hope," the Vichy colonial minister, Admiral Charles Platon, asserted in
fall 1940. De Gaulle, employing remarkably similar language, proclaimed the empire "a strong ray of hope," offering France "trump
cards in the game where its destiny will be decided."s For both regimes,
Indochina held special significance. Although less directly tied to French
national identity than Algeria and perhaps other African possessions, it
surpassed any other imperial holding in conferring great power status
on France. The farthest-flung of major French territories, it rivaled
India, the jewel in Britain's imperial crown, and entitled France to a
major role in Far Eastern affairs.
French determination to hold Indochina also stemmed from a pervasive, if less explicitly stated, belief in the territories' economic value. To
be sure, Indochina had not yielded the returns that French colonial
enthusiasts had hoped for over the years. As one U.S. study remarked,
French fiscal, tariff, and wage policies since the late nineteenth century
had been "unfortunate" at best, failing to generate a healthy colonial
economy or to bring many benefits to the metropole. A small group of
white planters and the Banque de l'Indochine reaped most of the rewards,
while French consumers paid high prices for colonial goods. So questionable was Indochina's economic value that one U.S. study suggested
in 1944 that France might simply be better off without it. Still, the
French economy had become dependent on Indochinese exports, especially rice, tin, and rubber, before the war, making the territory the
second most important source of French imports (after Algeria) within
the empire. In addition, Indochina represented a source of food and
labor for other French possessions and served as a hub of the French
colonial economy in the Far East and Pacific. All in all, Free French
leaders had reason to believe that Indochina, however weak its prewar
performance, could play a valuable role in metropolitan recovery.
While officials agreed on the need to recover Indochina, they differed
over precisely how French rule should be reconstituted after the war. A
conference of colonial administrators in Brazzaville, the capital of
French Equatorial Africa, in January 1944 revealed a wide range of
views about the possibility of postwar colonial reforms. Formally, the
Brazzaville meeting dealt only with Africa. But discussions reflected
thinking about the empire in general-its structure and the degree of selfrule that should be permitted within it. On one side, the embryonic Gaullist
colonial ministry based in Algiers offered relatively ambitious proposals. Henri Laurentie, head of the political section of the Commissariat
aux Colonies, laid out plans not only to liberalize administrative practices but also to reconfigure the empire as a federation allowing greater
autonomy for the various component territories. Laurentie stopped well
short of proposing self-government. "If there is to be self-government,"
he insisted, "it can come only at the end of a fairly long and strictly controlled evolution." But there was no mistaking that Laurentie's proposals envisioned significant change.
This view encountered stiff opposition from the assembled colonial
governors, most of whom had risen to positions of influence under a
prewar system that eschewed federalism and emphasized the assimilation of colonial peoples into a unified French empire. Under their sway
the meeting accepted only minute steps to improve indegene opportunities and coldly rejected Laurentie's guarded language about self-determination. To the conservatives, talk of "le self-government"-a phrase
so alien that it was always rendered in English, as historian Martin
Shipway has pointed out-flew in the face of the hallowed Jacobin principle of "France One and Indivisible." The conference's final declaration, though only an advisory document, left no doubt where the conservatives stood. The French "civilizing mission" in the colonies
excluded "any idea of autonomy [and] all possibility of evolution outside the French bloc," the statement asserted. "Also excluded," it added,
"is the eventual establishment of self-government in the colonies, even
in a distant future."
These opposing viewpoints laid out the parameters of a debate over
Indochina's future that would percolate within the Gaullist bureaucracy
for the remainder of the war and would, in later years, break into the
open. For the time being, however, deliberation over postwar reforms
remained muted. Far more pressing was the challenge of assuring that
France, rather than some other power, would make the decisions when
the moment came. On this score French officials of all political stripes 
were united in anxiety. While every other part of the prewar French
empire had rallied to de Gaulle by 1944, Indochina floated in precarious limbo, nominally under the control of a French administration loyal
to the Vichy regime but vulnerable to Japanese takeover.
From 1940 Gaullist leaders had done what they could to show their
determination to recover Indochina and to reclaim for France a prominent role in the Far East. Free France declared war on Japan immediately after the Japanese attack on Pearl Harbor in December 1941 and
a month later began planning for the eventual dispatch of forces to fight
alongside the allies in the Pacific. De Gaulle's Comit6 Francais de la
Liberation Nationale stepped up its efforts in 1943, approving creation
of an expeditionary force in North Africa for use in Indochina and
requesting British and U.S. permission to post an officer to the Allied
headquarters supervising the war in Southeast Asia.

</file>

<file= AmE06_J57>

While William Leuchtenburg is certainly correct in concluding that the shadow
of both FDR and the New Deal is receding, the decade of the 1930s is
still regarded as the acme of progressive reform in America. Although
the memories of the 1930s are fading and the reforms of the decade have been
contested, in this narrative the achievements of the New Deal are still heroic.
In many respects I do not challenge this assessment. However, an
important aspect of this heroic narrative deserves reevaluation. Robert
McElvaine, in his analysis of the decade in 1984, argues that the loss
of economic status by men in the Great Depression resulted in a ''feminization
'' of cultural norms: ''The self-centered, aggressive, competitive
male ' ethic ' of the 1920s was discredited. Men who lost their jobs became
dependent in ways that women had been thought to be. '' As men
found themselves much more in the traditional role of women  -  on the bottom, in a
state of dependence  -  they moved toward ''feminine '' values _ When, with the
New Deal, they got beyond passivity and became active in their quest to improve
their situation, Depression victims tended to do so through ''female '' values.
They sought to escape dependence not through 'male ' self-centered, ''rugged ''
individualism, but through cooperation and compassion.
According to McElvaine, the reformers of the 1930s were sandwiched
between Progressives and post-World War II liberals who remained
'' essentially 'masculine' '' in their ''urge for 'male' exploits, '' concern for
projecting ''toughness'' and '' fear of appearing to be a ' sissy. ' '' In his revised
edition (1993), McElvaine noted that he had inadequately emphasized his
gender analysis. The ''most significant fact about the Depression era '' was that
it was the only time in the twentieth century during which there was a major break
in the modern trends toward social disintegration and egoism _ Among the few
restraints that have existed in postwar America on the modern forces of social
atomization are carryovers from the values that were resurgent during the thirties.
Other analysts share McElvaine's assessment in whole or in part. Some
focus on ''FDR's body'' as a signifier of female values. Others find feminist
foundations in New Deal policies or in radical alternatives to the New
Deal. There are others too who do not explicitly employ gender motifs but
describe New Deal achievements in terms of the values identified by
McElvaine. Finally, even those who note the relative absence of female
voices in the 1930s speak of inadvertence and historical inopportunity.
One can argue, however, for precisely the opposite reading. The nearly
universal collapse of the economic base of masculine identity produced a
patriarchal reassertion that suffused all discourse in the 1930s. Deprived of
the authority that emanated from control of the household, men retrieved a
sense of autonomy by creating a new masculine public space. Catalogues 
of Depression life contain heartbreaking confessions of emasculation.
Relief agencies only compounded the loss of power for the person who
entered the '' intake'' room to apply for aid. The behavior of unemployed
men was described in negative female stereotypes. A father '' cried in relating
his situation to the case worker'' ; other men were reported as ''high strung''
and lacking in ''emotional stability. '' The anthropologist Margaret Mead
wondered what would be the consequences for male children born to a
generation of fathers who lacked an '' inheritance of aggression. ''
In response, New Dealers created a new world based on masculine values
other than appetitiveness, risk, and self-interest (though these too often
reappeared in veiled ways). These values were derived from a redefintion and
refocus of the concept of male responsibility and they took place in a
new sphere, a public one. To be a man no longer depended on material
acquisition but on public stewardship, no longer on speculation but on
public adventure. Strength too was invoked but it was strength derived
from discipline rather than self-assertion. I examine the development of
these tropes in both New Deal and radical discourses in the 1930s in order to
illustrate the pervasiveness of patterns of male assertion through new
homosocial structures. Though entry into World War II altered this reformulation
in unanticipated ways, the model of the citizen soldier too
expressed this notion of male responsibility as a collective effort. Comrades
became comrades in arms. Thus efforts to ''reopen'' the 1930s as a narrative
for a new point of reformist or radical departure may be a mistaken project
or at least a more complex one than is currently proposed.
TITANS/PLANNERS
Writers in the 1930s were preoccupied with examining differences from
the previous decade. For its participants, no sharper division had occurred
in the history of the republic. This division, marked by the brute fact
of the Crash, was theorized in binary terms, like gender itself. The binary
conception of political and economic crisis was expressed in general terms
of male/female through various dichotomies such as true/false masculinity,
responsibility/irresponsibility, power/powerlessness, will/indecision, individualism/
collectivism.
In Farewell to Reform, one of the most popular political commentaries of
the early 1930s, John Chamberlain concluded that reforms in previous crises
in America were preoccupied with a ''return connotation.'' If only the
''methods and possibilities of a more primitive capitalism '' could be restored,
the crisis could be resolved. Chamberlain was especially critical of the
Progressive tradition in this regard. Progressives failed so miserably in their
mission that they were helpless to stop the concentration of capital in the
1920s that had brought about the current crisis. Had the Progressives, as well
as previous reformers, been more manly in their confrontation with capital,
the current generation would not have been faced with the present emergency.
Thus Veblen, Croly, Dewy, and Beard could offer no help in ousting the
''high priests of profit'' and Americans must reject the whole reform
tradition.
Progressives themselves accepted this critique. Dewey, for example, in a
series of articles in the New Republic, complained that America had become a
''United States, Incorporated'' in the 1920s, in which the '' business mind''
crowded out individualism to make a ''pecuniary culture'' based on speculation
and consumption. ''The publicity agent is perhaps the most significant
symbol of our present social life,'' he concluded. Dewy had reservations
about the Soviet experiment but he acknowledged that it exhibited a sense of
''imagination'' and ''resourcefulness '' that deserved study. In March 1932
Adoph Berle, Jr., too, stated that the '' future is becoming visible in Russia. ''
He assured readers that soviets in America would look more like rotary clubs
than their counterparts in Leningrad. Berle's comparison is instructive since
the notion of soviets, rotary-style, suggested a more '' responsible'' imitation
of Soviet experimentation and also invoked the image of American male
reliability as well as masculine empowerment. In the same year George Soule
wrote whimsically, ''Perhaps Russia was not talking the same language that
we were, but at any rate she was using a lot of the same words.'' This
abandonment of Progressive ideas, often in the context of an interest in the
USSR, also partially erased the binary preoccupation between capitalism and
communism as it simultaneously accentuated the divide between the decades.
The proposals appearing in the new journal Common Sense repeatedly
focussed upon advertising as a development of 1920s capitalism that
was especially corrosive of a stable economic system. Co-editor Alfred
Bingham's ''Looking Forward,'' a view of America in 2000, spoke of a
''democratic republic of industry '' in which a ''new moral health, and a new
moral beauty'' would prevail. Harold Loeb's book-length utopia, Life in a
Technocracy, disposed of advertising as well as government and elections
altogether in favor of a national engineering CEO supervised by unions.
Richard Chase (A New Deal ) and George Soule (A Planned Society) offered
similar sentiments. To them as well, the 1920s represented an era in which
irresponsible economic elites promoted systemic chaos in their pursuit of
immediate gain. The future would replace acquisitiveness with an '' active
bond with our fellow citizens. ''
Each of these critiques thus put forward the values of collective
commitment as the responsible mode of behavior. Advertising in particular
was singled out as a feminized form of activity that focussed upon
consumption rather than production. Not only did this focus of capitalism
overburden the household with gadgets, but also the fixation on demand
created a world of caprice and illusion. True (responsible) action rested in the
excitement of common effort, since the household itself had been corrupted.
Americans had been seduced by a false American Dream unrelated to real
work. As the price, they had been cast permanently from Eden.
While these first ideological exits were dramatic and unprecedented in the
scope of their critique, it was FDR, more than any single figure in the early
1930s, who systematically discredited the 1920s on these terms. Hoover
actually remained the last public figure to defend the 1920s. He continued to
insist that the decade was a period of phenomenal economic growth, increase
in the standard of living, and democratization of consumption.
''Over-optimism as to profits'' and events in Europe fed an ''undue
pessimism'' on the part of business leaders. FDR rejected Hoover's contention
that '' the major sources of the depression now lie outside of the
United States '' with blistering images of the irresponsibility of economic
elites in the 1920s, with politicians as their accomplices. In fact, Hoover's
defense of '' irresponsible '' household consumption and his alleged inability
to confront the Depression crisis doubly feminized him as an exemplar of
female hysteria.
Seduction, promiscuity, immediate pleasure, gullibility, and undeserved
leisure are historical male constructions of the irresponsibility of Eve and
they floated through FDR's critiques of the 1920s. The Oglethorpe
University speech is often quoted for FDR's announcement that '' bold,
persistent experimentation'' was now required of government. But the bulk
of the address was devoted to portraying the 1920s as a period in which
economic elites created a ''mirage'' of real wealth. In these '' rose colored
days'' every American was told that he ''could just sit back and read in
comfort the hieroglyphics called stock quotations which proclaimed
that wealth was mounting miraculously without any work on their part. ''
With the government's complicity, corporate leaders in the 1920s were
responsible for '' gigantic waste, '' ''superfluous duplication, '' '' questionable
methods of raising capital, '' ''continual scrapping of useful equipment'' and
''depression of wages.'' In his nomination acceptance speech, the 1920s
were condemned as a ''period of loose thinking, descending morals, and an
era of selfishness. '' FDR even asked all Americans to accept some degree of
responsibility : ''Let us be frank to admit that many of us too have made
obeisance to Mammon.''
Late in the 1932 campaign Roosevelt gave a speech to a group of
business leaders in San Francisco and pulled all these elements into a general
philosophical approach to government. The original title of the address was
'' Individualism: Romantic and Realistic. '' The speech is structured around
sets of pairs. In this binary analysis, however, the negative is not a feminized
disruption but one form of masculine endeavor. Roosevelt compares the
growth of central governments to that of centralized industry. He compares
the rise of European monarchies to the new '' princes of property. '' He
compares Jefferson to Hamilton. He compares the needs of a country with a
frontier to those of one whose '' plant is built. '' And, of course, he compares
the responsible and irresponsible exercise of power.
Roosevelt presents his analysis of great leaders in ambivalent terms. Each
creates arenas of freedom for others, though inadvertently, as they pursue
their own projects. Lincoln had spoken of the need for men of great ambition
at the time of the founding and of the threat they posed in an established
republic. Roosevelt recounts the contributions of the '' creators of national
government'' in Europe. ''The people preferred the master far away to the
exploitation and cruelty of the smaller master near at hand.''

</file>

<file= AmE06_J58>

In the second half of the sixteenth century, experiences and narratives of English travel
to distant places first began to matter enough to be collected and published. Tracing
early accounts of West Africa and Muscovy through the several collections of Richard
Eden (1553, 1555) and Richard Hakluyt (1589, 1600) allows for comparison of how
different editors handled the same materials at different moments. The evidence suggests
that both editors differentiated between the African and Russian materials according
to perceptions of these materials' value, or meaning, for their own collecting and
publishing projects. Looking at how this was so, and considering why it was so, provides
a closer and more detailed look at how travel writing acquired value in the context of
print; it also offers an an approach to the larger question of how Englishmen "read"
the places and cultures they encountered, actually or virtually, outside of Europe.
I begin with a massive material object: the second edition of Richard
Hakluyt's great collection of travel documents, commonly known as The
Principal Navigations of the English Nation. Roughly 2000 closely-printed folio
pages, it was issued as two or three large volumes in the successive
years 1598-1600. In imagination, set next to this book its earliest English
predecessor. Richard Eden's collection, A Treatyse of the newe India (1553),
was a poorly printed octavo of 204 pages. These are books which take
up quite different amounts of space, and have enjoyed quite different
reputations. Eden's has remained relatively obscure; Hakluyt's has become
known as the "great prose epic of the English nation". Apart from their
contents, the sheer difference in size has surely played some role in the
physical survival of both books, and consequently in their present value - 
as distinct from their reputation. Hakluyt's three volumes are a common
possession of rare book libraries: a recent census has located some two
hundred and forty sets. As this article was being written, a copy priced
at $20,000 was for sale online: hardly a casual purchase, but still not
outside the reach of a generous research budget, not to mention a reasonably
well endowed library. Eden's Treatyse of the newe India survives
in only six copies; were one to come on the market, it would be almost
beyond price.
What changed between 1553 and 1598? While the pace of English
travel and exploration certainly increased during these 50-odd years,
Eden and others were simply not interested in many of the existing
materials which Hakluyt sought out for his larger collection. Many of
Hakluyt's documents had never been printed before, and some of the
voyages they recorded were forgotten by the time Hakluyt began his
work; in some cases, the records themselves had already vanished, and
testimony was solicited from elderly survivors. The idea of getting voyage
narratives into print and disseminating them was evidently not as
obvious in the middle of the sixteenth century as it may now seem in
the wake of Hakluyt, Purchas, and the last century or so of publications
by the Hakluyt Society and others. During the 45 years which separated
Eden's first book and the second edition of Principal Navigations, the materials
which Hakluyt collected had acquired a value which they did not
seem to have earlier.
In this essay, I will examine how value is assigned both to, and in,
two particular sets of materials present in both Eden's and Hakluyt's
collections: English accounts of West Africa and Muscovy from the
1550's. As compared to other parts of the Hakluyt anthology, these
materials have received relatively little attention from modern scholars;
yet they have a particular importance for a history of the travel book.
Documents on both regions appeared in Eden's second collection (1555),
were reprinted by Richard Willes in his 1577 revision of Eden, and again reprinted with changes and additions by Hakluyt in 1589 and
1599. They represent an early stage in the collection of first-person narratives,
while also allowing us to compare the treatment given them by
editors and editions over several decades; Eden's and Hakluyt's framing
comments and organizational schemes give some sense of the ways these
two editors understood both the documents and the places the documents
described.
What kinds of enterprises led the English to the coast of West Africa
and to Muscovy? In the early years of the decade, overlapping groups
of English investors set in motion a series of voyages in two directions.
The first trading voyage to West Africa took place in 1553, the same
year that a newly constituted company sent three ships to the northeast
in search of a passage to Cathay. Both voyages combined a kind of
disaster with a kind of success.
From the West African coast, two of three ships returned, but with
only forty out of the initial complement of one hundred and forty men;
the rest died of disease or were abandoned in Benin, and more died
soon after the return to England (PN VI, 151). Nonetheless, the voyage
turned a considerable profit, and in 1554 a second voyage was captained
by John Lok - whose brother Michael would become London
Agent for the Muscovy Company. Two of the three ships which sailed
northeast likewise came to grief, delayed and finally wintering in Lapland,
where the crews froze to death. The remaining ship, the Edward Bonaventure,
managed to round the Kola Peninsula and anchor in the White Sea,
at the mouth of the Dvina River (roughly, present day Arkhangel'sk).
From there, its captain - Richard Chancellor - proceeded south to Moscow
at the Tsar's invitation and delivered a letter from Edward IV intended
for "the Kynges, Princes, & other potentates inhabytynge the Northeast
partes of the worlde towarde the myghtye Empire of Cathay" (Eden
1555, 308). Chancellor returned with information about the country
and its goods which encouraged the promoters to seek a charter from
the crown, which they received from Philip and Mary in 1555. Later
that year, Chancellor returned to Muscovy with two ships, bringing 
agents of the newly incorporated Muscovy company who were duly
granted privileges by the Tsar. Goods traded included cordage and
hemp, flax, train-oil (oil made from the rendered fat of whales or seals),
furs, hides, tallows (candles made from animal fat), and wax. The
Muscovy Company retained a monopoly of the White Sea trade into
the 1570s, as well as an exclusive right to trade in the interior without
duties or tolls.
This northern route to Muscovy was not unknown, and had been
described in print by the Hapsburg emissary Sigismund von Herberstein,
whose account both Eden and Hakluyt made use of in their collections.
The White Sea route was "not commonly frequented" before 1555,
much less was it a corridor for regular trade, and Muscovy itself remained
poorly known to western Europeans. More broadly, the English shared
with some other Europeans both intense interest in the northern parts
of Asia and a general ignorance about them. When members of what
would become the Muscovy company met to consider setting forth a
voyage in this direction, they could do no better than to interview two
Tartar grooms:
. . . two Tartarians, which were then of the king's Stable, were sent for, & an interpreter
was gotten to be present, by whom they were demaunded touching their
Countrey and the maners of their nation. But they were able to answere nothing
to the purpose: being in deede more acquainted (as one there merily and openly
said) to tosse pottes, then to learne the states and dispositions of people.
The scene suggests both the cosmopolitan nature of London's population,
and the paucity of its geographical knowledge.
So if the "discovery" of a trade to Muscovy was new more in regard
to English trading patterns than in respect of actual knowledge, it was
nonetheless undertaken within the context of very long-distance exploration,
and the company would continue (albeit sporadically) to pursue
the grail of a Northern passage to Cathay and the Indies even as it
undertook regular and settled trade along the route Chancellor pioneered.
The voyages to West Africa, by contrast, were not a discovery
at all, since Portugal had an established trade along the coast dating 
from the 1440's; by the mid-sixteenth century, there were permanent
trading bases and a significant Afro-Portuguese population, some settled
on the previously uninhabited Cape Verde Islands off present-day
Senegal. The French had preceded the English as interlopers in this
trade, which at the time primarily exchanged cloth and metal goods
for ivory, gold, and malaguetta pepper ("grains of paradise"). As the
results of the first voyage and others made clear, the coast was not without
its dangers, but these came primarily from the disease environment
rather than from a population which, generally speaking, had been
involved in long-distance, cross-cultural commerce well before the advent
of Europeans and saw certain advantages in adding new trading partners
to compete with the old.
For Hakluyt, both sets of materials - like many other documents in
the collection - were valuable in general terms because they described
the extension of English presence, and commerce, into areas that were
at least new to them. Hakluyt wrote in the preface to the first edition
of Principal Navigations (1589) that
during my five years abroad [in Paris, 1583-88] . . ., I both heard in speech, and
read in books other nations miraculously extolled for their discoveries and notable
enterprises by sea, but the English of all others for their sluggish security, and continual
neglect of the like attempts . . . either ignominiously reported, or exceedingly
condemned.
His collection sought to remedy the sense that, outside Europe, most
areas were still new to the English. For Eden in the 1550's, the situation
was quite different, both rhetorically and historically: there, the secondary,
imitative, or dependent quality of English endeavor was simply
a fact, with consequences visible both in the work of the editor, and in
the experiences which his collection records.
A well-connected translator and sometime employee of the treasury,
Eden assembled his Treatise of the New India in the last year of Edward
VI's reign. It was followed in 1555 by a second collection, Decades of
the new world. Eden's collections both ostensibly translated other books.
The first both translated parts of Sebastian Münster's cosmography, and
followed it in dividing its material between Spanish voyages to the West,
and Portuguese ones to the East. Eden's prefatory matter mentions two
English initiatives - both the Cabot discoveries of the late 1400's and
the voyage then under way to find a Northeast passage to China - but
neither was documented in the collection. The second Eden anthology
translated part of Pietro Martire de Anghiera's De orbe novo (1516-30).
Eden added to this translation not only additional Spanish material but
two accounts of English voyages to West Africa, and a rather larger
assortment of items related (somewhat loosely) to English voyages in the
North.
Decades opens with an encomium to the almost divine achievements
of the Spanish in discovering and conquering America and an effusive
description of marriage celebrations between Philip of Spain and Mary
Tudor, proceeding to an insistent demand that England acknowledge
the authority of Spain. The contrast between this moment and the patriotic
prefaces of Hakluyt's Elizabethan collections is nothing short of
breathtaking: "stoop, England, stoop and learne to know thy lord and
master, as horses and other brute beasts are taught to do" (Eden 1555,
bi verso). Nonetheless, the deference of Decades was complicated. John
Parker suggests that England's northern voyages (begun in the reign of
Edward VI, continued after the accession of Mary) lay behind the
appearance of Eden's collections, devoted even as they were largely to
the accomplishments of other nations.
The first of Eden's two original narratives of travel to West Africa
was thematically concerned precisely with the issue of how to place oneself
in relation to a foreign expertise which had to be acknowledged,
because it could not be duplicated at home. It is a story with a moral,
and a surprisingly pointed one: the disastrous consequences of an English
captain's bad behavior towards his Portuguese second-in-command.

</file>

<file= AmE06_J59>

The influential role of Alfred Stieglitz's Camera Work in introducing modern art and literature to an American audience has been noted, studied, and celebrated for many decades now.  Despite all this attention, however, the most pertinent question has rarely been asked: why should it have been a camera magazine that played such an influential role in bringing abstract art and experimental literature from Europe to the United States? By some accounts, Stieglitz's initial interest in painting was motivated by nothing more elaborate than his fear that there was not enough good photography to fill his magazine and the gallery associated with it.  On the other hand, Stieglitz often claimed that exhibiting and printing paintings along with photographs would help define photography more precisely and make its place more secure among the arts. 3 And yet, the publications and exhibitions mounted by Stieglitz in the early years of this century are now considered most significant because of the aid they gave to a general attack on the very category of art, as if photography had somehow communicated its own equivocal status to the paintings associated with it, instead of gaining some of their artistic status for itself.
As inconsistent as they are, these various positions all share the basic assumption that photography, art, and literature are independent entities to be put in some sort of relation to one another. Perhaps, on the other hand, modern art and literature appeared in this particular camera magazine because the medium of photography itself had already brought them into a new relation. Stieglitz himself suggested something like this in 1912 when he declared to one puzzled reader that Gertrude Stein's prose portraits of Matisse and Picasso would explain

"What Picasso & Co. have to do with photography."  By this he did not mean that Stein has commented in any explicit way on photography. In fact, she hardly comments in any explicit way on Picasso and Matisse. What Stieglitz may have meant instead is clarified by his editorial statement in Camera Work that Stein's prose provides "a Rosetta Stone of comparison" between the unknown visual language of the modern painters and the common tongue of the contemporary spectator.  The metaphor implies that Stein's verbal portraits of these painters "attempt to fuse word and image," as Marjorie Perloff puts it,  or perhaps that they put word and image in a mutually interpretive context, as the Rosetta Stone did in juxtaposing hieroglyphics and Greek text. But how would this explain what "Picasso & Co. have to do with photography"?
What is most remarkable about Stieglitz's metaphor is that it is perhaps the very oldest to be applied to photography itself. When Francois Arago first presented the daguerreotype to the French Chamber of Deputies in 1839 he invoked the Rosetta Stone and proposed Daguerre's copper plates as superior versions of the stone tablet that had made Egyptian hieroglyphics intelligible to modern Europe.  A variation of this metaphor has become perhaps the most famous to be applied to Stieglitz's own work. As Egmont Arens put it in 1924, Stieglitz captures with his camera "a symbol which is as convincing as any brush stroke, or any arrangement of words, rhythmic or otherwise. These pictures of his are the hieroglyphics of a new speech ... "  More than fifty years later, writing of the same photographs, Rosalind Krauss is drier and more technical, but she uses the same metaphor: 
In calling this series Equivalents, Stieglitz is obviously invoking the language of symbolism, with its notions of correspondence and hieroglyph. But what is intended here is symbolism in its deepest sense, symbolism as an understanding of language as a form of radical absence - the absence, that is, of the world and its objects, supplanted by the presence of the sign.  
Krauss's purposeful invocation of the hieroglyphic metaphor of another era is a reminder that photography has always seemed to promise a new sign system, a medium neither precisely alphabetic nor pictorial.
The idea that photographs are somehow hieroglyphic was conventional even in Arens's time, and it seems quite likely that Arens would have been familiar with the most famous contemporary version of the idea, for he published poetry by its popularizer: Vachel Lindsay. The first issue of Playboy, Arens's magazine of "Art and Satire," which appeared in 1919, contained Lindsay's hymn of praise to Mary Pickford, who was one of his most cherished cinematic hieroglyphs.  The throne she uses in the 1914 film Such a Little Queen figures in the chapter entitled "Hieroglyphics" in Lindsay's pioneering book The Art of the Moving Picture, which made widely influential the idea that "American civilization grows more hieroglyphic every day."  In Lindsay's mind, and for those like D. W. Griffith who eagerly accepted his analysis, the movies revived a primitive unity of pen and brush, lost when writing split itself off from decorative painting.  The picture language Lindsay thus imagined for the screen was certainly supposed to be more direct and concrete than alphabetic writing, but it was at the same time more abstract and conventionalized than mimetic painting. Movies mattered so much to Lindsay because they brought pictures and writing into contact, to make a continuously unrolling Rosetta Stone that translated back and forth between images and linguistic constructs.
However, the most significant connection between Lindsay's polemic and Stieglitz's magazine would not become apparent until long after both men were dead. Invited out to Hollywood in the mid-1920s, Lindsay was overwhelmed by the movies he saw in production, especially Douglas Fairbanks's Thief of Bagdad, of which he wrote a lengthy analysis, delivered as lectures at the University of California, Los Angeles and then forgotten.  Lindsay found in this reworking of the Arabian Nights a self-reflexive parable of the hieroglyphic ancestry of the movies. The three treasures on which the drama turns - a magic crystal, a flying carpet, and a golden apple - are at once emblematic summaries of the conflicting aims of the characters and simultaneously shorthand symbols for the film medium itself, "a new alphabet, or a very, very old one," as he puts it.  In one sense, these treasures are instances of a kind of picture language mythologized in European accounts as old as Herodotus. In another sense, they are self-reflexive representations of the powers added to human experience by photography: the power of the crystal to see far away, of the carpet to travel to distant places, of the apple to revive the dead.
Lindsay puts special emphasis on the apple, which, together with a scepter containing a poisonous serpent, provides a perfect shorthand illustration of the "hieroglyphic method," one of the "fundamental, structural principles of the motion picture play,"  and this emphasis, quite unbeknownst to him, brings his discussion back to the hieroglyphics of Stieglitz and Camera Work. For the Court Magician, who enacts the "hieroglyphics of death" by finding the magic apple and then testing it on an innocent fisherman killed by the snaky scepter, is played
by Sadakichi Hartmann, who had published in Camera Work what is close to the earliest aesthetic appreciation of the movies to appear in the United States, preceding even Lindsay's own (fig. 2). 
After a long career spent in close association and sometime rivalry with Stieglitz, Hartmann had come to Hollywood himself, where he knew Fairbanks well enough to be offered a part in Thief of Bagdad. It is a more appropriate part than anyone involved could have known, for Hartmann, deploying his apple and scepter, becomes a kind of hieroglyph himself, which is fitting, since he had long been referring to photography as "a new kind of writing." 19 Perhaps Lindsay is so impressed by the hieroglyphic power of the apple and scepter because they are wielded by a fellow critic, one who had long since recognized in still photography the symbolic possibilities that Lindsay found so exciting in the movies.
The presence of these two figures, poet and art critic, on either side of the screen, joined together in their hieroglyphic situation, suggests how various and complex was the photographic context in which words and images met in the early years of this century. The advent of photography as a representational medium inevitably altered the relation of the visual arts to language, sending Lindsay back to ancient times for an analogue to this new situation in which the two seemed to merge. The arrival of moving photographs as the single most influential mass medium of the modern age also altered the relation of art to its audience, in so fundamental a way that understanding the new relation became one of the defining tasks of modernism. Camera Work achieved the iconic position it still enjoys not just because it published Picasso, Matisse, and Stein but also because, as a magazine devoted to photography, it offered a conceptual space in which the relations between them could be realized, in polemics and in works of art. Though it is impossible to take literally the idea that photography is hieroglyphic, the insistence that it is represents the radical change it brought to the relation between language and image, a change that Camera Work helped to advance in a number of ways, some of them hidden even from Stieglitz himself.
Throughout his long career, in all his publications and at each of the several galleries he directed, Stieglitz tirelessly sought to clarify and solidify the status of photography, first in relation to the other arts and then in relation to its audience. It is all the more remarkable, then, that his ideas on both of these relations should have been so inconsistent and so poorly defined. Perhaps this very vagueness was the source of his 
influence, however, for it reflected what was truly exciting about photography in the early years of the twentieth century, that it could not be easily inserted into the traditional economy of the arts, that it was modern in a very basic sense, since it demonstrated how new inventions could require a wholesale reorganization of the artistic and cultural apparatus. That Stieglitz had no idea what that reorganization would look like did not prevent him from becoming indispensable to its beginnings.
Perhaps the simplest and most straightforward of all the positions taken in Camera Work on the aesthetic and representational nature of photography is that which also conflicts absolutely with Lindsay's notion that it is essentially hieroglyphic. When Roland Rood insists, in the very title of his 1905 article, on "The Evolution of Art from Writing to Photography," he means to put photography at the furthest remove from writing, at the very end of an inevitable evolutionary process that has faithful mimesis as its goal. Rood agrees with Lindsay that in ancient times art was barely distinguishable from writing, but he insists that as humanity developed it left behind the schematic, conventional representations of its infancy for faithful reproductions of the actual sensuous qualities of things. Photography, since it most fully and faithfully reproduces these qualities, stands at the pinnacle of human aesthetic evolution: "it can not possibly be denied by anyone who has studied the evolution of the art of any country, that this evolution unfolds itself in but one manner: from literature (picture-writing) at the one end, to full tonality (photography) at the modern (more civilized) end."  It is hard to imagine from this account how Camera Work could have helped anyone to discover modern literature, because for Rood literature is by definition primitive, even savage, while all good modern art strives toward a "non-literary, non-savage, photographic quality."  
Rood's is an extreme and somewhat eccentric version of a common, obvious, and straightforward opinion about art, which was especially congenial to photographic polemicists at this time because it made all art essentially photographic. As Charles Caffin put it in 1908, "the point of view of all painters who affect truth to nature is photographic and has always been so." 
Caffin is certainly aware that there have always been painters who do not "affect truth to nature," but these, in such accounts, are held to be sports and side shoots, deviations from the essential development of art toward complete mimesis, which is often thought to have brought photography into being.

</file>

<file= AmE06_J60>

Introduction.
Mechanical Recording and the Modern Arts.
In historical terms, at least, photography is the first of the modern arts. The very existence of a modern period, broken away from the time before, is to some extent the creation of photography, which has made all time since the 1840s simultaneously available in a way that makes the years before seem that much more remote. As Carlo Rim maintains, "the advent of modern times dates from the moment the first daguerreotype appeared on the scene. ... Thanks to the photograph, yesterday is no more than an endless today." In this most basic sense, art and literature have had to emulate the modernity of the photograph, attempting in their various ways to achieve the instantaneous newness that photography first brought to the cultural scene.
Photography and the modern arts are linked in a number of other ways as well, as several recent studies have shown. Whenever the issue of faithful mimesis is raised, the example of photography is apt to be influential, in a positive or a negative way. It is also clear that the habit of seeing photographically has affected modern experience to such an extent that certain oddities of the camera, especially its tendency to frame particular points of view and to isolate one moment from another, have become second nature for human observers as well. Recent studies of photography and literature have therefore shown in some detail how camera vision affects the motives and perhaps even the perceptions of influential modern writers. 
In showing how the camera has affected modern writing, however, scholars may have missed the more fundamental fact that photography is itself a kind of modern writing. One of the most revolutionary effects of the new medium was in offering hope of new methods of representation, neither linguistic nor pictorial but hovering in a kind of Utopian space between, where the informational utility of writing meets the immediacy of sight. If, as Yve-Alain Bois and Rosalind Krauss have long maintained, it is the mutual interference of the linguistic and the optical that makes modern art distinctive, then perhaps the common beginning of modernism in literature and the arts is to be found in the recording technologies that brought the whole relationship of word, sound, and image into doubt. 
Photography began to trouble this relationship from the very beginning, from the 1839 diary entry in which Fox Talbot jotted down the phrase "Words of Light." Talbot was working out the technical problems of a new procedure he was still calling "photogenic drawing," but the neologism caused much dissatisfaction, and the happy metaphor "words of light" inevitably provided what became the accepted name for the new medium.  Talbot's metaphor has been dead for so long that no one looking at a photograph today thinks of it as "light drawing," much less "light writing," but in fact the oldest of Talbot's photographs to survive is a picture of his own handwriting, tracing out the alphabet.  And though this particular subject was most probably necessitated by the practical limitations of Talbot's early method, it is the case that photography was originally promoted as a replacement for writing as well as a rival to painting. The famous bill placed before the French Chamber of Deputies in 1839 to secure Daguerre's invention for the state actually listed the reproduction of graphic notes as one of the new mediums primary uses.  Talbot offered his invention as a superior means of copying written documents, but even when it bypassed writing it could be used, he suggested, for keeping inventories, as a system of notation superior to writing but playing much the same role of information storage and retrieval.  In these early days, the sense that the "graph" in "photograph" pertained to writing rather than drawing was so strong that Elizabeth Eastlake could say the medium offered a "new form of communication ... neither letter, message, nor picture." 
The sense remained strong throughout the nineteenth century and well into the twentieth that photography provided a sort of notation suspended somehow between letters and pictures, a new alphabet, as it were. Impressed by Nadar's photographs of nineteenth-century Paris, Mallarmé exclaimed that light had become "photography's own form of 'writing.'"  In the next century, even as the new, more purely optical style of "straight" photography took hold, an underlying belief remained, as Sadakichi Hartmann put it, that "photographic illustration has become a new kind of writing."  Thus photography was strongly associated from its very beginnings with hieroglyphs, another form of writing that, in the popular view, bypassed sound and spoken language to reach the mind directly through the eye.  One of the first projects suggested for photography in France was a new expedition to Egypt, where it might record and bring back more Rosetta Stones,  but in a sense every photograph was felt to be a Rosetta Stone, decoding every written language by reference to a visual language common to all. Photography, Oliver Wendell Holmes predicted, was to become "the card of introduction to make all mankind acquaintances."  The new medium created a hopeful fascination with new forms of notation, revolutions that would also return to an ancient purity and directness, a pre-Babelic unity of word and thing.
In so doing, though, photography was just the first in a series of technologies that promised new and better forms of writing, some of which, like the phonograph, staked out that territory with their very names. Historians of the phonograph often note Edison's surprise that it might be used for music, since his original purpose for it was primarily stenographic.  All the early applications for this invention imagined it as a repository of language, reading aloud books and newspapers, recording speeches, taking letters. Even the music critic of the New York Post, as Lisa Gitelman notes, imagined phonographic newspapers and not recorded music.  This expectation recalled and reinforced the original association of photography with language: the common suggestion that Edison's new invention photographed sound depended on the idea that legible recording had in some way to be visible recording, like writing. Edison himself was mildly obsessed with the idea that phonograph records should be readable, not just by his own machines but by human beings, and he spent many hours with a microscope trying to find the letter  impacted in the indentations of a record.  Despite his failure, artists began to speculate about the possibility of phonographic writing through direct incision of what Laszlo Moholy-Nagy called "the language of the groove."  Among the general public there was widespread speculation about a phonographic alphabet, a set of patterns in the grooves that might be direct and specific yet also regular enough for iteration and recognition, and there was much consternation, popular and legal, when this hope was disappointed, and the idea had to be confronted of a notation, a writing, that could not in fact be read.  Thus phonographic inscription also came to be associated with hieroglyphs, which had themselves resisted being read for centuries, though they were finally found to be legible. The direct transcription of sound into any visible form, no matter how variable, seemed to promise a new universal language, or at least a universal alphabet, on which a fully motivated and wholly transparent language might be based. 
The rhetoric surrounding these two technologies is so similar because they represent popularly successful versions of an older and far more various project to induce nature to transcribe itself directly and automatically and therefore perfectly. To some extent, this project violates the currently accepted boundary between science and superstition, and to some extent it represents a profound shift in the way science defines and uses signs. The scientific method itself, according to Thomas L. Hankins and Robert J. Silverman, involved a repudiation of language in favor of firsthand sensory observation.  The old magical notion that words and things were necessarily linked was replaced by a desire to remove the subjectivity introduced into observation by language. As early as the sixteenth century, however, scientists needed to "observe" phenomena not particularly amenable to the physical senses, objects too small or too far away to be seen with the naked eye. Soon the telescope and the microscope were followed by other instruments, which did not just alter sense data but rather represented it, often in graphical form. Thus the problem of language was reintroduced into the research situation, for these instruments occupied a curiously ambiguous position as indices of physical phenomena that did not always iconically resemble what they were supposed to represent. What were they, then, languages or things? What were products like graphs to be considered to be, pictures or words? New systems of notation, such as the graphs of Edouard-Léon Scott de Martinville, seemed simultaneously to make pictures that were conventional and writing that was somehow motivated. Though the whole purpose of such scientific instrumentation was to codify and thus objectify investigation, it had the curious unintended effect of making observation seem dependent on mediation, while simultaneously presenting the possibility of languages that would, like the old magical languages, be linked necessarily to reality. The whole notion that there are two quite different ways in which human beings relate to reality, immediately through the senses and indirectly by means of symbols, began subtly to alter, with consequences so far-reaching they might be considered fundamental to the modern condition.
Hankins and Silverman place photography and phonography as episodes in a long history, in which science attempts to replace words with graphs, particularly graphs drawn by automatic recording devices. Perhaps the earliest attempts to perfect direct graphical notation are to be found in eighteenth-century projects for a kind of "traceology," a method of analyzing the gait of horses and human beings by preserving and schematizing the prints they make.  The first truly successful technology of this kind - the prototype, therefore, of all subsequent recording media - were the sound figures of Ernst Chladni, produced by vibrating thin plates of glass or metal dusted with sand. Early in the nineteenth century, these figures were celebrated as "the script-like-Ur images of sound," allowing scientists to "re-discover, or else to find the primeval or natural script by means of electricity."  At about the same time James Watt perfected an automatic indicator diagram to record variations in the pressure maintained by his steam engine, but this, the first automatic recording instrument, was kept a secret and therefore had little influence.  Throughout the nineteenth century and into the early twentieth, experimentalists tinkered with dozens of such machines meant to induce transcriptions directly from nature, many of which occupied the ambiguous territory between entertainment and science.
Perhaps the most commonly remembered of these researchers is Etienne-Jules Marey, now primarily thought of as a major figure in the prehistory of moving pictures. Early in his career, Marey invented a whole series of complex mechanisms designed to transform the muscle contractions of a frog, the flight of a bird, or the walk of a human being directly into graphic notation, "to coax a graphic language directly from living things." To capture more detail, he turned to photography and then film, developing in each case sophisticated techniques to schematize and graph the movements he wanted to study.  Marey's followers and assistants expanded his experiments in several directions, attempting to photograph speech or transcribe human locomotion, establishing a new scientific discipline of physiological registration that Francois Dagognet has called "biogrammatology."  The unifying aim of all such experiments, no matter what sense they addressed or what medium they exploited, was to induce nature to record itself, to extract from visible objects what Talbot called "self-representations" that would necessarily be encoded, insofar as they were encoded at all, in "nature's own script." 
The idea that these experiments in "biogrammatology" had yielded what Marey himself called "a kind of universal language"  depended on the apparently automatic connection between the subject and its self-transcription. "The 'trace,'" as Dagognet puts it, "was to be considered nature's own expression, without screen, echo or interference: it was faithful, clear and, above all, universal."

</file>

<file= AmE06_J61>

Stanley Cavell's philosophy and Alfred Hitchcock's
films share a distinctive feature: they are
both concerned with the relationship between
doubt and romance. Cavell is a thinker who has
forcefully articulated the idea that the narrative
of heterosexual romance is a place where philosophical
doubt or skepticism enters into the
everyday or the ordinary. For Cavell, skepticism
is the central problem that is confronted in the
lives of protagonists who seek to enter into and
sustain a relationship of intimacy, and classical
Hollywood cinema is a privileged place for
staging the relationship between heterosexual
romance and skepticism. In the history of cinema,
it is Hitchcock who has most consistently
charted the vicissitudes of romance in relationship
to the representation of doubt about what
characters within the romance and the spectator,
alongside those characters, see and hear. Furthermore,
Cavell himself has written an essay
on Hitchcock's film North by Northwest (1959)
as an exemplification of what he calls "Comedies
of Remarriage" that play a significant role in the
articulation of his philosophy.
Yet I shall argue in this essay that it is precisely
because Hitchcock's films share Cavell's
concern with the relationship between doubt
and romance that they call into question his presupposition
of the bond between skepticism and
romance, and the manner in which he uses texts
to illustrate and illuminate it. Cavell's philosophy
is based on two interrelated claims. First,
our relationship to other people and the world is
not one of knowing. This is what Cavell calls
"the truth of skepticism." Second, romantic
love and, in particular, marriage, is the place
where we best learn to live with "the truth of
skepticism," where we overcome or succumb to
its corrosive effects. Although romance in
Hitchcock's films is a place inhabited by doubt
that tests the capacity of a protagonist to understand
the true intentions of his or her romantic
partner, and the world of Hitchcock's films
from North by Northwest to Psycho (1960)
seems governed by a sense of arbitrariness and
incipient chaos, Hitchcock's films do not demonstrate
"the truth of skepticism," for the protagonists
of his works, together with the
audience, typically arrive at certainty about
what it is that they see and hear. Although the
protagonists of Hitchcock's works, and even the
audience for a time, may be uncertain about the
motivation of other characters, these doubts are
not essentially irresolvable, they do not provide
evidence of an inescapable skepticism that
informs everyday interactions. By at once making
doubt explicit and typically resolving it,
Hitchcock's films call into question Cavell's
assumption that the portrayal of romance in
popular American cinema and in Hitchcock's
films, in particular, dramatizes "the truth of
skepticism."
In suggesting that Hitchcock's films provide
a counterexample to the claims made by Cavell,
I hope to establish the broader point that
Cavell's interpretations of the works of popular culture fail to provide "evidence" for the truth
of his philosophy and, insofar as the truth of his
philosophy depends on the interpretation of
texts, that truth is open to question. Of course, I
do not expect Cavell or his followers to be persuaded
by the putative counterexample to his
philosophy provided by Hitchcock's films. For
it seems to me that Cavell's assumption that
"the truth of skepticism" is portrayed in
the vicissitudes of romance is probably an intellectual
presupposition of the kind that
Ludwig Wittgenstein after Goethe called an
Urphänomen, "a preconceived idea that takes
possession of us." By captivating the mind of
the person who adopts it, the Urphänomen is
claimed to underlie all phenomena, regardless
of anything that might count as evidence against
it. Cavell implicitly presupposes that the
explicit representation of doubt and its resolution
in texts, such as those of Hitchcock, are
secondary to the radical doubt or "the truth of
skepticism" that he perceives to underlie the
human condition and that is dramatized in the
romance. In this sense, citing Hitchcock's treatment
of doubt in romance as a counterexample
to Cavell's philosophy will do nothing to dislodge
the conviction of Cavell and his followers
in the truth of that philosophy. However, the
grounds for the truth of Cavell's philosophy
cannot be convincingly based on the interpretation
of texts, despite the appearance of "evidence"
that his discussions of texts seem to
provide, if nothing could qualify as a counterexample
to those interpretations.
CAVELL, SKEPTICISM, AND ROMANCE.
Cavell writes that skepticism "is a place, perhaps
the central secular place, in which the
human wish to deny the condition of human
existence is expressed." By skepticism, he
means the familiar philosophical argument that
"we can never know with certainty of the existence
of something; call it the external world,
and call it other minds." What is "the condition
of human existence" that human beings wish to
deny and why do they wish to deny it? Broadly
speaking, by "the condition of human existence"
Cavell seems to mean human mortality or
"finitude." The wish or impulse to deny human
mortality seems to arise out of the very fact that
to be human is to be aware of one's mortality.
For it seems, for Cavell, that human beings are
inevitably drawn to conceive of their embodied
nature as a constraint or limitation that they
seek to overcome or deny. Skepticism, for
Cavell, is thus an expression of this wish to
transcend our mortal coil. Why? The skeptic
claims that human beings possess a seemingly
direct, unmediated, and therefore indubitable
awareness of our own sensations, thoughts, and
feelings. By comparison, the external world is
known only through our perception of it. Since
in any given case we may be deceived about
what it is that we perceive - for example, we
may wrongly infer that the stick that appears
bent in the water really is bent - how can we be
sure that the external world exists? Furthermore,
since the sensations, thoughts, and feelings
of another can be inferred only through our
perception of what he or she does and says, the
existence of these sensations, thoughts, and
feelings is also subject to doubt. If we can mistakenly
infer that someone is in pain when they
are not, how can we even be sure that his or her
behavior is expressive of something called a
mind? Thus it seems that, for the skeptic, the
condition of human embodiment that makes
possible our orientation in the world and interaction
with others is experienced as a limitation
or constraint that would have to be transcended
in order to achieve certainty. Thus, for Cavell,
skepticism expresses human beings' inability to
transcend their condition of embodiment. Since
awareness of embodiment as a constraint is a
condition of being human, skepticism, for
Cavell, is itself an inevitable consequence of
the human condition. In this sense, Cavell is a
skeptic.
However, what the skeptic overlooks in his
or her pursuit of certainty is the manner that
human expressive behavior in general and the
skeptic's expressive behavior in particular are
embedded in human community. For Cavell,
what makes someone's behavior pain behavior,
as opposed to, say, the expression of pleasure or
an empty gesture, is the fact that other people
recognize and respond to that behavior as providing
criteria for the existence of pain, for
example, by expressing sympathy. Cavell calls
this recognition or response "acknowledgment."
Acknowledgment is a form of attunement with
others and characterizes the way we exist with 

others in the world in a manner that is not based
on knowing. The skeptic thus seeks to replace
the only relationship to the world and to others
that is available to him or her - attunement with
the behavior of others that recognizes or
acknowledges that behavior as human behavior
or as expressive of mind - with something else,
namely, certainty. By seeking certainty in the
existence of the other and thereby coming to
doubt that existence, the skeptic withholds his
or her response to the other; he or she refuses to
acknowledge, for example, pain behavior as
expressive. Skeptical doubt thus expresses a
failure of acknowledgment.
To withhold, or hedge, our concepts of psychological
states from a given creature on the ground that our
criteria cannot reach to the inner life of the creature,
is specifically to withhold the source of my idea that
living beings are things that feel; it is to withhold
myself, to reject my response to anything as a living
being: to blank so much as my idea of anything as
having a body.
There is not only confusion but also a tragic
pathos in the skeptic's position. The skeptic
seeks certainty in the existence of the other in
order to affirm his or her own membership in
the human community. However, by repudiating
the idea that the behavior of others is
expressive behavior, the skeptic repudiates the
conditions of his or her own existence in the
world with others that depends on the acknowledgment
by others of his or her own behavior
as expressive behavior. For it is only through
responding to the behavior of others as expressive
behavior, for example, by sympathizing
with someone who is in pain, that I call on my
own behavior to be affirmed as my behavior, as
expressive of my self. By withholding his or her
response to the other, the skeptic renders himself
or herself unknown to and isolated from the
other. The skeptic thus ends up tragically misrecognizing
the condition of human finitude,
one that is shared with others, as a condition of
isolation or of expressive silence. In so doing,
the skeptic creates the very conditions of existence
that he or she purported to derive from
reflection on those conditions of existence. Yet,
at the same time, it is this very fact that allows
the conclusion of the skeptic to be contested.
For the skeptic can be brought back from his
or her condition of skeptical isolation through
the acknowledgment that his or her response to
the human condition, however extreme, is nonetheless
a human response, that the skeptic's
behavior, too, may be acknowledged as human
behavior.
For Cavell, the drama of skeptical denial and
acknowledgment is played out most intensely in
the relationship between an individual and a
privileged other, in particular (though by no
means exclusively) the other privileged in a
romantic relationship, exemplars of which
Cavell discovers in the works of Shakespeare
and popular culture alike. The reason that
Cavell focuses on this privileged other in his
philosophy is that if I am inclined to disavow
attributing minds to human beings and hence to
disavow their humanity (their autonomy from
me) and hence my humanity (my autonomy
from them), the best case for testing my theory
would be "a given other who exemplifies all
others for me, humanity as such." This best
case then becomes at once for the skeptic the
occasion for the rejection of the humanity of
the other - their autonomy and uniqueness - 
through their metaphorical or literal annihilation
(which reciprocally entails the annihilation
of the self), as in Shakespeare's tragedies such
as Othello, and, at the same time, an occasion
for the most forceful and articulate assertion of
human possibility and renewal through the creation
of a relationship between equals, as in the
cinematic genre Cavell identifies as Comedies
of Remarriage.
Cavell characterizes the Comedy of Remarriage
in terms of a founding myth in which "[a]
running quarrel is forcing apart a pair who recognize
themselves as having known one another
forever, that is from the beginning, not just in
the past but in a period before there was a past,
before history."8 They discover sexuality at the
same time as they enter the social world, which
is a complementary discovery marked by marriage
"as if the sexual and social are to legitimize
one another." But there is trouble in the
paradise of marriage: "call it its impotence to
domesticate sexuality without discouraging it,
or its stupidity in the face of the riddle of intimacy."
Disappointment engenders a desire for
revenge in which the woman leaves the man
for a simpler soul who is a father substitute.
Archetypically, Cary Grant is traded for Ralph Bellamy.

</file>

<file= AmE06_J62>

I STILL REMEMBER THE DAY a colleague at art school told
me about Donald Judd's compound in West Texas.
Already being a fan of Judd's work, though not
knowing much about him, I had imagined it as my
friend told me: that somewhere in the desert, under
lock and key, two giant hangar-sized structures were
filled with Judd's perfectly austere, milled aluminium
objects, and that Judd himself would visit them
alone to commune with them, fantastic. I decided right then and there that I would have to
see these things for myself one day. Last year I finally had the opportunity to join the ranks of
Judd pilgrims when I went to Marfa for the opening of an exhibition by the artists Elmgreen
& Dragset. Among the many wonderful discoveries I made during my time there, one of the
more bizarre ones, was the real estate feeding frenzy underway in both the one-horse town of
Marfa (population 2,500) and the surrounding counties  -  well, at least rumours of one anyway. During my brief visit, it was hard to avoid talk
along the lines of 'That one bedroom shack that was
$20,000 a few years ago is on the market for $200,000
now' or 'People in Manhattan and LA are snapping up
properties sight unseen!' Not vast sums of money compared
to east or west coast real estate prices, but certainly
worth paying attention to, given the context.
Being an amateur student of real estate, at some
point I suppose you'd have to pay attention to the evidence
that, when artists occupy depreciated or unwanted
property, money often follows. Today, some of the most
expensive real-estate in Manhattan can be found in
SoHo and Tribeca. During the late 60s however, these
neighbourhoods were neglected slums where artists
could afford to pay the dirt-cheap rents. Judd, for example,
was an early pioneer of SoHo where in 1968 he
bought a classic five-storey building at 101 Spring St for
$68,000. In London, areas such as Hackney and
Shoreditch have developed along similar lines (see
AM246). Not so long ago even blue chip real-estate markets
like the Hamptons were inexpensive alternatives for
artists who couldn't afford to summer in more fashionable
locations. And then there are the Guggenheim Bilbaos
of the world: Frank Gehry's museum was so
successful, that developers and urban planners the
world over (who refer to the phenomenon as the 'Bilbao
effect') use it as a model for revitalising economically
depressed, metropolitan communities. Like it or not,
history will tell you that art and artists attract people,
who in turn attract business.
Marfa Texas, home to Judd's sprawling museum, is a
speck of a town in the middle of nowhere. So it's a little
difficult to entertain the notion that creative people from
all over the place are moving there just so they can exist
within close proximity to the art. Founded in 1883, Marfa was little more than a watering stop on the railroad
route from El Paso to San Antonio. Legend has it
that a railroad engineer's wife named it after a character
from The Brothers Karamazov by Dostoyevsky. Originally
known as Tank Town, some 60 miles from the Mexican
border, it is set on a plateau a mile above sea level and
ringed by mountain ranges. As early as 1911 the US government
sent soldiers to Marfa in response to the Mexican
uprising. Eventually this outpost, little more than
barracks and artillery sheds, became Ft. DA Russell.
Because the land was so cheap, the site suited his purposes,
and because he loved the landscape, Judd bought
the entire site in the 70s and transformed it into the
Chinati Foundation (named after a nearby mountain
range). Judd also went on to buy up half the town, three
ranches totalling more than 40,000 acres, and the
Kingston Hot Springs near the Rio Grande  -  which
evidently had been used for 200 years by the locals until
Judd closed it to the public. It is not entirely clear when
Judd officially moved to Marfa, though many people
agree that it coincided with the divorce from his wife in
1976. During the years preceding his move, Judd had
refined his enormously influential concepts about integrating
art with the space around it: 'that the building be
useful for living and working and more importantly,
more definitely, be a space in which to install work of
mine and others'. At some point around the time of his
divorce it became clear to Judd that what he had created
at his building in SoHo was a blueprint for a larger scale
project that required more space. He had also become
disenchanted with the New York art world and the constraints
of the commercial gallery system describing it as
a 'harsh and glib situation'. And perhaps more than anything,
Judd was irritated by the distractions of life as a
famous artist. 
Many people who have moved to Marfa recently and
who are responsible for the buzzy type of attention it has
been getting, did so partly for the same reasons Judd did
and partly not. The town itself is charming enough, has
a pinkish, second empire style courthouse in the middle,
a bunch of new contemporary art galleries, a world class
bookstore, a place called the Brown Recluse where they
roast fair trade coffee beans on the premises, a hotel
where the cast of the movie Giant stayed in the 50s (the
film was shot in Valentine up the road), a restaurant, an
old wool and mohair factory stocked with John Chamberlain
sculptures, and a pizza place appropriately
named the Pizza Foundation. All of the renovations are
subdued and in keeping with the town's overall aesthetic.
The original character of Marfa, architecturally speaking,
appears well preserved if not improved. And
strolling around town there is always the palpable whiff
of authentic Americana. The most persuasive draws,
though, are the surrounding geography. The arid,
mountainous desert, the wide open spaces, the enormous
skies, the alien sunsets that make you understand
the Dan Flavin sculptures, the spectacular canopy of
stars at night, and the sense of freedom and possibility
inextricably wrapped up in romantic ideas about the
frontier West. The dramatic contrast to urban environments
is attractive. Its inaccessibility (the nearest pharmacy
is a two-hour drive) shields it, at least for the time
being, from the rest of homogenised America  -  the Wal-
Marts and the Starbucks.
The recent migration of people to Marfa is typified
by Nick Perry and his wife, both painters, who moved
from New York three years ago. 'We first came here to see the collection and really loved the town.' Perry, who
works at the Chinati Foundation as coordinator for education,
says that they both wanted a larger place to live
and work, but that the realities of being artists in NY
meant not being able to foster those type of dreams.
When asked if the town has suffered under the current
media scrutiny responsible for labelling Marfa 'The
Hamptons of Texas' or 'The Texas Palm Springs', Perry
says: 'Towns evolve. Regardless of how much the town
expands, barring sky-scrapers marring the view  -  the
collection will always be protected by its literal position
in the landscape. A lot of artists have moved here for
the landscape alone and could care less about the Chinati
Foundation.' Perry also informs me that a recording
studio is currently being built, that the town
philosophers meet at the Dairy Queen, and that the
owners of the restaurant Maya's have built a loft-style
Montessori school for the new generation of kids growing
up in Marfa.

Boyd Elder, artist, long-time collaborator with rock
band The Eagles, and sometime resident of Marfa actually
moved there before Judd did. 'When I heard Don
was moving here it was exciting. Our kids were about
the same age, and I was looking forward to having
someone around I could talk about art with. Of course,
we never talked about art, just sleazy gallery owners and
dealers in Manhattan.' Elder has long-time ties to that
part of West Texas, his great-grandfather having done
business with local ranchers. When asked if the recent
developments pose a threat, Elder says: 'It's not a danger
it's an attraction. Ballroom Marfa and the Marfa Book
Co have really contributed to the culture and increased
its intrigue. Before these kinds of people moved here,
the community was depressed. Now it's a whole conglomerate
of fascinating characters, because you know,
the extremes of living here, it's a far-fetched notion. I
believe that art creates a magnet.'
Ballroom Marfa, a non-profit gallery whose inaugural
exhibition was in October 2003, has contributed a lot to
the town's evolving identity. Its current show 'You Are
Here' features an impressive roster of mainly young,
New York artists. Founded by Virginia Leberman and
Fairfax Dorn, Ballroom is also a centre for film screenings
and music. Having moved to Marfa from Manhattan,
Dorn says: 'We thought there was space for more to
happen here ... people are getting tired of the financial
pressures and social distractions of urban life. I feel
lucky to live here.' September of this year sees Ballroom
Marfa building a semi-permanent drive-in just west of
town where MoMA New York is curating the film programme.
United Artists Ltd is also a project space that
opened its doors in October 2005. It was created by New
York-based artist Michael Phelan after he was invited to
show work at Ballroom. Similar in spirit to Andrea Zittel's
High Desert Test Sites, UAL 'is structured by a single
method: three artists and a writer are invited four
times a year to Marfa where they meet, install work, and
spend the week under one roof  -  hanging out, conversing,
exchanging ideas, and of course, drinking lots of
tequila. The first project included Carol Bove, Adam
McEwen and Seth Price. Over the course of the week
conversation included Bob Dylan, the Marfa lights, John Chamberlain's The Secret Life of Hernando Cortez, DMT
and the elves, Joan Didion, best Neil Young song, worst
artists with the best careers, and which artists' work we
would like to see under one roof.'
There seems to be a small contingent in town that
resents people who are discovering Marfa after them  - 
trespassers on their secret slice of the American dream.
Other people display a worshipful reverence for Judd, forgetting
that Judd, as magnificent as his Chinati Foundation
is, isn't the only visionary either historically or in
recent times to have utilised the desert's raw potential. A
couple of states over in Arizona for example, there's
Frank Lloyd Wright's Taliesin West, James Turrell's
Roden Crater, Paolo Soleri's Arcosanti; and Jeff Bezos,
the Amazon.com billionaire, has recently bought
239,000 acres of land north of Marfa where he intends
to build a spaceport. Artists have been flocking to places
like Taos in New Mexico for the light, the beauty, and the
lack of consumerist culture for years. Since places like
that are now dominated by wealthy retirees, it makes
sense for artists with little capital to jump at the opportunity
to get in on something before it becomes too expensive
for them. Another worry is that Marfa will become
'an artists' colony in a kitschy sort of way  -  one souvenir
shop next to another' in the words of Marianne Stockebrand,
former head of the Chinati Foundation and Judd's
lover has said. Craig Rember, the collections manager for
the Judd Institute, remarked: 'I appreciate the new developments
in Marfa and don't really see them ending soon.
Every time a new project seems to be the last big thing,
something else comes along.' It's true that some of the
locals have been squeezed out. The town's biggest
employer is border patrol (when Judd first moved there
he was the biggest employer), and most of these locals
are moving out because they can't afford to park their
land yachts in town anymore, although most people who
live in Marfa will tell you that a lot of the jacked up property
isn't actually selling.

</file>

<file= AmE06_J63>

Introduction: Thinking Skilfully.
Try for a moment to imagine a field of human endeavour that does not require reasoning. This is likely to be impossible as there is little that human beings do that does not involve our ability to think. Many philosophers over the years have postulated that if there is anything essential to the human being, it is the capacity for reason. Indeed, thought is so fundamental to the human experience that it is only rarely that one is without it. The operations of the mind come so naturally and so easily that one scarcely realizes that they are going on. It is perhaps for this reason that the notion of thinking skilfully may seem strange. The goal of this book is to consider thought as an activity, as an act or series of acts that a person deliberately, intentionally and wilfully undertakes. In simpler terms, the present purpose is to think about thinking. Close attention will be paid to the process of thought with the aim of evaluating reasoning and cultivating good thinking skills. This project falls within the confines of logic , which is the branch of philosophy that studies the reasoning process and seeks to understand the differences between good and bad reasoning . Human beings naturally reason well when they take the time to do so, and if their attention is properly directed, they are, in the vast majority of cases, capable of great insight. If there is one fundamental difference between a person who is an incisive critical thinker and one who is not, it is that the critical thinker takes the process of thinking seriously, consciously attends to that process and asks the right questions. The focus, therefore, will be on the concerns that one should keep in the forefront of the mind and on which questions should be asked. It is also worth noting that this book does not introduce much that the average person does not already do. Everyone has carried out all of the described reasoning activities countless times. What the book offers is a careful cataloguing of the various types of reasoning with a discussion of which of them are reliable and which are not. It is, in a sense, a guided tour of the human capacity for reasoning along with instructions for its use. With most human activities the development of skills does not come without practice, and it is no different with thinking. It is probably unreasonable to expect them to come easily, but it is almost certain that time and effort will pay a hefty dividend. Considering the length of this book, its scope is perhaps somewhat larger than it should be. The idea is to present the basics of critical thinking and informal logic and to point the way towards further study, so it is perhaps best to consider the book as a primer. It will provide a solid introduction to the fundamental concepts and considerations as well as links to other resources on the web and in print.
Chapter 1: The Basics.
Before delving into definitions of the basic elements of critical thinking, a brief overview of the structure of these elements will perhaps be helpful. The discussion offered in this book will focus primarily on the argument , which itself consists of at least two propositions . All arguments are either deductive or inductive , and an understanding of this distinction is required for criticism. The success of deductive arguments is evaluated in terms of what philosophers call validity and soundness , while inductive arguments are rated along a spectrum from weak to strong. Many of these words are familiar to most, but it is likely that they are associated with a number of different possible meanings. To remove these potential ambiguities, precise definitions will be introduced. The importance of a thorough familiarity with these concepts cannot be overstated.
Propositions.
The precise nature of propositions is a matter of some philosophical debate, but for present purposes, it will suffice to define a proposition as a claim or assertion that affirms or denies that something is the case . All propositions are either true or false, and no proposition can be both true and false. Furthermore, they are the only sort of thing that can properly be called true or false. Put simply, propositions are the sole bearers of truth and falsehood, and as will become clear shortly, this feature is of crucial importance for identifying them in ordinary language. Here are some examples of propositions.
All triangles have three sides. 
Either George W. Bush won the U.S. election, or John Kerry won it. 
People ought not to lie.
If today is Wednesday, then tomorrow is Thursday. 
All circles are squares. 
The majority of propositions that one encounters come in the form of a declarative sentence, but it is important to note that a proposition is not identical to the sentence that expresses it. A proposition is that to which a declarative sentence refers. For this reason, multiple sentences may express or refer to the same proposition. George W. Bush won the U.S. election. The U.S. election was won by George W. Bush. George W. Bush was the winner of the U.S. election. It is perhaps helpful to think of a declarative sentence as pointing to a kind of abstract object. It is this sort of object that philosophers have termed a proposition. Non-propositional language: exclamations, commands, and questions There are many uses of language that do not express propositions. As noted above, all propositions are either true or false, so whether a particular phrase or sentence can be considered to be true or false will determine if it indeed expresses a proposition. Consider the following.
What are we doing here? To decide whether this question expresses a proposition, simply ask whether it makes sense to say that, 'what are we doing here?' is either true or false. Clearly it does not, and just as clearly this sentence does not express a proposition. Another tactic is to add, 'It is the case that...' in front of the phrase in question and consider whether this new construction makes sense. If it does, then it is very likely that the phrase expresses a proposition. If it does not, then it is likely a non-propositional use of language. Consider. It is the case that, 'what are we doing here.' This is, of course, nonsensical. By contrast, consider the following propositions.
It is the case that, 'all triangles have three sides.' It is the case that, 'either George W. Bush won the U.S. election, or John Kerry won it. 'It is the case that,' people ought not to lie.' It is the case that, 'if today is Wednesday, then tomorrow is Thursday.' The concern here is not whether the constructed sentence is true or false but whether it makes sense. Consider a false proposition. It is the case that, 'all circles are squares.' Obviously this claim is false, but it does make sense, and thus the phrase 'all circles are squares' expresses a proposition. Other non-propositional uses of language include commands and exclamations. Go to bed. Oh, no! A quick consideration of each of these constructions reveals that they are neither true nor false and thus do not express propositions. Note that the following do not make sense.

It is the case that, 'go to bed.' 
It is the case that, 'oh, no!' 

This provides the basis for a general rule. Questions, commands, and exclama- tions do not express propositions . They are non-propositional uses of language. Questions of interpretation: rhetorical questions as assertions and commands as normative claims It should not come as a surprise that there are exceptions to this rule. In some cases certain questions or commands can be interpreted as actually expressing propositional language. A rhetorical question is a question that is meant to have an obvious answer and can be interpreted and reformulated as a declarative sentence that expresses a proposition . Consider the meaning of the question 'who won the 2007 election?' in the following.
Marie: I think it is clear that France is moving to the right politically. Thomas: I'm not so sure. Marie: And who won the 2007 election? Thomas: It's a good point, but I am still not entirely convinced. In this dialogue, Marie is not really wondering who won the French election in 2007. She is, rather, asking a question that is entirely rhetorical. The meaning of this phrase should be interpreted as 'Nicolas Sarkozy won the 2007 French election' which is clearly a propositional use of language. Like rhetorical questions, it is sometimes possible to interpret and reformu- late a command as a proposition. Consider. Don't lie. In some contexts, this command is simply a non-propositional command that is neither true nor false. In others, however, it can be interpreted as a normative claim that contains 'ought' or 'should.' If this is the case, then it can be refor- mulated as one of the following declarative sentences, both of which express a proposition. You should not lie. You ought not to lie.
Unfortunately, the correct interpretation of language is seldom a straight-forward task. It requires constant and careful thought about what the author or speaker actually means. Rhetorical questions and commands that express normative claims are fairly common, and when critically analyzing an argument, it is important to recognize them and to reformulate them as clear and concise declarative sentences. Simple and compound propositions A simple proposition makes only one claim or assertion , and a compound proposition contains two or more simple propositions . There are three distinct types of compound propositions, and it is important both to recognize them and to understand the conditions under which they are true or false (truth conditions).
The first type is a conjunctive proposition, which consists of two or more simple propositions that are connected by the conjunctions 'and' or 'but.' While these two words carry somewhat different connotations, they have the same truth value, so for present purposes, they will be treated as synonyms. Here are some examples of conjunctive propositions. Nicolas Sarkozy won the French election, and Segolene Royal lost. All triangles have three sides, and all circles are squares, but all squares have four sides. A conjunctive proposition asserts each of its simple parts, so such propositions are true just in case all of the simple propositions that it contains are themselves true. If one or more of the constitutive simple propositions is false, then the entire conjunctive proposition is false. The first example above is true because both simple propositions are, in fact, true claims. With respect to the second, while it is the case that all triangles have three sides and that all squares have four, it is not the case that all circles are squares. Thus, the second example is false.
Disjunctive propositions. 
The second type of compound proposition is a disjunctive proposition . Such propositions contain two or more propositions that are connected by 'or.' There are actually two possible meanings of the word 'or' and for reasons that will become clear later, it is important to carefully distinguish between the two. The 'inclusive or' means 'one or the other or both' while the 'exclusive or' means 'one or the other but not both.' In this book, the inclusive meaning will be assumed, and any use of the exclusive will be noted. Following are some examples of disjunctive propositions. Either George W. Bush won the US election, or John Kerry won it. Either George W. Bush is the US President, or Nicolas Sarkozy is the French President. All circles are squares, or all triangles have three sides. A disjunctive proposition is false if and only if all of the constitutive simple propositions are false. If one or more of them are true, then the entire disjunctive proposition is true. Consider now an implication of these truth conditions.
Assume that the following disjunctive proposition is true. 
All gadgets are widgets, or all widgets are sprockets.

</file>

<file= AmE06_J64>

During the last two decades of the nineteenth century, the fair-haired
boy of serious American music was Edward MacDowell, the first native
composer to become internationally famous. Only twenty-one when he
was praised by Liszt, his works were published soon afterward and
frequently performed in both Europe and America. Theodor Thomas,
Arthur Nikisch, Anton Seidl, and other important conductors added
MacDowell to their repertoire, as did Ignacy Paderewski, Teresa
Carren˜o, and many of the leading pianists of the era. The most influential
American critics, including James Gibbons Huneker, Philip Hale,
and Henry T. Finck, acclaimed him as the country's chief contributor to
music. Joachim Raff, Jules Massenet, and Edvard Grieg applauded him,
and young American composers took pride in his stunning success and
hoped to emulate him. His breakdown at age forty-five (he was born in
1860, not 1861) and his illness of almost three years was a major blow
to American music.
It was not surprising that a biography of MacDowell, about whom
so much had been written, appeared less than a year after his death in
1908. But what is surprising is that no thorough, musically authoritative,
balanced biography has yet appeared. Edward MacDowell: A Study
(1908), by the then young music critic Lawrence Gilman (1878 - 1939),
does not pretend to be a full biography, as the title indicates. Only
about half of its fewer than two hundred small pages is strictly biographical
and some of it, including MacDowell's birthdate and illness, is
inaccurate. The second half of the book, dealing with MacDowell's
work, is musically sound though dated, but it has been criticized as
being too favorable to MacDowell and unfair in its comments on other
American composers of the period. Alan H. Levy's Edward MacDowell:
An American Master (1998) the most recent biography of the
composer, is a little longer and contains a fuller picture of his personality
and his tribulations with Columbia University, but it, too, is not free of errors and is the work of a cultural historian rather than a musicologist.
The definitive, critical biography of MacDowell remains to be written.
"In years to come," Marian Nevins MacDowell wrote her husband's
closest friend, composer Templeton Strong, on 4 June 1914, "someone
may think it worth while to gather material for a new life [of
MacDowell]." She herself wrote her memoirs and carefully saved papers
and other memorabilia indispensable for a thorough biography of her
husband, but she refused to make them available, during her lifetime, to
several potential biographers (including John Erskine) who enlisted her
cooperation. After her death in 1956, her archives passed into the
hands of her longtime companion and foster-daughter, Nina Maud
Richardson, who, she had hoped, would undertake the biography of
herself and MacDowell. Perhaps Miss Richardson intended to at one
time, but realizing eventually that she had neither the training nor the
strength for such a project, and unwilling, I think, to reveal certain facts
about MacDowell, she gave much of the material to the Library of
Congress. (Several years after her death in 1969, the Library acquired
the rest of it.)
In 1965, having completed a biography of MacDowell's friend and
critical champion, James Huneker, about whom I had corresponded
briefly with Mrs. MacDowell not long before she died, I thought of
writing a biography of MacDowell and then of his widow, but eventually
abandoned both projects. But I did write a less ambitious, still unpublished,
study of MacDowell and Huneker. While revising the manuscript,
I suddenly realized why there had been no adequate biography of
MacDowell. Now, after extensive research, I am convinced that he was a
victim of syphilis and that Mrs. MacDowell wanted to bury the fact with
her husband.
In all but one of the published accounts of MacDowell's mental and
physical breakdown that I have seen, the breakdown is described
vaguely and mysteriously. Gilman writes: "There were indications of an
obscure brain lesion, baffling but sinister. . . . A disintegration of the
brain-tissues had begun - an affection [sic] to which specialists hesitated
to give a precise name, but which they recognized as incurable."
According to Gilman, MacDowell "began to manifest alarming signs of
nervous exhaustion" in the spring of 1905. Gilman says nothing about
a horse-cab accident, which MacDowell's friend, the writer Hamlin
Garland, mentioned in his diary in the fall of 1904: "The immediate
contributing cause of his breakdown was a fall in the street - or rather an accident in the street. He was struck by a cab, so his wife tells me,
and the concussion stunned him so that he was dazed for some time. He
was able to walk home, but this shock, coming upon him at a time
when he was worn with his teaching and suffering from his controversy
with Columbia [University], undoubtedly caused the collapse."
(MacDowell had resigned from Columbia early in 1904 and had engaged
in public recrimination with president Nicholas Murray Butler and the
trustees.) Yet when Garland issued a press release about MacDowell's
condition in November 1905, he did not refer to any accident.
MacDowell himself wrote to Templeton Strong on 4 January 1904,
that he had strained his right wrist in a "street car collision"; but if this
was the accident that bulked large in hindsight, it did not seem very
serious to MacDowell at the time, for he added, in the same letter,
"we are in fine health and spirits." Though she did not date it,
Mrs. MacDowell supplied details about this accident (or another one?)
in her unpublished memoirs, written long afterwards. It happened, she
recalled, when MacDowell was escorting his parents to a streetcar, and
she included the important point that the wheels of the cab crossed his
spine. In an unpublished letter to Mrs. Robert Underwood Johnson,
written on 19 June 1905, Mrs. MacDowell mentioned the "accidents" to
which "the doctors are beginning to attach much importance"; but what
they were - or what the other one was - she does not say. Margery
Morgan Lowens, author of an excellent doctoral dissertation on
MacDowell, tried to find police records of an accident involving
MacDowell but was unsuccessful. On 4 October 1905, it was probably
Henry T. Finck, music critic of the New York Evening Post and an old
friend and admirer of MacDowell (Gilman's book is dedicated to him),
who gave the first public account of the horse-cab accident: "A year ago
last spring, Mr. MacDowell, while crossing the street one evening, was
run down by a cab; the horse had slipped on the sleet, and the driver
was unable to check it. The wheels passed over the composer's neck,
and evidently injured the spinal nerves." Another old friend of
MacDowell's, T. P. Currier, confirmed the incident but dated it in the
spring of 1905 rather than 1904. An accident could have played some
part in MacDowell's illness, but it was not the main cause.
In November 1905, Garland stated that MacDowell's doctor, an
"eminent specialist" (his name was not given) had concluded that the
composer's "oversensitive, highly wrought brain [had burned] itself away
in overwork." Such an opinion is medically dubious, but overwork at
Columbia was, indeed, the cause publicly pinpointed by Mrs.
MacDowell. The first book on MacDowell to appear after Gilman's,
Elizabeth Fry Page's slight, sentimental 1910 panegyric, Edward MacDowell: His Work and Ideals, also attributes his death to "a mental
collapse brought on by overwork." That this was not the real cause
was suggested much later by John Erskine, who reported that those who
had studied with MacDowell at Columbia could not understand his
breakdown "in spite of explanations offered by Hamlin Garland and
others."
The only published account I know of which hinted that
MacDowell had syphilis was James Huneker's. In his moving article on
"The Passing of Edward MacDowell," written eighteen months before
MacDowell died, and published in the New York Herald on 24 June
1906, Huneker seems at first to accept overwork as the reason for
MacDowell's breakdown. The composer was "reckless of the most precious
part of him, his brain," Huneker writes. "He killed that organ by
overwork. . . . He burned away the delicate neurons of the cortical cells."
Then comes this sentence: "He suffers from aphasia and locomotor ataxia
[italics mine] has begun to manifest itself." Elsewhere in the article, he
makes two allusions intended, no doubt, for the cognoscenti. Referring to
MacDowell's piano teacher, Karl Heymann, Huneker remarks that
Heymann "went quite mad and died young in an asylum." And describing
MacDowell's symptoms, he reports that
he reads slowly and, like the unfortunate Friedrich Nietzsche, he rereads
one page many times. I could not help recalling what Mrs. Elizabeth
Foerster-Nietzsche told me in Weimar of her brother. One day, noticing
that she silently wept, the poet philosopher exclaimed: "But why do you
weep, little sister? Are we not very happy?" MacDowell is very happy and
his wife is braver than Nietzsche's sister.
Huneker was far more interested in and knowledgeable about the
sex life of artists than most Americans were in 1906. He had read the
studies of his friend Havelock Ellis and those of other sexologists, had
gossiped about the sexual peccadilloes of celebrities, and later (in 1913)
even wrote a newspaper article about the latest research in the treatment
of syphilis. As the Musical Courier's "Raconteur" from February
1888 to October 1902, he had probably retailed in his column more
scandals about musicians than any man in America. He had heard, I
suspect, that Heymann was a victim of syphilis.
The source of his information is unknown, but considering how
little has yet come to light about Heymann after MacDowell stopped
studying with him in 1880 or 1881, one is not surprised that Huneker's
statement about him is largely incorrect: Heymann did not die young,
nor did he die in an insane asylum; according to his death certificate (which does not list the cause of death), he died on 5 December 1922,
at the age of sixty-nine in a hospital in Haarlem, the Netherlands.
This does not mean, of course, that he might not have spent time in an
asylum. Gilman states that by 1881, when Heymann left the Frankfurt
Conservatory, he "had already begun to show symptoms of the mental
disorder which ultimately overcame him." He may have been under
care by early May 1884, when MacDowell visited him in Wiesbaden.
Thereafter Heymann seems to disappear, and he could have been in an
institution. After MacDowell himself broke down, his nurse Anna Baetz
recorded in her diary that he used to speak of Heymann as being "in a
madhouse," a fact which, Baetz added, Mrs. MacDowell confirmed.
Huneker, at any rate, apparently discovered his mistake, for when he
reprinted the Herald article in his book Unicorns (1917), he deleted the
reference to Heymann.
With Nietzsche, whose life has been studied thoroughly, Huneker
was on firmer ground. He probably knew by 1906 that Nietzsche's insanity
and death were caused by syphilis. In the one essay on Nietzsche he
had already published (and, indeed, in the two he wrote afterward)
Huneker, to be sure, did not state that Nietzsche had syphilis; overwork
or eye trouble were the causes he seemed to accept. He had even
described as "careful and convincing" a now discredited study of
Nietzsche's illness which appeared early in 1904, by Dr. George
M. Gould, the well-known ophthalmologist, who maintained that bad
eyesight was responsible for Nietzsche's ill-health and denounced the
"silly and nauseating attempt of Moebius to fasten upon [him] the
stigma of . . . syphilis, a charge utterly without justification, medically,
scientifically, individually, or socially." But Huneker did not really
agree with Gould - or perhaps he changed his mind. "You don't suppose
that I would be allowed to print the truth about Nietzsche!!!" he
remarked in a letter to Benjamin De Casseres on 21 March 1904, after
his first study of the philosopher had been reprinted. And in an
unpublished letter dated 12 June 1914, Huneker told his friend John
Quinn, the art patron, that the Nietzsche case was "a classic one.

</file>

<file= AmE06_J65>

A WIDE VARIETY OF retail settings across the nation, children's books are so ubiquitous as to be nearly invisible. Perceived by many adults as an important aspect of child culture, they are taken for granted. Some of these books have proved remarkably durable, continuing over the years to delight thousands-probably millions-of children. The books of E. B. White, Kate Douglas Wiggin, Kenneth Grahame, Laura Ingalls Wilder, J. R. R. Tolkien, Dr. Seuss, and others remain reassuringly constant, having attained the mysterious stature of "classic" that denotes an imprecise combination of a book's age, profit history, and "timelessness."

Less ambiguous than the definition of a classic children's book, however, is the fact that the authors and illustrators who created them were(and are) supported by individuals, with contingent interests in the book publishing enterprise, who help ensure the success of books by producing, promoting, and evaluating them. Historically, such individuals considered themselves "bookmen," denoting a passionate devotion to and substantial knowledge of books. This study considers a group of female bookmenI call them "bookwomen"-whose persistent and innovative efforts helped to shape the specific economic and cultural niche of the modern children's book industry between 1919 and 1939. The group consists of two librarians, Anne Carroll Moore and Alice Jordan; two editors, Louise Seaman Bechtel and May Massee; and the two founders of the important children's periodical the Horn Book, Bertha Mahony and Elinor Whitney Field.

The institutions bookwomen represented were situated in a complex historical moment that is particularly rich for investigation, in part because of widespread contemporary concerns about children. A growing number of self-identified experts had sustained their agitation on behalf of "good" books for children for several decades in the late nineteenth century. While their efforts had been far from successful, since young people did not discontinue reading serial and pulp fiction in favor of books with more erudite and uplifting themes, the campaign on behalf of children was in full swing by 1919. It was framed by a view of childhood, first, as a distinct time of life with unique social and cultural requirements, and second, as an inherent right in need of protection. These concepts, and the reforms stemming from them, represented one way in which America's more affluent citizens articulated their vision of the world, simultaneously satisfying a culture-wide demand for expertise and specialization in the early twentieth century. Bookwomen were part of the advance guard of the crusade to ensure that children received what they regarded as the best reading material possible, affirmed in their efforts both by personal conviction and by discursive communities within their institutions. So affirmed, bookwomen engaged in a variety of activities that exerted substantial influence over the institutions in which they worked and the book-buying public.

Yet, much more was at stake for bookwomen than the quality of children's books. However important that was to them, they were also interested in building their own careers. This study, therefore, investigates ways in which bookwomen helped to alter permanently not only the infrastructure supporting the production of children's books but also the American workplace for women during the early twentieth century. In some instances, bookwomen achieved professional identity by seeking admission to professions heretofore closed to them; in others, by expandingor transgressing-boundaries of careers already open to women. In either case, their strategies are significant markers in understanding women's relationships to each other and to society.

The debate over women in the American workplace had stretched from the nineteenth into the twentieth century, but during the period between the two world wars it intensified, kept alive in periodicals prominently featuring articles warning the public about the dangers of women in the workforce. A repertoire of explanations for this existed: working women were more likely to bear unhealthy children, working women might consciously limit their family size, home life would suffer. During the 1920s, middle-class periodicals like Ladies' Home Journal and Harper's routinely described the social consequences of working, urging women to surrender their career aspirations and return to traditional domestic responsibilities. Themes of imperiled physical health were expanded to suggest that women who chose work over traditional home life were "unnatural." For married women especially, the message was clear: a woman must choose between a family and a career.

In the midst of this debate, however, women were achieving higher educational levels than ever, and new professional opportunities for them emerged. Editorship, to which a substantial part of this study relates, provides one example. Driven largely by a proliferation of print and specialized knowledge that necessitated area-specific experts, the appearance of the modern editing profession near the end of the nineteenth century signaled an important change in the organizational structure of modern publishing. As a new layer of middle management, editorship opened career opportunities within an industry traditionally noted for its tight, often nepotistic, control over meaningful decision making, and women were hard-pressed to gain access to them. In the case of publishing, it had not been unusual to find women in publishing during the seventeenth and eighteenth centuries, but their presence had become increasingly rare during the nineteenth century. A few women, certainly, attained successful careers in publishing, frequently as a result of kinship ties with the publisher, but the industry did not offer real professional opportunities to most, or even many, qualified women until the 1920os, a decade of particular prosperity for the publishing industry.

Despite such gains, it is nonetheless true that educated women in the 1920s were more likely to marry than in the previous generation, resulting in an overall decline of females in the workforce. Moreover, while women pursued college degrees as never before, education did not necessarily translate into careers outside the narrowly defined boundaries of"women's work:" Nearly 40 percent of educated women of the late nineteenth and early twentieth centuries entered teaching, either for long- or short-term employment. Social work was also available, a vocation popular with educated progressive reformers who did not wish to become teachers, along with nursing and librarianship. Thus, women faced a workforce offering severely limited options despite unprecedented levels of education.

Several arguments have evolved to explain women's exodus from the workplace during the 1920s. Primary data drawn from studies about women's career aspirations convinced some mid-twentieth-century historians that the departure was the result of the alleged sexual revolution. Other midcentury historians attributed the diminishing presence of women in the workplace to disillusionment with suffrage and its failure to produce the job satisfaction and sociopolitical changes many women had anticipated. Disappointed, the narrative went, women reverted to traditional female roles as wives and mothers.

More recently, historians have disputed the theory of disillusionment, insisting that it does not fully explain female professional life during the interwar period. The result of seeking more detailed explanations about who stayed in careers why, and how they managed to do so has been a valuable literature examining such topics as the relationship of male professional authority and family claims to women's careers, the impact of professional associations and mentoring on career women, and the sometimes precarious balance between the old female ideal of public service and the new ideal of professionalism.2 This study contributes to that dialogue.

As a corollary, this study also engages the longstanding issue of professional ghettoization. It is clear enough that entering child-centered careers was frequently an evolutionary process rather than one of forethought and decision; women were sometimes channeled into careers they did not necessarily desire. Several bookwomen would have preferred careers unrelated to children but considered them unattainable. Instead, they entered child-related careers either because they lacked the financial means to pursue their primary interests or because they were "guided" into them by the cultural institutions within which they worked. Evidence dealing with bookwomen suggests that, in the late nineteenth and early twentieth centuries, career-minded women were most likely to achieve and maintain professional success when their careers accommodated the prevailing social belief that women possessed special nurturing qualities and an innate knowledge of children. As nurses, teachers, social workers, and librarians, women generally found the comfort of social approval, and the early careers of the bookwomen in this study reflect the effectiveness of this strategy.

To accept the traditional narrative that women were merely forced into unwanted careers, however, simplifies a complex phenomenon. An examination of bookwomen contradicts the notion that child-helping careers necessarily led to a professional "dead end." By organizing their professional lives around children and cooperating with generally accepted beliefs about "women's work," bookwomen were rewarded with relative (and highly coveted) autonomy within their work environments, a crucial precondition for genuine professional authority. Authority, however, also derived from "pioneership," a concept with deep resonance in American culture and not necessarily easy to claim in child-centered careers. That authority rested both in adherence to tradition and in claims to "firsthood" is highly significant to the story, creating tensions difficult and often impossible to reconcile.

While it seems unlikely that bookwomen could have anticipated the impact of their career strategies on expanded options for future generations of women, their willingness to mentor younger colleagues suggests a level of awareness of their significance to professional women. Recognizing that professional survival was linked to identity creation, bookwomen actively participated in shaping their unique place among child experts rather than passively accepting widespread notions of "women's work" or roles as pawns of ill-defined "market forces." By consistently expanding the boundaries of their careers, becoming active agents in market creation, participating in establishing the standards, credentials, and rewards defining their professions, and promoting new talent in the field, bookwomen simultaneously reinforced the importance of their roles in the literary world. As critics, bookwomen enhanced the visibility of women in literary careers. As authors, they had an impressive publishing record, ranging from fantasy to literary criticism. Five are the authors of what Betsy Hearn and Christine Jenkins have called "sacred texts," the canon of modern children's literature.

The process of creating or augmenting professional identity relied partially upon appropriating traditional material and cultural structures, such as publishing firms, libraries, literary criticism, and book reviewing, and ownership of bookshops and printing presses. Once on the inside of the well-established and well-organized apparatus of book production, bookwomen interacted with it in innovative ways, exercising liberty to move outside existing publishing structures whenever those structures proved inadequate to their purposes. Unlike their female counterparts in what Robyn Muncy has called the political "dominion" of children's work, bookwomen were not constrained by legally mandated lines of authority. The luxury of such fluidity allowed them to reconfigure space, both literally and figuratively, in which to negotiate their authority. One specific consequence of this fluidity was the establishment of the Horn Book, a periodical without precedent in American publishing.

Founded in 1924, and ostensibly a "children's magazine;' the Horn Book became the fulcrum for bookwomen's community of practice and a critical site of affirmation for them at a time when no other such forum existed.

As the Horn Book's founding generation, they dedicated the magazine to discussing struggles and celebrating achievements among themselves and among those striving for literary success as children's authors, illustrators, and editors. As a critical component in the professionalization of bookwomen at the time under investigation, therefore, the Horn Book figures prominently in this study.

In the 193os, pressure on women to abandon their careers intensified. The impact of the Great Depression on female employment is indisputable, but additional factors help to explain why some women lost ground in the workplace during this decade. Basing claims to authority on inherent female knowledge of children became increasingly precarious throughout the 1920s, relentlessly assaulted by those child experts who preferred scientific certainty to what they considered maternalist platitudes. On the surface, at least, the target of this attack was motherhood, historically linked to instinctual knowledge of children. By extension, however, any woman's claim to special knowledge of children became dubious, including those in professions related to children. "Natural" knowledge, heretofore a typical and fundamental element in professional identity among women, was thus compromised, leaving those professionals, including bookwomen, challenged to clarify the basis of their authority.

</file>

<file= AmE06_J66>

CHAPTER 1.
Our Field of Inquiry.
The history and the philosophy of music are obviously
dependent upon music for their existence, but they are not for
that reason of a lesser value or importance. They are in fact
intrinsic to musical experience in some measure, for it is difficult
to imagine any musical activity that is totally unaccompanied
by conceptual thought, or not in some way guided by theoretical,
aesthetic, and historical notions, or not provocative of some kind
of evaluation and criticism. But if we cannot perceive or think
or act while remaining completely unaware that we are doing
so, reflective thought would appear to be grounded in a central
characteristic of consciousness, and as the exercise of a natural
mental function, philosophy will be pleasurable as well as
inevitable. The pleasure it brings, however, is also due to an
attendant satisfaction of our desire for knowledge and to the
enhancement of our understanding.
Yet how can we justify each successive age and author in
writing philosophy down instead of just engaging in it?
Certainly it is only novel ideas and new values that can logically
claim to be worth recording and preserving. The basis of novelty
is found at once, however, in the distinctive perspective of each
cultural setting and each succeeding vantage point in time, and,
grounded in these, of each individual personality. It can
therefore be expected that musical philosophy will produce a
series of conceptions, which may be considered a counterpart
of the diverse cultural manifestations of music itself, or of the
historical unfolding of its potentialities. As a particular outlook
comes into being, along with corresponding kinds of music, the
conceptions of other times and places will be modified or
discarded in accordance with their relative compatibility to the
ideas that prevail or their contradition of them
But thought and culture build upon the past, and in so
doing they should tend to produce-as they clearly have done
in scientific fields-some kind of progressive insight; new
perspectives should not simply succeed but also somehow
amplify or incorporate older ones, becoming by that token more
complete. In the case of a cultural product such as music-as
opposed to an essentially constant object such as the physical
world-we are dealing not only with an increase in our
knowledge and understanding, but also with a change in and
an enlargement of the very object of study, a change which itself
often contains progressive and cumulative elements. Thus we
can reasonably hope to formulate something more than a
contemporary perspective coordinate with others; our view
should provide a conception of music in many respects more
complete than those of the past. But we cannot expect to secure
an understanding of music that will remain definitive or that
will be complete in an absolute sense. The future obviously
transcends both systematic analysis and past history and the
unpredictable novelty of human creativity is at least as evident
in speculative thought as it is in patterns of behavior or in art.
Thoughout most of its history, philosophic thought
concerning music proceeded from metaphysical, ethical, and
religious principles. The properties of music were conceived in
accordance with these more general concerns. But starting in
the European Rennaisance, increasing attention was given to
the perceptual properties and details of artistic experience. The
discoveries of the history and anthropology of music have
enabled us increasingly to examine a wide range of artistic
experience, so that the foundation of inductive aesthetics has
taken on a new breadth and importance. The aesthetician and
the philosopher of music can no longer content themselves with
an awareness that is limited either in range or in sensitivity.
A corresponding change has taken place in the area of
psychology, where the search for inherent features in human
perception has similarly drawn closer to musical experience.
For it is now not so much the analytic laboratory experiment
that arouses our interest-the investigation of elemental acoustic
events by physics and physiology, or the measurement of
isolated tonal attributes and intervallic responses by
psychophysics-but the study of configurational and cultural
properties. Influenced by phenomenology and Gestalt
psychology, our experimental procedures have turned away to
a great extent.from the examination of abstractly defined
elements and the artificiality of the laboratory task in an effort
to discover the intrinsic organizing capacity of perception.
This capacity, however, is dependent in part upon society
and culture. Indeed not only the configurational properties of
melodic phrases but even the nature and meaning of individual
sounds and tones are partly shaped by cultural forces. It is only
in modern Western civilization, for example-which a century
earlier had invented the analytic procedures of psychophysicsthat electronically generated sound with precisely constructed wave forms could become musical material. The synthesizing
power of perception, then, whether it results in the hearing of a
single tone or of a melodic and harmonic configuration, gives
rise to a compound process in which the inherent and cultural
components cannot be separated. For the effects of cultural
conditioning are essentially the same as those of native
endowment. Both have a corporeal manifestation as part of the
structure and function of the nervous system, and the exercise
of the ability they shape jointly is in both cases accompanied by
a feeling of selfevidence and naturalness. In this respect our
perceptual faculty is no different from our creative capacity,
which selects or devises materials for art and then combines or
forms them into meaningful patterns. Thus the basic data of
psychology-tones, intervals, melodic phrases-will ultimately
be of the same nature as the data of history or anthropology.
Both cultural and psychological studies were strongly influenced by the philosophy of natural science, which reduced history to the isolated specimen of music and psychology to
behaviorism. The musical composition was examined objectively by dissection, while perception, with equal objectivity, was studied by tabulating previouslydefined responses to
simple but essentially meaningless tasks. To be sure, historical
tendencies were responsible for this situation: the decline of
function and genre left the processes of musical evaluation and
understanding without a foundation; the single composition
was exposed to the advances of practitioners of "analysis" and
"aesthetics."
A humanistic history and psychology, on the other hand,
will not take the musical score, the phonograph record, or the
behavior of laboratory subjects as final facts, but will examine
the nature of the auditory objects of music phenomenologically,
as they are given in consciousness. For it is clearly always the
operation of the mind, now on the inchoate matter presented
by history and culture, now on that presented by nature and
human nature, that produces the objects of knowledge. In
psychology, of course, the auditory perceptual objects we study
are generally not examples of music in the full sense of the term;
they tell us more about perception than about music. They point
directly to transhistorical and transcultural features of music
but only indirectly, therefore, to music itself. A further
consideration is called for to determine the extent to which the
elements and configurational properties we have examined are
in fact part of musical experience or relevant to it. Although
psychology and history are fundamentally akin, it can be said
with some justice that the one deals more with the activity of
consciousness and the other more with its objects. We must add
the qualification for psychology, however, that the activity of
consciousness is not only grounded in corporeal experience but
also shaped by social forces; and every abstraction from the
organic totality or from the whole of culture involves the danger
of misunderstanding.
As far as aesthetic objects are concerned, we may consider
them to be modifications of the objects of commonsense
experience. Thus the processes of biological and social
adjustment, are not entirely irrelevent to the understanding of
works of art. It is in our everydayworld that we satisfy our need
for play and for exploration, and the individual objects and
people we learn to deal with often possess aesthetic
attractiveness as a prominent feature. But true aesthetic objects
are those in which aesthetic properties are dominant,although
only by exception does this dominance entirely suppresses
either biological or social functions. An aesthetic object
represents a degree of abstraction from the normal practical
involvement of perception; yet it plays a role in our orientation;
it has its place, and helps us to know our place, both in our
immediate environment of space and time and in our world of
imagination and history. And if we participate in a musical
performance, or attend a recital as a member of the audience,
or even listen alone to a phonograph record, the experience is
in every case structured by social institutions and relationships.
It is clear that objects of every type exist only as
counterparts of human existence. Their properties in general,
their weight, size, color, sound, and feel, like their reactions to
our actions, all have meaning only in relation to human
experience. This is true also as well of the conceptual forms that
accompany our sensorimotor experience, for these also arise
from the intrinsic properties of consciousness and of our sensory
apparatus. Our ability to deal with the physical and biological
world in which we are placed is due to the fact that in the last
analysis we have a similar nature; and even cognition would
seem to rest not only on bodily mobility and sense perception
but also on neurological patterns which are grounded in a
substratum of physical and chemical processes just as are the
events and objects ofthe external world. Similarly, in the very
different realm of historical and cultural understanding, our
knowledge depends upon a basic kinship in experience with
other human beings.
Thus the things we experience and know are colored and
even constructed in great measure by ourselves, both by the
constancies of human nature and by the idiosyncrasies of particular cultures or individuals. This is obviously the case with our
knowledge of people and their creations, but it is true also of
our knowledge of the physical world, In the case of our
knowledge of logic and mathematics, in which the truth of
propositions seems absolutely independent of human existence,
what we learn is no more than the tautologies of thought itself.
It is only art, aside from practical life and engineering, that
is directed at least in a general way, toward objects perceived
by the senses. Yet every field of human endeavor must have
some reference to such objects. History and science, for example,
in spite of the obvious differences between them, are alike in
being centered on objects of a conceptual or theoretical nature.
Science dissolves the world into fields of forces, harmonic
patterns, and equations: imaginative structures which change
in step with man's creativity or with the pressures of the
scientific community. In order to underpin and verify such
objects we must extend our native sensory capabilities by means
of a highly intricate instrumentation which nevertheless
ultimately registers what it detects on charts or in numbers that
are accessible to ordinary vision. History too creates theoretical
patterns of consecution and relationships that differ with each
historian; and it is perhaps even further from any validating
objects of commonsense observation, for the people whose
experiences we seek to narrate are no longer having these
experiences. Yet we depend again, ultimately, upon the
observable clues they have left behind in the form of writing
and artifacts.
Anthropology is more concrete than both science and
history; it provides the presentness of scientific facts without
their abstraction from experience, but the theoretical objective
is present as well, in the project of description and interpretation.
Indeed theoretical formulations will soon occupy the same
dominant position in anthropology that they do in science and
history, for the simple societies that constitute its object of
investigation are rapidly vanishing. Every field of study is
subject to history as well as to the varying interests and concerns
of culture. Even the sensory ingredient of art may take on the
less immediate form of imagined experience, as it does
characteristically in the case of literature; and even in music,
sensational qualities at least occupy a position of lesser
prominence in The Art of the Fugue than they do in the music of
Debussy or Stravinsky.

</file>

<file= AmE06_J67>

In his influential essay "Nationalism and Social Division in Black Arts Poetry of the 1960s," Phillip Brian Harper defines late sixties' black epistemology through opposing structures of unity and discord. Using Amiri Baraka's "SOS" and "Black Art" as models, he argues that the latter, divisive poem trumps its predecessor's general call for racial solidarity, and that this act of superseding necessitates a reexamination of the common conceptions of Black Nationalism within the Black Aesthetic. "[S]ocial division within the black community," he contends, "is fundamentally constitutive of Black Arts nationalism"; Baraka's work, serving as a synecdoche for all subsequent Black Arts poetry, performs these divisions with manifold ferocity (248). 
   Harper modifies this claim with an eye towards sexual and epistemological concerns in his revision of the essay within the collection Are We Not Men? Following a long line of previous critics,1 Harper defines Black Nationalism as an essentially male project, and writes that "insofar as black identity [. . .] depends upon identification specifically as man [. . .] blackness will partake of the very uncertainty, tentativeness, and burden of proof that [. . .] characterize conventional masculinity" (40). Within this patriarchal orientation, Harper establishes two female archetypes: older poets such as Gwendolyn Brooks who make "Baraka's [initial unifying] enterprise [their] own," and younger counterparts such as Sonia Sanchez and Nikki Giovanni who "[wrote] beyond the 'call' manifested in [. . .] 'SOS,'" and espoused the later, polemical Barakan ideals (46). Thus, he divides the movement along sexually encoded ideological lines, with those who simply repeat Baraka's "call" and therefore "succumb to the rhetoric" on one side, and those who incorporate a "phallic standard of political engagement" to actively challenge such rhetoric on the other (43, 52). Harper's comments leave little doubt as to where his aesthetic interests lie, and he dismisses Brooks from the remainder of the essay after providing an oft-quoted passage from her 1972 autobiography, Report from Part One: 
My aim, in my next future, is to write poems that will somehow successfully "call" (see Imamu Baraka's "SOS") all black people, black people in taverns, black people in alleys, black people in gutters, schools, offices, factories, prisons, the consulate; I wish to reach black people in pulpits, black people in mines, on farms, on thrones[.]2 
   Curiously, Harper's quotation omits the final sentence of Brooks's statement: "My newish voice will not be an imitation of the contemporary young black voice, which I so admire, but an extending adaptation of today's G.B. voice" (RPO 183). This omission and other comparable lacunae are problems endemic to the scholarship concerning both Brooks's relationship to the Black Arts movement and, more generally, the position of black women within the patriarchal organizations of Black Power. Houston Baker, for example, uses the same elided quotation to argue that Brooks "cast[s] her lot with the new generation" (108), while Arthur Davis insists that "like many young and middle-aged writers, she [came] under the influence of the Black Aesthetics Movement" (100). In each of these cases, Brooks is figured as a passive agent with respect to Black Arts; she "submits" to its rhetoric, where alternative figures such as Baraka and Giovanni construct and reconstruct its epistemology. 
   The purpose of this essay is to interrogate (and finally to challenge) such suppositions and to recover what "newish voice" Brooks brings to her poetry upon exposure to the younger poets. Examining her three "Sermons on the Warpland," I argue that Brooks actively resists both the simplistic unitary rhetoric of Black Nationalism and the ideologically polemical stance that Harper describes. Shifting her poetry away from the performative (and ultimately coercive) aspects of both positions, Brooks instead employs a rhetoric of ambivalence in her representation of the nascent Black Aesthetic. In so doing, she situates the Black Nationalist movement, and, concomitantly, the black community from which it sprung, as highly contested hermeneutical spaces of inquiry, locations where previously irreconcilable social divisions can be negotiated and redefined. By examining this alternative response to the question of Black Nationalism, we can begin to see Brooks not only as a figure influenced by the Black Arts movement, but, more importantly, as a poet whose work challenges and reconsiders the grounds upon which the movement rests. This study therefore offers a critical alternative to the previous ideological understanding of the Black Arts movement by uncovering a feminine (if not feminist) counterpart to Baraka's masculine aesthetic program. 
"Define and / medicate the whirlwind" 
Critics such as Angela Jackson and Norris B. Clark consider the events of 1967 to be the transformative impetus behind Brooks's interest in the new Black Aesthetic. During the spring of 1967, Brooks attended the Second Writers' Conference at Fisk University, where she met the fiery Baraka and witnessed his galvanizing effect upon the primarily black audience. Shortly thereafter, Brooks befriended Walter Bradford, the man who would introduce her to the Blackstone Rangers and to whom the "Second Sermon" would be dedicated. Late in the year, Brooks facilitated the Rangers' poetry workshops in Chicago, while contemplating the transfer of her own publications from the white-owned Harper and Row to Dudley Randall's new Broadside Press. These decisions illustrate the changes that were taking place in her catholic mind, and, as Brooks's autobiography makes clear, the decisions resulted in a radical shift in her poetic program. Writing just three years later, she would categorize 1967 as the fulcrum about which her poetry turned: "Until 1967 I had sturdy ideas about writing and about writers which I enunciated sturdily. [. . .] Until 1967 my own blackness did not confront me with a shrill spelling of itself" (RPO 73, 84). 
   Published the following year, In the Mecca marks Brooks's first poetic engagement with the "shrill spelling" of blackness.3 In a shrewd analysis of the work, Norris Clark describes the ways in which Brooks's political transformations reproduce themselves in her poetic explorations, offering a sense of the changes in Brooks's epistemological landscape: 
Her emphasis has shifted from a private, internal, and exclusive assessment of the identity crises of twentieth-century persons to a communal, external, and inclusive assessment of the black cultural experience. That change not only corresponds to the fluctuating social, political, and ideological positions of the national black American communities during the sixties and seventies, but it also correlates with the evolution of aesthetic humanism's fundamental concerns about the nature of reality, our relationship to it and its vast variety. 
(84) 
Although Clark separates the socio-political / black and aesthetic / humanist components of Brooks project, when examining the two poems that close the collection - "The Sermon on the Warpland" and the "Second Sermon" - I find the two projects to be inextricably linked. In both poems, Brooks casts "the nature of reality" and "our relationship to it" as necessarily political questions that pertain directly to the diverse meanings ascribed to Black Nationalism. As her epigram to "The Sermon on the Warpland" makes clear, "The fact that we are black / is our ultimate reality," and it is from this reality and its manifold implications that Brooks tests her "newish voice." It is upon this reality that Brooks builds her own conception of the Black Aesthetic. 
   Brooks begins "The Sermon on the Warpland" with a series of structural paradoxes: "And several strengths from drowsiness campaigned / but spoke in Single Sermon on the warpland. / And went about the warpland saying No" (451). Beginning in medias res, Brooks jars the reader into the lyric present of her poem while alluding to a preceding, though absent, structure. This hypotactic entry and its repetition in the third line join with the explicit binaries in the stanza - "strengths" / "drowsiness," "saying" (proclaiming) / "no" (negating), and "several" / "Single" - to destabilize causation and offer a hermeneutical key to the poem. To understand the "warpland," the reader must confront the elaborate ways in which accumulation of meaning, with paradox and juxtaposition, is a necessary component of the black "reality." 
   What is the warpland? Leslie Wheeler maps the polyvalency of the term to the Sermon on the Mount, the "warped land" of an America torn by racial strife, the "'war planned' by black nationalists against white America, and even a 'warplane,' a carrier for this militant message" (231). Embedded within these possibilities lie both paradox and consonance. Just as Brooks claims to speak a "Single Sermon" from "several strengths" (note the genderless designation here), her polysemous subject - "warpland" - challenges her readers through a language of what could be called motivated ambiguity or, more polemically, a feminine semiotics of black empowerment. 
   This language contrasts greatly with that of Baraka, whose thematically analogous "Sermon for Our Maturity" illuminates the divide between the masculine and feminine lexicons of Black Power. In the poem, he outlines his Black Nationalist project in terms both singular and direct:  
We want to see you again as ruler of your own space Big Negro 
Big ol Negro 
          growin 
          wind storm flyin thru 
          your huge blue lung 
     Lung filled with hurricanes 
     of transparent fingerpops 
     and need to be changed to moans 
Stretch out negro 
      Grow "Gro 
     Gwan "Gro Grow 
Stretch out Expand . . . 
While his language can stray from the literal ("transparent fingerpops"), the bulk of Baraka's message is a clear appropriation of Manifest Destiny rhetoric: the male Negro must rule his own space. Indeed, as Harper suggests, "Ambivalence can have no place [. . .] in the prosecution of such a revolutionary political program as the Black Aesthetic was supposed to represent" (52). Sandra Hollin Flowers takes this sentiment one step further, claiming, "When the masses become confused about ideology, the entire philosophy loses credibility" (12).4 But whereas Baraka's plain style presents little room for political ambivalence, Brooks's poem embraces and indeed requires a reader who actively interrogates the precepts upon which Black Power rests. As such, when Brooks finally employs imperatives in her "Second Sermon," she couches them in opacity: 
Salve salvage in the spin, 
Endorse the splendor splashes; 
Stylize the flawed utility; 
Prop a malign or failing light. . . 
Through the elaborate, sonic word play of "salve salvage" and "splendor splashes," Brooks casts poetic language as a genderless, abstract, and therefore unstable political mechanism that must be negotiated and transformed by the reader. 
   In so doing, Brooks's poem challenges the implicitly hierarchical relationship between speaker and reader of the Black Aesthetic. As Harper argues, poets such as Baraka engender divisions within the black community through a binary system of "I" (or "we") / "you" wherein the poet achieves a position of cultural authority. Conversely, "any 'you' that these Black Arts poets invoke can function as a negative foil against which the implicit I who speaks the poem can be distinguished as a politically aware, racially conscious, black nationalist subject" (48). In the "Sermon for Our Maturity," this binary arises in Baraka's assumption of "we" and subsequent diminution of the reader to "you" or "Negro." Brooks's poetry makes no such claims - in fact, she announces the opposite by assuming a voice of "several strengths." Her voice thereby enacts a process of the black community speaking to itself and explores the diverse implications of its "ultimate reality" from a position within, rather than external to, its audience. Lesley Wheeler disparages such a move, writing that in so doing, the poem "minimiz[es] Brooks's literary authority" (231). But this seems to be exactly Brooks's point; by minimizing her own authority, she maximizes her black audience's political efficacy: a perfect inversion of the Barakan program. 
   Thus, Brooks's poetry moves away from prescriptive imperatives towards a program comparable to the consciousness-raising efforts of the women's liberation movement of the late sixties. Both manifestations of the broader "Movement" redefine their audience demographics, speaking directly to their oppressed compatriots rather than white patriarchal figures of power. Moreover, the efforts foreground the incomplete applicability of any holistic ideology and champion the complexity of any social or cultural epistemology. As Alice Echols writes, 
[F]or some women's liberationists consciousness-raising was a way to avoid the tendency of some members of the movement to try to fit women within existing [. . .] paradigms.

</file>

<file= AmE06_J68>

By the thirteenth century, the English had learned that the Welsh were
treacherous and fickle. In the last quarter of the fourteenth century, the
English trembled when the Welsh both raided English territory and produced
an alarming increase in the number of settlers migrating into English
border counties. After the widespread Welsh uprising in the first decade
of the fifteenth century, the English realized that the Welsh were not the
submissive and deferential natives they had feigned to be, but were a
perfidious people, on par with the wild Irish. These "insights" into the nature
of the Welsh were widely held perceptions among the English in the
late Middle Ages. Lest the English forget that the Welsh possessed these
characteristics. Sir Gawain and the Green Knight (SGGK) was prepared to
remind them.
SGGK was thoroughly tied to England's colonial project in Wales when
the poem was composed. SGGK is typically dated between 1350 and 1400
(a common ascription being the last quarter of the fourteenth century), a
period during which the English were attempting to complete their colonization
of Wales, while the Welsh violently opposed such domination.
Resembling several Arthurian histories from medieval Britain, SGGK is
structured by these colonial conflicts and, appropriately, arises from a border
culture: the poem is conventionally believed to have been composed
in northwest England, alongside the Welsh border, and employs a northwest
midlands dialect, specifically, the dialect of Lancashire and Cheshire.
Appropriately, the bulk of SGGK's narrative action unfolds in the English-
Welsh borderland. This location is specified at the beginning of Gawain's
quest to find the Green Knight. Gawain initially journeys through the
realm of Logres (England, south of the Humber) and eventually reaches
northern Wales. Gawain passes the Anglesey Islands, fords rivers near the
headlands, crosses at Holy Head, and lands "In be wyldrenesse of Wyrale"
("In the wilderness of Wirral," 701), a peninsula just inside England,
by the northeastern border of Wales. Gawain is in Wirral when Bertilak's
castle magically appears, making SGGK a border romance. 
This article investigates SGGK's participation in colonial struggles
between the English and the Welsh in the late fourteenth century. As the
models of ideology employed in British cultural studies attest, a text does
not simply reflect the political climate in which it is composed but intervenes
in the political terrain and participates in the production of the
social formation. Hence, using a methodology in dialogue with Stuart
Hall, Raymond Williams, Louis Althusser, and Antonio Gramsci, this article
examines how the ideologies speaking through SGGK attempted to
reformulate readers' conceptions of themselves and of their neighbors and
thus shape their perceptions of how to negotiate English-Welsh conflicts.
The defining work of SGGK in relation to England's colonization
of Wales is that of Patricia Clare Ingham, and we disagree dramatically
about how to understand the English-Welsh negotiations embedded in
the poem. Ingham argues that, as the poem unfolds, the issues surrounding
colonization raised early in the text disappear and that the ethnic and
geographic disparities between the English and the Welsh in the first half
of the poem collapse to be replaced by gender difference. I maintain that
SGGK insists throughout the entire poem - as did, in general, the English
and the Welsh in the late fourteenth century - that the two peoples differed
greatly. Ethnic and geographic incongruities are not effaced as SGGK
unfolds, but are reinscribed at the locus of gender - more precisely, at
the site of female sexuality - in a conventional move that acts to further
elaborate and consolidate colonial power by buttressing ideologies of colonialism
with ideologies of gender.
This divergence points to a more fundamental disagreement between
Ingham's work and my own. We understand the English colonization
of the Welsh, and hence the poem's colonialist politics, very differently.
Ingham writes,
Welsh and English interaction in march towns, at regional marketplaces,
on the battlefield, or in the narrative tropes of a Middle English
poem become the multiple places where unity is forged from ethnic heterogeneities.
Colonial union becomes an act of cultural synchronicity,
a coordination of capitulation . . . Rhetorics of distance and differentiation - 
the desire to separate "Welsheries" from "Englishries" in late
medieval histories, or in the case before us to determine once and for
all which parts of Gawain are Welsh or English - efface the familiarities,
shared dreamings, common spaces of household and story.
Ingham views the intermixing of the Welsh and English (in the poem
and in Wales) as the creation of a hybridity, a conflation which is, for
the most part, a reasonably pleasant cultural and geographical commingling.
Rhonda Knight's work on SGGK and colonization also centers on
hybridity, insisting that Gawain's identity is a "cultural collage" and that
Bertilak embodies Anglo-Welsh hybridity, reflective of the border region.
Knight acknowledges the harshness of England's seizure of Wales but
claims that by the time SGGK was composed, the intensity of conquest and
occupation had subsided to be replaced by a more settled coexistence.
Hence, both Knight and Ingham seem unaware that the English conquest
of the Welsh in the late fourteenth century was frequently bloody: many
Welshmen and women were dispossessed of their lands and livelihoods;
and many were killed by the invaders. Focusing more strongly on consensual
relations than coercive ones, Ingham roots her discussion in the
dialectical (specifically in Homi Bhabha's formulation of mimicry); but
when discussing a dialectic, it is important to emphasize that two disparate
groups do not approach their convergence on equal terms, as
Ingham's work frequently seems to imply. As Antoruo Gramsci and Stuart
Hall argue, when a dominant group seeks to produce hegemony, there is
a dialectic, where, in order to be effective, the ruling group must take account
of the interests and tendencies of the subaltern groups over whom
hegemony will be exercised. However, such compromise does not imply
equity, and the two groups do not contribute equally to the production of
the new social formation. Ingham and Knight make the power relations
in the poem and in Wales seem more equitable and especially more palatable,
I would argue, than they indeed were. As the historical discussion
in this article will demonstrate, relations between the Welsh and the colonizing
English - including in northeastern Wales and in the Marches in
the southeast - were generally bitter in the second half of the fourteenth
century. Accordingly, rather than arguing for the leveling of dissimilarities
and the liquidation of "ethnic heterogeneities . . . into nothing more than
the differences of an extended family," as Ingham holds or for some collage
of identities as Knight maintains, this article argues that SGGK insists
that the Welsh and the English are two distinct groups and that the poem
promoted England's conquest of Wales.
England's Conquest of Wales.
As R. R. Davies explains, by the close of the twelfth century, the Anglo-
Normans had entrenched themselves firmly in much of Wales, especially
in the southeast, in the far southwest, and, in places along the eastern
border. By the end of the next century, Edward I had paid extraordinary
amounts to effect and to sustain the conquest of the remaining areas of
north and west Wales, areas that had more successfully resisted colonization.
Thereafter, the Welsh provoked constant anxiety in the English.
There were occasional outbreaks of violence by the Welsh against their
colonizers, including anti-English violence in the north in the 1340s.
The English feared Welsh uprisings, a fear that intersected with apprehension
that the French and the Scots would employ Wales as a threshold for
attacking England. This trepidation mounted rapidly after the resumption
of the Hundred Years War in 1367, producing English garrisons at most
Welsh castles in the 1370s. Tensions in Wales grew palpably in the last
three decades of the fourteenth century. Ruthless English lords exploited
Welsh tenants to a particularly pronounced degree, and under these
conditions the indigenous community grew increasingly resentful. Relations
between the rural Welsh and burgesses in English towns in Wales
were fraught in places as distant as Carmarthen and Flintshire. In the
1370s, Owain Lawgoch, another in a long line of Welsh rebels emerged,
proclaiming his intention to recover Wales. Lawgoch's cause attracted the
support of the French, the Castilians, and some leading men in northern
Wales, and in response, the English government assassinated Lawgoch.
The last quarter of the fourteenth century witnessed a resumption and
intensification of campaigns by English counties bordering Wales against
Welsh raids and against what the English perceived to be an alarming
influx of Welsh settlers into English border counties.
Northeast Wales in particular generated considerable anxiety for the
English near the end of the fourteenth century: in the 1390s, it was the site
of both a bitter family feud and a violent demonstration that threatened to
become a sizable uprising. In 1400, an insurrection, led by Owain Glyn Dwr,
did erupt. Northern Wales was the seat of Glyn Dwr's power, with Glyn Dwr
also enjoying the support of his powerful cousins from Anglesey. English boroughs
in northern Wales were the primary targets of Glyn Dwr's followers
in their initial forays, although insurgency quickly spread throughout Wales.
Glyn Dwr's uprising was the longest rebellion in Wales since the Norman
Conquest, and the uprising threatened to secure Welsh independence from
foreign rule. However, by 1408, Welsh rebels had effectively lost the battle.
Whatever their biological or cultural interminglings, the Welsh and
the English understood themselves to differ greatly. This was evident on
the part of the English through their pronounced divisions between themselves
and the Welsh administratively and institutionally. As the thirteenth
century unfolded, the categorization of "the Welsh" versus "the English"
became much sharper, and the existence of two separate peoples was institutionalized
in the governance of Wales. The Welsh not only differed from
the English but were for many purposes considered inferior. This shift in
perceptions was discernable in newly emergent pejorative representations
of the Welsh in official documents - especially claims that the Welsh were
treacherous and fickle. In the fourteenth century, the distinction between
conqueror and conquered, settler and native, was even more clearly defined
in formal and institutional terms than at any other time. For the Welsh,
a pronounced distinction between themselves and the English was intact 
not only as the sporadic anti-English violence throughout the fourteenth
century testified, but as the widespread rebellion, led by Glyn Dwr, of the
colonized against their colonizers proclaimed. Most Welshmen and women
were united against the English under a common feeling of oppression and
under a shared political and historical identity. SGGK articulates - and
helps to produce - the understanding that the English and the Welsh differed
dramatically and that the two peoples were antagonistic.
Colonialism, King Arthur, and SGGK.
English-Welsh colonial struggles included contestations over King Arthur.
According to Welsh traditions, the invading Saxons drove Arthur and his
Britons into Wales. The Welsh not only claimed to be Arthur's descendants,
but they traced their glorious heritage through Arthur to Brutus,
to Aeneas, and ultimately to Troy. Elissa R. Henken explains that, in the
Middle Ages, Arthur confirmed the inherent worth of the Welsh by establishing
their honorable and often glorious past and by offering the promise
of a glorious future. Arthur became one of the most renowned redeemerheroes
of the Welsh in the Middle Ages - a figure who would someday
return to emancipate his people. Medieval Welsh texts, including the
Mabinogion and Geoffrey of Monmouth's writings, typically situate Arthur
and his court in Wales, frequently at Caer Llion ar Wysg (Caerleon-on-
Usk). For the English, however, Arthur and the Britons were forebears of
Englishmen. Hence, medieval English texts more readily identify Camelot
as Arthur's primary dwelling, located in some undisclosed spot in England.
Some English kings, especially Edward I and Edward III, claimed
descent from Arthur, citing his sovereignty over Britain as proof that English
monarchs ought to rule the British Isles.
SGGK emphatically participates in English-Welsh contestations over
Arthur and his lineage. In its opening lines, the poem provides a thumbnail
sketch of Arthur's genealogy: Aeneas flees Troy, leading to the founding of
Rome (and later of Tuscany and Lombardy); Brutus, descendent of Aeneas,
abandons his homeland to settle in Britain; and, of the subsequent kings of
Britain, Arthur is the noblest (1-26).

</file>

<file= AmE06_J69>

Modern dynamic web sites support many features for user collaboration and personalization. To provide such services, web sites contain custom computer programs, often written in one of a family of programming languages that have grown up around (or been adapted for) the web.
There is at least one dictum of program design that we cannot escape on the web: performance matters. As a web publisher, visitors are your livelihood. But will your servers and scripts be ready for the day that your site is featured on prime time television, or on slashdot.org? If tens of thousands of potential users drop by to find a sluggish (or dead) server, most of them will never return.
This happens so often to sites featured on Slashdot - a "news for nerds" discussion site - that it has come to be known as the Slashdot effect: "a site that might be designed to handle a few hundred hits per day can suddenly find itself handling that many a second". Although some ISPs have bandwidth limitations, Slashdot creator RobMalda says most sites that fail suffer from poor planning and architecture:
Anybody who has a pretty good understanding of web design [has] done a good job of learning what information to cache [and] what needs to be pre-generated. So when you're actually loading a page, even if it's a complicated page that looks dynamic and custom, on the back end of that, what they're really doing is putting together a bunch of puzzle pieces that have been pre-generated, and making the simplest, quickest decisions they possibly can.
Malda's observation points (informally) to the idea of staging the computation performed by a web service. Indeed, web publishing is an application area that is naturally staged1:
(1) Content (text, images, programs, etc.) created off-line is uploaded to the server - the publish stage;
(2) A user's browser requests content, which is transferred from server to client - the serve stage; and finally
(3) The content is rendered within the user's browser - the display stage.2
At each stage there is an opportunity for computation to take place.3 Consider the example of a conference calendar, such as the one illustrated in Fig. 1. After specifying your areas of interest (perhaps using the ACM classifications),
the server delivers a table of matching conferences, with dates, locations, deadlines, and links to conference websites. Events remain in the table until a few weeks after they occur, but deadlines that have passed are marked in red.
You may click on any column header to change the sort order. The next time you visit, the server remembers your preferences. Perhaps it even sends you email to remind you of upcoming submission and registration deadlines.
Now, how might this conference service be staged? Can anything be computed off-line (at the publish stage)? Yes: since this page is probably part of a much larger site (whose structure does not change every day), the menus and other
navigation aids can be laid out in advance.We will not know which conferences to display until the user presents some identification (in the form of a "cookie"), but since the conference data change infrequently, it may help to prepare the text of each row in advance.
During the serve stage, we look up the user's topic preferences, and ship out just the matching rows. If we delay sorting the table until the display stage, then the user ought to be able to adjust the sort criteria without any further communication with the server. What about marking past dates in red? If this is also delayed until display, then the code could be cached client-side for long periods of time, yet still behave dynamically.
At this point, we should emphasize the importance of profiling in developing scalable web services. This particular design for the conference calendar may not be optimal, depending on the number of entries and the relative speeds of the CPU, memory, database, network, and disk. Rather, our aim is to provide a single language in which the various staging possibilities can be expressed naturally.
This approach is in stark contrast to the status quo, where each system targets one stage only. The Website Meta Language4 is an "off-line HTML generation toolkit" designed for the publish stage. But many other programs (and countless ad hoc scripts) spit out HTML pages: LATEX2HTML, for example. Google reports surprisingly many programs5 for creating family tree web sites from genealogy database files; these also count as publish-stage tools.
The serve stage is well-served by the "server page" languages, including JSP, ASP, and PHP. The Common Gateway Interface (CGI)6 addresses the serve stage, as do the embedded interpreters (such as mod perl and mod python for Apache) that exist to ameliorate some of the overhead of CGI.
There is, relatively, a paucity of languages that operate client-side (display stage), probably due to the difficulty of securing an installed base of interpreters. JavaScript, Java, and Flash applets are notable exceptions.
Imagine implementing the conference calendar, as conceived above, using currently deployed technology: a Perl script outputs a PHP page which embeds JavaScript! Values are passed from one stage to the next as strings, and the programmer must manage all the quoting and persistence issues by hand.
Strictly speaking, these languages are not exclusively confined to the stages that we have indicated: Javascript can be run server-side, PHP can be run off-line, and so on. Nevertheless, migrating code between stages is hard, and the need for quoting and persistence are practically show-stoppers. For comparison purposes (further described in Section 5), we staged some code using PHP. To achieve persistence of composite variables from one stage to the next,
it contains gems like this:
where the serialize library function occurs in stage one and the unserialize in stage two. Notice that the preceding the first occurrence of list is quoted, but the second occurrence is not. The addcslashes function is needed in case the serialized representation contains special characters (such as double quotes or newlines) that would be misinterpreted by the PHP parser in the next stage. Interpreting the stage-one program guarantees nothing about the well-formedness of the stage-two program (generated as a string).
We present "MetaOCaml server pages", a new domain-specific language for web applications programming. It leverages the staging annotations and static typing of MetaOCaml [1,19] to provide safe and precise control over the first two stages. (We leave further consideration of the display stage as future work.) The system is implemented as two components: a translator transforms the server page language into a MetaOCaml module, which then can be incorporated into our multi-threaded HTTP/1.1 server (also written in MetaOCaml). The scalability gained by staging certain applications is stunning: In Section 5 we describe a directory browsing service where staging yields a factor of 30 improvement in throughput. The unstaged version would certainly succumb to the Slashdot effect.
The next section sketches the design and translation of MetaOCaml server pages, and Section 3 includes some non-trivial examples. The server implementation is described in Section 4. Performance and scalability are discussed
in Section 5.
Design
The general idea of a server page language is that we write HTML by default, and embed code <?like this.
PHP programmers are familiar with this syntax for embedding code, but in our case, the code itself is written in MetaOCaml. Here is a trivial MetaOCaml server page:
The OCaml function failwith raises a Failure exception containing the provided message. If a code block raises an exception, the message is sent to the client's browser in boldface, and the rest of the page is aborted.
In this example and throughout this paper, a sans-serif font is used for embedded MetaOCaml code, with bold sans reserved for keywords and code delimiters. A typewriter font is used for MetaOCaml character strings. The regular serif font is used for plain text and HTML within the server page, and for comments within the MetaOCaml code blocks.

A very common use of code blocks is to print out (i.e., send to the browser) the result of evaluating some expression.
The syntax  is designated for this task; e must have type string. Alternatively, messages may be formatted with sprintf by placing the format string immediately after the code delimiter.
The output is:
One more kind of code delimiter is used for declarations; these are lifted to the top of your program, and evaluated during the publishing stage:
Permissions on current directory are 
Output:
Permissions on current directory are Once published, this output will never change! The stat call is executed only once (because it is in a declaration block), not on each request. This is already a rudimentary kind of staging, but with the annotations of MetaOCaml, we will gain both flexibility and safety, as we'll see in the rest of this section.
Review of staging annotations
MetaOCaml augments OCaml with just three annotations, to indicate how programs are to be staged. Brackets construct future-stage computation. The code within is not executed in the current stage of computation, but just returned as a code value that can be run later.
Within brackets, the splice or escape operator  may appear. It interrupts the code construction to evaluate the expression e (in the current stage) and splice its result into the future-stage computation. Thus, e is required to evaluate to code of the proper type.
To compute and splice in a regular (non-code) value, we define a function let lift. this takes any value and turns it into code. We use it like this: The addition is performed immediately (because it is escaped), and the result is spliced into the code. Finally, there is an operator to execute constructed code. Applying it to the example above, produces 14.
Translation to MetaOCaml
To see how all this works, consider how a MetaOCaml server page is translated into a proper MetaOCaml program, to be executed at publish time. Since the program is executed before any browser has requested the page, it cannot directly return or output HTML. Instead, it will construct and return a code object which is subsequently run on each request (serve stage). The third (display) stage proposed in Section 1 is not yet supported by this design. Fig. 2 shows a sample MetaOCaml server page, and Fig. 3 contains its translation.
Declaration blocks have been lifted to the top; any side effects contained there are executed when the page is published. The constructed code begins on line 5.
The a b c represent publish-stage arguments (the names are specified with pragma args), whereas req and puts are (fixed) serve-stage arguments. req encapsulates the HTTP request details, including the headers and query arguments. puts is a function, provided by the server, to transmit text across the network to the user's browser. The library function Request.arg of the type request!string!string option looks up the string value of the request parameter with a given name. The option type constructor permits the function to return None if there is no matching parameter in the HTTP request. On line 6, arg is defined as a short-cut for retrieving a parameter by name. Since req and puts are serve-stage arguments, it is incorrect to use them in the publish (first) stage, and indeed the MetaOCaml type system prevents this.
These, along with the lift function defined near the top, are essentially primitives from the point of view of the server page code.
Staged code blocks
Now that we understand how the server page is assembled into a staged program, the effects of adding MetaOCaml staging operators to our pages should be predictable. Below is another example using stat, this time to display the size of some text file on the server. With the serve-stage argument unit, we can specify whether the size should be expressed in bytes (the default), kilobytes, etc.
If this text file does not change frequently (and reporting outdated information is no problem), the stat and float of int calls could be lifted into the declaration block, as with the permissions example. Furthermore, we may also use lift and the splice operator to perform the divisions in advance, even though they are underneath the match (which cannot happen until the serve stage):

</file>

<file= AmE06_J70>

An inexpensive tool for indicating microbial quality in watersheds is presented that utilizes the
conventional total coliform test, comparing the relative concentrations of different colonies that form on a membrane filter fed by m-Endo media. These bacterial colonies can be classified into 3 types; typical (TC), atypical (AC), and background. The ratio of the concentrations of AC colonies to those of TC is related to water quality, fecal loadings, and fecal age. The AC/TC ratio relies upon shifts in populations between indigenous and introduced bacteria, with the indigenous bacteria providing a baseline against which the concentrations of the introduced are evaluated. When the AC/TC ratio is low (<5), fresh fecal material is in the water and pathogen risk can be expected to be higher. As time passes, the AC/TC ratio increases (>20) and can be related to healthier water quality conditions. Different types of runoff have different AC/TC values with human sewage at the lowest end of the spectrum with a value of 1.5 under normal conditions.
INTRODUCTION.
In order to effectively manage a watershed impacted by nonpoint source fecal pollution, and reduce the risk of waterborne illness to recreational and other watershed users, one must have an easily applied tool, or analytical yardstick, to indicate problem areas, design effective interventions, and to measure intervention success. However, many of the analytical techniques developed for microbial water quality are expensive, meticulous, and difficult for the average engineer to apply. The ratio presented here is simple, uses well-known and easily obtained technology. The AC/TC ratio has proven to be an invaluable tool for initial watershed screening by microbiologists, engineers, wate r plant operators, and even high school students. The ratio relies upon comparing a heterogeneous group of generally autochthonous bacteria that are stimulated to grow by nutrient enrichment and show a color change upon analysis for total coliforms that brands the colonies "atypical" (AC), to gram negative, fast-lactose
fermenting, coliform bacteria colonies (TC) considered to be indicative of potential fecal contamination.

Brion and Mao (2000) first published observations on the differences in the ratios of AC/TC obtained from several years of sampling runoff in urban and agricultural streams and ponds impacted by different fecal sources. Since then the AC/TC ratio has been used in several other follow-up studies and has been shown to be related not just to the predominant source of fecal material (Brion and Lingireddy, 1999; Brion et al., 2002b; Booth and Brion, 2004), but to the age of fecal material (Neiman and Brion, 2003; Booth and Brion, 2004), and even to indicate general microbial watershed quality (Neiman and Brion, 2003, Booth and Brion, 2004). As of yet unpublished studies have shown the AC/TC ratio to be constant throughout conventional sewage treatment, although impacted by rain events, and to behave similarly in animal compost heaps as it does in the watershed environment. This paper will present a concise summary of these findings and provide an outline of how to apply this new bacterial ratio and interpret the results for watershed management of nonpoint sources of fecal pollution.
METHODS.
Microbial analysis for total and fecal coliform and fecal streptococci used membrane filtration techniques as described in Standard Methods (Standard Methods, 1992). Growth media were M-Endo broth, M-FC broth, and KF Streptococcus agar obtained from Difco and prepared according to directions for total coliform, fecal coliform, and fecal streptococci respectively. Each bacterial test was incubated at their specified temperature and counted at the specified time with respects to phenotypic colony appearance. 
On the total coliform plates, total (TC), background (BC), and atypical colonies (AC) were counted
consistent with the criteria specified in Standard Methods (Standard Methods, 1992). At least 3 dilutions of each sample, with 2 replicates per dilution, were analyzed to provide final counts. Statistical analysis of data was done utilizing SigmaStat, a program available from Systat Software Inc.
RESULTS.
Identification of predominant fecal sources in surface water.
In a multiyear study of a small watershed used as a supplemental potable water supply for the city of Lexington, Kentucky, USA, distinct differences in the AC/TC ratio from areas with different land-use associated fecal material types were noted. Samples were taken from numerous sites throughout the watershed and classified as suburban or agriculturally impacted, with suburban sites split further into impounded or free-flowing categories. Prior bacteriophage typing studies had not shown human associated, F-specific RNA phage to be present at the agricultural or suburban sites (Brion et al., 2002a) so the predominate fecal sources were concluded to be the animals observed to be associated with the land-use classifications. Microbial samples were taken 1-2 times a week at these sites for a multi-year period and the information evaluated for insights into fecal source by the conventional fecal coliform/fecal streptococci ratio (FC/FS), and the new AC/TC ratio. Table 1 presents the average values for the AC/TC and FC/FS ratios, irrespective of weather conditions, at these different types of fecally impacted sites for the year studied.
As can be seen, the AC/TC ratios for surface waters impacted by different fecal types associated with observed land use is quite different, ranging from 103 for impounded suburban runoff to <5 for human sewage. The geometric mean AC concentrations between the surface water sites were not found to be significantly different (average all sites = 104.9 AC cfu/100mL), so the difference in the AC/TC ratios is due primarily to the differences in fecal coliform concentrations as referenced against the atypicals. These average AC/TC ratio values were found to be significantly different by Kruskal-Wallis one-way ANOVA (H = 76.557 with 3 degrees of freedom, P = <0.001) and between all pairwise comparisons, with successful in identifying the predominant fecal source with significance, with the exception of distinguishing between sewage and flowing agricultural runoff. When stratifying the data to exclude days when rainfall was impacting the quality of the influent sewage, the AC/TC ratio for sewage was 1.6, a value close to that previously reported from earlier studies by Brion and Mao (2002) of 1.5 and distinguishable statistically from agricultural impacted surface water. The influx of aged fecal material from stormwater scour increased the AC/TC ratio from an average of 1.6 to 5.9, and decreased the FC/FS ratio from 3.8 to 2.8; both ratios moving towards values not representative of fresh human sewage or animal feces.
The FC/FS ratio was of little value in classifying the predominate fecal sources in this study. The
differences in the average FC/FS values were not found to be significant by all pairwise multiple
comparison procedures, except for comparing flowing agricultural stream quality with that of flowing suburban (Dunn's Method, P<0.05 for significance), and these average values would indicate human sewage (FC/FS < 4) for sites impacted primarily by cattle feces.
Identification of hot-spots in watersheds. 
As reported by Booth and Brion (2004), significant drops in the average value of the AC/TC ratio were able to pinpoint inputs of known sources of untreated domestic sewage into a small creek before (2002) and after (2003) some of these communities were provided with a forced main sewer line. As can be seen in Figure 1, from the agricultural headwaters of this creek (Site 7) where the AC/TC ratio is >10, and well within the range of flowing surface water values cited above in Table 1, the AC/TC ratio steadily declines to <10 with sharp decreases seen as it passes through inadequately sewered towns that added untreated human sewage to the agriculturally impacted creek. In the next year, after partial sewage service had been provided to the communities along the creek via a large forced main, the AC/TC ratios are overall higher at all points along the creek. However, areas that did not fully connect to the sewer main still showed significant declines in the AC/TC ratio after the creek absorbed their untreated sewage input. Average concentrations of indicator bacteria were not able to pinpoint the impacts of inputs of human sewage for either year as the concentrations at each site were statistically indeterminate, but the AC/TC ratio was able to pinpoint the human sewage impacted hot-spots along this creek with ease.
Fecal loading at Site 7 was mildly increased in 2003 as compared to 2002 and the AC/TC ratio slightly depressed from last years average as a result. This is thought to be due to a much wetter summer in 2003. The previous summer had long periods (weeks) of no precipitation, whereas the summer of 2003 hadrainfall nearly every week providing fresh fecal inputs from land scour, reflected in a slightly depressed average AC/TC ratio at site 7 (16.8 for 2002 vs. 14 for 2003) and an increase overall in headwater fecal coliform (FC) concentrations. FC concentrations were not found to be statistically different between the sites based on the geometric mean, and in 2002 were 101 cfu/100mL at the headwaters rising to 149 cfu/100mL at the confluence with the Kentucky River. Overall, in 2003, FC levels fall along the entire length of the creek from a geometric mean of 140 cfu/100mL at the headwaters to 90 cfu/100mL at the confluence, suggesting the effectiveness of the sewer project, but this trend is not statistically significant.
Indeed, T-tests do not indicate the FC levels in 2003 are significantly different from those of 2002 (P<0.01), nor when compared between sites by ANOVA for just the values obtained in 2003. As is the case examining FC values for both years along the length of the creek, the FC concentrations would not index any particular source as cause for the elevated FC levels as all geometric mean values are statistically indeterminate. Yet, the AC/TC ratio shows improvement in the creek, and still pinpoints hotspots of human fecal addition.
Prediction of relative fecal age.
It has been reported by Neiman and Brion (2003) that in both bench scale, batch laboratory studies on river water, and in the Kentucky River from which the water was taken, that the AC/TC ratio falls after an influx of fresh fecal material, and then rises over time as the fecal material ages. Also, it has been observed that AC/TC ratios invariably drop in surface water after rain events as fresher fecal material is washed into the system (Brion et al., 2002b; Neiman and Brion, 2003). Reviewing data from the Kentucky River, it was seen that after an intense period of rain, where the AC/TC ratio dropped below 3, it rose slowly to a value of 10 by the third clear day after the storms stopped, rising to a high of 79 on day 7, before dropping with the next storm event back into values of the upper teens. The only known exception to the drop in AC/TC ratio with rainfall events is for domestic sewage as noted prior where the influx of aged fecal material caused a rise in the ratio. The drop in the AC/TC ratio in surface waters after a rain event is due to a combination of factors, the die off of fecal coliforms and the growth of the
atypical group in response to nutrient influx into the system. Both of these factors would cause a rise in the observed AC/TC ratio. In support of these published findings are presented the unpublished results of an animal manure study that shows the same rise in AC/TC ratios over time in two types of aging animal manure (Table 2). Briefly, fresh manure was obtained from nearby farms and put on a platform for two weeks. At the very beginning and at the end 5 replicate samples of leachate solution with varying amounts of manure were prepared and analyzed for bacterial content with the concentrations averaged.
As is readily apparent, AC/TC and FC/FS ratios in fresh manure both start at values <1 and rise over time. The average AC/TC values for both fresh and aged horse versus cow manure leachate are statistically different by Mann-Whitney Rank Sum test (P<0.001), but the values for the FC/FS ratios are not. The rise in the AC/TC ratios seen in the controlled studies is expected to occur in the environment as it did in these pile studies, but this finding has not yet been confirmed.

</file>

<file= AmE06_J71>

Modern ships typically have a number of distributed systems. Distributed systems are used because it's simpler, cheaper, and better to centrally produce a commodity such as electricity or chill water, than to locally produce it with the users of the commodity. For naval warships, in addition to cost, two measures of performance are very important: Survivability and quality of service. Survivability relates to the ability of the distributed system, even when potentially damaged by a threat, to support the ship's ability to continue fulfilling its missions to the degree planned for the particular threat. Quality of service measures the ability of the distributed systems to support the normal, undamaged operation of its loads. This paper defines a number of key terms, details a number of different zonal architectures, describes the situations where the architectures are best suited and proposes a framework for zonal ship design that promises to satisfy survivability performance requirements and quality of service requirements.
Introduction.
The advantages of zonal systems design have
long been recognized and documented (Petry and
Rumburg 1993), (Shiffler 1993). Since then, zonal
a.c. electrical distribution systems have been used
in the DDG 51 class, LPD 17 class, and LHD 8.
The next advance in zonal electrical distribution,
integrated fight through power (IFTP) featuring
d.c. zonal electrical distribution is being developed
for the Navy and is a candidate for future
installation on DD(X) and CG(X). (Ciezki and
Ashton 1999), (Roberts 2002), (Hiller 2003),
(Walsh 2003), (Zgliczynski et al. 2004)
To date, zonal design concepts have been applied
to distributed systems (usually just the electrical
system) in an ad hoc fashion. A systematic study
of zonal architectures has not been published.
Likewise, the impact of zonal system design
on total ship design has not been adequately
addressed. This paper defines a number of
key terms, details a number of different zonal
architectures, describes the situations where
the architectures are best suited and proposes a
framework for zonal ship design that promises
to satisfy survivability performance requirements
and quality of service requirements.
Background.
Modern ships typically have a number of distributed
systems. Distributed systems are used
because it's simpler, cheaper, and better to centrally
produce a commodity such as electricity
or chill water, than to locally produce it with the
users of the commodity. For naval warships, two
measures of performance are very important:
survivability and quality of service.
Survivability relates to the ability of the distributed
system, even when potentially damaged by
a threat, to support the ship's ability to continue
fulfilling its missions to the degree planned for
the particular threat. The threats for which a
ship is designed to are its design threats, and the
residual capability following exposure to the
design threats is the design threat outcome.
While survivability measures the ability of the
ship to continue to function during damage,
quality of service measures the ability of the distributed systems to support the normal, undamaged
operation of its loads. Quality of service
is measured in terms of a mean time between
failure (MTBF) where a failure is defined as any
interruption in the supply or deviations outside
of normal bounds of commodity characteristics
that prevent the load from performing its assigned
function.
Although survivability and quality of service are
usually not the source of design conflicts, design
features may impact one more than the other.
For example, the routing of cables in an electrical
distribution plant will have little impact on
quality of service, but will have a tremendous
impact on survivability. On the other hand, the
reliability of generator sets has a bigger impact
on quality of service than on survivability.
In the design of distributed systems, cost is always
a major consideration. Because the relative
costs and capabilities of different distributive
system components differ from system to system,
a universal zonal design that applies to all cases
does not exist. In selecting an architecture, the
following strategies for reducing acquisition
costs (while still meeting performance requirements)
should be considered:
a. Eliminate hardware and software
b. Substitute expensive hardware and software
with cheaper hardware and software. This includes
increasing cost and capability of device
A to enable the reduction in cost and capability
of device B as long as there is a net savings.
c. Enable the hardware to be installed more
easily
d. Enable the hardware to be tested before
installation onboard ship
e. Reduce the engineering effort needed to design
the ship
Because this paper does not address specific distributed
systems, these cost reduction strategies
will be addressed only in general terms.
Definitions.
Zone.
A zone is a geographic region of ship. In a general
sense, the boundaries of the zone can be arbitrary,
but to maximize survivability, the zones of
multiple distributed systems as well as damage
control zones should be aligned. For shipboard
distributed systems, this typically means the zone
boundaries are the exterior skin of the ship and
selected transverse watertight bulkheads. The
zone boundaries may rise above the watertight
bulkheads into the superstructure, or the superstructure
may be composed of one or more zones
independent of the zones within the hull.
Adjacen t Zones.
Adjacent Zones are zones that could simultaneously
be damaged by a design threat. Zones
are typically sized so that usually only 2 zones
are simultaneously damaged by a design threat,
although in some cases a third zone (such as the
superstructure) may also be damaged.
Zona l Survivability.
For a distributed system, zonal survivability is the
ability of the distributed system, when experiencing
internal faults because of damage or equipment
failure confined to adjacent zones, to ensure
loads in undamaged zones do not experience a
service interruption. Zonal survivability assures
damage does not propagate outside the adjacent
zones in which damage is experienced. For many
distributed system designs, zonal survivability
requires that at least one longitudinal bus remains
serviceable, even through damaged zones.
At the ship level, zonal survivability facilitates
the ship, when experiencing internal faults in
adjacent zones due to design threats, to maintain
or restore the ships primary missions as required
by the design threat outcome. Ship level zonal
survivability focuses restoration efforts on the
damaged zones, simplifying the efforts required
of the ship's crew to maintain situational awareness
and take appropriate restorative actions.
Ship level zonal survivability requires sufficient
damage control features to prevent the spreading
of damage via fire or flooding to zones that were
not initially damaged.
Compartment Survivability.
Zonal survivability only addresses loads outside
of the damaged adjacent zones. For some
important loads, including those implementing
mission systems, providing redundant capability
across multiple non-adjacent zones may prove
to be infeasible. This situation often arises in the
superstructure where the sensor masts are located
in the same or adjacent zones. In some cases,
these loads may be perfectly functional although
damage has reached into its zone. Likewise, maximizing
the probability of maintaining loads that
support damage control efforts within the damaged
adjacent zones also assists in preventing the
spread of damage to zones not initially impacted.
Examples of such loads include emergency lighting
and power receptacles for portable dewatering
pumps. In these cases, providing compartment
survivability for the distributed systems for the
specific loads is warranted.
Compartment survivability requires that every
distributed system required by a specific load
provide independent normal and alternate sources
of its commodity (power, cooling water, etc.). For
the specific design threat, one of the sources of the
commodity should be expected to survive if the
specific load is expected to survive. The point at
which the in zone distribution of the commodity
merge (such as with an automatic bus transfer
- ABT) from the normal and alternate sources
should be within 1/2 of the expected damage
radius of damage centered at the specific load.
Mission System,
A mission system consists of the hardware and
software dedicated to the performance of a primary
or secondary mission of the ship. Examples
of mission systems include aircraft launch
and recovery equipment (ALRE), propulsion
systems, combat systems, and C4ISR systems.
Ideally, the mission systems of a ship should be
designed such that the capability to perform
the ship's missions is not lost if mission system
equipment in adjacent zones are not operational.
Unfortunately, ship design constraints will often
preclude the level of redundancy required to ensure
continuous capability. If mission capability
can not be assured continuously, then the ability
to restore capability to achieve the desired threat
outcome must be provided.
Distributed System.
A distributed system moves a commodity
from one or more sources to multiple loads
distributed through-out the ship. Examples of
commodities include electrical power, cooling
water, firefighting water, and fuel. For a given
commodity, distributed systems can generally
be described by an architecture consisting of the
following functional elements:
Generation.
A generation element produces the commodity.
Examples include gas turbine generator sets,
firepumps, and chill water plants. Generation
elements for one distributed system are generally
loads for other distributed systems.
Distribution.
A distribution element transports the commodity
between other functional elements. For zonal
distribution systems, the longitudinal buses are
instances of distribution functional elements.
Conversion.
A conversion element converts the commodity
from one form to another. An example of a conversion
element is a transformer in an electrical
system. A transformer changes the voltage level
of its commodity, electrical power.
Load.
A load is a consumer of the commodity. A load
for one distributed system can be a generation
element for another distributed system. For example,
a chill water plant is a load to the electrical
distribution system and a generation element
for the chill water distribution system.
Storage.
A storage element stores the commodity for
later use. In some systems, such as fuel systems,
storage elements (fuel tanks) functionally replace
generation elements. In other systems, such as
electrical systems, storage elements (uninterruptible
power supplies) serve as buffers to prevent
power disturbances from propagating to loads.
Control.
A control element coordinates the other elements
of a distributed system to enhance quality of
service and to facilitate the restoration of service
following a casualty. For new designs, the control
element typically consists of software that resides
within the total ship computing environment.
For an example of this architecture as applied
to an integrated power system, see Doerry and
Davis (1994) and Doerry et al. (1996).
Design Threat.
A design threat is a threat to the ship where a
design threat outcome has been defined. Examples
of design threats could be specific cruise
missiles, torpedoes, guns, explosives, weapons
of mass destruction as well as accidents such as
main space fires, helicopter crashes, collisions,
and groundings.
Design Threat Outcome.
The design threat outcome is the acceptable performance
of the ship in terms of the aggregate of
susceptibility, vulnerability, and recoverability,
when exposed to a design threat. Possible design
threat outcomes include:
a. Ship will likely be lost with the loss of over
25% of embarked personnel.
b. Ship will likely be lost with the loss of 25% or
under of embarked personnel.
c. Ship will likely remain afloat and not be capable
of performing one or more primary mission
areas for a period of time exceeding one day.
d. Ship will likely remain afloat and be capable
of performing all of its primary mission areas
following restoration efforts not exceeding
one day using only that external assistance
that is likely available within the projected
operating environment.
e. Ship will likely remain afloat and be capable
of performing all of its primary mission areas
following restoration efforts not exceeding
two hours using only organic assets.
f. Ship will likely remain afloat and would be
capable of performing all of its primary mission
areas following restoration efforts (if needed) not
exceeding two minutes using only organic assets.
g. Ship will likely remain afloat and would likely
be capable of performing all of its primary
mission areas without interruption.
h. The threat weapon is not considered a significant
threat because the probability that
the threat weapon would have been defeated
before striking the ship is greater than 98%.
Note: The term "likely" should be assigned a
specific probability of occurrence.

</file>

<file= AmE06_J72>

Southern California leads record year in volume
For many on the Southern California waterfront, the biggest event in 2006 was the lack of any big event.
Unlike some recent years in which volume increases led to labor shortages or cargo back-ups, 2006 saw record movement without significant delays or deficiencies. The
Southern California ports moved a record-breaking number of containers, with totals of 6.06 million loaded container TEUs in Los Angeles and 4.71 million in Long Beach. Combined, those numbers were up more than 12 percent from 2005, and represent a 43 percent increase
in the past four years.
Steps taken by PMA in recent years to facilitate this cargo growth  -  an agreement with the ILWU to hire additional workers as needed; aggressive forecasting and analysis to determine labor needs; and training programs to enable employers to meet those needs  -  led to a year in which 99.9 percent of all labor orders were filled. Put another way, there were no discernable shortages of
labor on the Southern California waterfront. Coast-wide numbers show strength
At other major container ports along the coast, recent volume gains solidified:
Oakland moved a record 1.63 million loaded TEUs, while Tacoma and Seattle
fell just short of previous records, at 1.36 million and 1.30 million TEUs, respectively.
Given the enormous gains in Southern California, these figures showed the tremendous
strength of the West Coast waterfront.
In the last four years, coast-wide container traffic has risen 41 percent, while the registered
workforce has increased by 38 percent  -  to a total of 14,279 workers. For 2006,
overall coast tonnage rose at a healthy 8 percent, to a record 361 million revenue
tons. Put another way, 1 million tons of cargo  -  ranging from automobiles to fruit to
clothing, shoes and consumer electronics  -  moved through West Coast ports each day
of the year. To learn more about 2006 cargo
movement, please see pages 57-76 of this report.
Technology Advances Continue
Four years after the landmark agreement that enabled employers to bring information and clerk technology to the waterfront, PMA members have made significant gains in modernizing terminals. Looking ahead, employers have their sights set on further innovation
that will allow the West Coast waterfront to continue to grow.
A primary goal of the 2002 PMAILWU agreement was to change the
practice of clerk intervention when readily available electronic information
could speed the flow of information  -  and therefore cargo  -  on the terminal.
Examples include the introduction of video cameras and OCR readers that
are in use at 85 percent of truck gates coast-wide. Other such innovations
have significantly reduced the number of clerk shifts required to move containers
through the terminal, while enabling volume growth that has supported
thousands of new industry jobs and brought major benefits to the regional and U.S. economies.
By 2008, it is anticipated that many terminals will have fully integrated
systems that include video cameras, OCR readers and GPS systems. The
result will be the ability to know where a container is at any given time, in a
manner that is accurate, reliable and rapidly obtainable. Further innovations
will enable the continued efficient
deployment of workers throughout
the facility.
Terminal Security Moves Forward
In 2006, steps were taken to advance the Transportation Worker Identification
Card (TWIC) program, while a pilot program in Oakland is laying the foundation
for increased terminal security. The TWIC program is a federally mandated
secure identification credential that is essential to port security. It has been under development in various forms since September 2001, and the first phase (TWIC enrollment) is
expected to be underway in 2007. In addition, as part of this phase, there will
be TWIC biometric reader prototype testing at Los Angeles/Long Beach. The
employers have participated at all levels of planning, design and testing, in conjunction
with the federal government. The ILWU has participated in these
forums as well. The industry is vitally concerned with
port security, and the TWIC is an essential element to aid in knowing who
is on the waterfront at any given time. PMA stands ready to assist the Coast
Guard and Transportation Security Administration with the successful
implementation of TWIC so that neither the enrollment nor rollout processes
unnecessarily impact terminal operations nor impede commerce.
The Oakland pilot program uses grantfunded turnstile entry gates and readers
installed by the port, along with a special electronic PMA training ID
card, to monitor terminal access. This pilot program has already provided
valuable lessons for the broader roll-out of TWIC coast-wide.
GSST VI Brings Focus to Security
The General Safety Training program, widely heralded as an outstanding tool to orient new
workers and veterans alike to waterfront safety
issues, has undergone its latest evolution  -  and a slight name change.
Reflecting the ongoing importance of waterfront security, and the recognition that workers play a key role in maintaining it, the program is now titled General Safety & Security Training VI (GSST VI). After significant revisions in 2006, it includes a new stand-alone module on security, and an increased focus on waterfront vigilance and alertness.
GSST VI will be rolled out in 2007. Key themes include: "own your safety," "scan and focus," and "create a safety net." The program includes classroom
learning, instructional videos and a hands-on workbook to reinforce key
lessons. All waterfront workers are required to participate in this program upon entering the industry and then every three years thereafter.
PMA Members Fuel Environmental Gains
In 2006, several PMA members announced major environmental initiatives,
while the industry as a whole found ways to reduce air emissions from vessels and terminal equipment. Industry initiatives have also resulted
in fewer trucks on the road and less idling in the yard.
Vessels: One company made the decision to burn low-sulfur fuel in the
main and auxiliary engines on all of its ships calling California. The switch, made 24 miles from the coast, is reducing emissions by an estimated
400 tons each year. Several PMA members announced or implemented
plans to "cold-iron" their ships, making use of shoreside electric
power while in port. Others are burning low-sulfur fuel in auxiliary
engines to reduce vessel emissions at the dock. Nearly all carriers have
reduced vessel speeds entering San Pedro Bay, lowering annual NOx emissions by hundreds of tons. PMA members are testing marine engine
technologies  -  slide valves, a next-generation lubricating system, sea water
scrubber technologies and on-board emulsification of residual fuels  - that hold promise for further pollution reduction.
Terminals: Marine terminal operators in California have voluntarily replaced
or retrofitted more than 1,100 pieces of yard equipment with post-combustion
technologies such as Diesel Oxidation Catalysts, particulate filters
and cleaner diesel. One terminal in Southern California operates a fleet
of more than 50 propane-powered yard tractors, while others are experimenting with LNG-powered tractors.
In some cases, PMA member-operated equipment already meets or exceeds
2010 air quality standards. On-dock rail programs are eliminating thousands
of truck trips each week. And it is estimated that Southern California's
PierPass program has shifted about 40 percent of cargo moves to off-peak
hours, saving thousands of hours of truck idling on area freeways.
Terminal Operators Reduce Emissions
The California Air Resources Board has implemented stringent
regulations for terminal cargohandling equipment. PMA and
the terminal operators worked closely with CARB to hammer out these tailored regulations that
will improve air quality for communities
surrounding the ports.
As of this writing, terminal operators are 30 percent ahead of
expected emissions reductions. In addition, they have tested a
variety of means to further reduce emissions, such as alternative and
low-sulfur fuels, post-combustion catalysts and particulate filters.
McNeill Retires from PMA
John McNeill, Vice President, Contract Implementation, retired in June 2006. McNeill joined PMA in
2003 after having spent more than three decades in the industry with Marine Terminals Corporation. A seasoned executive, he used a combination of strategic perspective and strong management skills to organize and create a well-run contract implementation
department. This unit is known for being tenacious and pragmatic in its approach to the observation and application of manning rules under the labor agreements.
PMA President Jim McKenna said: "John's leadership has made a real difference in our ability to manage the West Coast waterfront. He's tough, strategic and
always has the industry's best interests at heart." A long-serving member of PMA's Coast Steering Committee and its Negotiation Committee, McNeill became
chairman of the Employers' Pacific Coast Safety Committee in 1999. He also served as a PMA member of the Board of Directors of the National Maritime Safety Association. McNeill continues to provide consulting services to PMA. 
Training: Record Numbers Continue
As a result of continued increases in both the size of the workforce and the
need for skilled workers, the PMA Training Department had a busy, successful
year in 2006. Training staff worked with record numbers of longshore
workers in skilled equipment operations such as container gantry
crane, top handler, side pick, fork lift, heavy lift and straddle carrier. Furthermore,
casual processing  -  bringing new workers into the industry  -  continued
in Los Angeles/Long Beach, and began in San Diego, Tacoma and
Seattle. Skilled equipment operators are at a premium in the major Southern
California ports of Los Angeles/Long Beach, where terminals are moving more cargo than ever before without having additional space to expand.
In many cases, containers are being stacked rather than stored on wheeled
chassis. As a result, more labor is needed to move and stack the cargo.
An example of the increase in training needs can be seen in top-handler & side-pick operations. Plans were made for 280 workers to be trained during the period from July 2006 through June 2007. Halfway through that period  -  by the end of 2006  -  270 had
already been trained. Further details on PMA training
programs can be found on page 70. 
Air Quality Tests: Terminals Within Limits
A federal agency charged with overseeing worker safety has found that
on-terminal air quality is well within regulatory limits. The National
Institute for Occupational Safety and Health (NIOSH) issued its final report after overseeing tests at the ports of Los Angeles, Oakland and Tacoma. Further PMA-ILWU tests
in Portland using a private contractor also showed emissions to be well
within legal limits. These tests were requested by the ILWU  -  and agreed to by PMA  -  during their 2002 contract negotiations. Further tests by local agencies are expected.
Legal Developments
During 2006, PMA and its members reached settlement with a group of
casual longshore workers in California who claimed they were owed wages as
a result of late dispatch from the hiring halls that are jointly operated
by PMA and the ILWU. It is worth noting that conditions have changed
on the West Coast waterfront in such a way that this matter is unlikely to be an issue in the future. With an increase in the size of the registered
workforce and the addition of new casual workers, PMA is confident that
work on the waterfront will continue to run smoothly, even at a time of
record cargo volume.
Throughout 2006, PMA continued to vigorously defend the interests of its
members against a number of lawsuits and charges filed with administrative
agencies. PMA continues to succeed in having most lawsuits dismissed
prior to trial.
PMA was pleased to welcome a new attorney into its Legal Department in
mid-2006. Todd Amidon, an experienced labor and employment attorney,
joined as Senior Counsel, replacing Greg Wellons, who opted to move on
from PMA in 2005. Amidon joined General Counsel Craig Epperson, Senior Counsel Kathy O'Sullivan, and Legal Secretary Edie Apostolos at Headquarters.
Regional Developments
Northern California
The Port of Oakland, while not experiencing the exponential growth of 2005, saw modest increases in both container traffic and overall volume in 2006. At the same time, gains in technology and changes to the layout of several terminals have enabled greater efficiency and throughput.
In the last year, employers have focused on technology at the "in" and "out" truck gates to speed the flow of containers into and out of the yard. Technology has also improved the efficiency of vessel operations, and several terminals have begun construction projects that are expected to further enhance operations.

</file>

<file= AmE06_J73>

Until 1984, the Civil Aeronautics Board (CAB) was responsible for regulating airfares
in the United States. As a consequence of that regulation, commercial passenger carriers
competed on many dimensions other than price. Such behavior was recognized as being
economically inefficient. The price system was not being allowed to direct resources to
their greatest value in use. The CAB was dismantled on the premise that price
competition among carriers would benefit consumers and direct productive resources
to their greatest value in use. It was felt that, inter alia, the threat of entry would be
sufficient to prevent airlines from being able to exploit apparent monopoly power. That
premise ignores the fact that consumers are an essential element in the exercise of
market power. If consumers do not search for low fares, then it is unlikely that the threat
of entry will have much impact on the fare structure. The effect of the entry of a low fare
carrier will only be the reallocation of fliers among carriers at an airport, with little
impact on the allocation of passengers among airports. Indeed, one of the current stylized
facts about air travel is that there is more variation in price among carriers at an airport
than among airports. It is possible to evaluate the effect of low fares on consumer
behavior, and by implication the likely success of the threat of entry as a disciplinary
device, by examining multi-airport markets. The unwillingness of flyers to travel to other
airports to obtain lower fares increases the ability of carriers to exploit monopoly power
and discriminate in prices. Since broad geographic markets are often used in merger
cases,1 our analysis may shed some light on such markets.

Ashford and Benchemam [1987] studied airport choice in central England for the
period 1975Y1978. Among business travelers, distance to the airport was the most
important variable, followed by frequency of service. Fare was found to be most
important among those traveling for pleasure. Caves et al. [1991] found access
time, frequency, and fare to be significant variables in a model of choice between mature
and emerging airports in England. Thompson and Caves [1993] used data for 1983 to
study airport choice in northern England. For both business and leisure travelers,
distance to the airport and number of available seats were important. Frequency of
service was also important for business travelers. In the San Francisco market, Harvey
[1987] found access time and frequency of service to be determinative. None of these
earlier efforts would lead one to believe that the difference in fares from different
airports would lead to more competition among carriers, or that fare differences could
lead to the reallocation of market share among airports. More recent studies, using
various modifications of the multinomial logit model, also confirm the importance of
access time and frequency of flights in airport choice [Basar and Bhat, 2004; Pels et al.,
2003; Windle and Dresner, 1995]. The lack of searching for the best fare among airports
is perhaps understandable given the time cost of travel to a lower fare airport may
swamp any differences in fares.
In Spring of 2000, a phone survey was conducted of residents of the market area of
Philadelphia International Airport (PHL). The eventual goal of PHL was to learn about
its customer base with an eye to increasing its market. PHL management considers its
facility in competition with not only the small regional airports but also with its large
neighbors to the north and the south: JFK International, Newark International, and
BaltimoreYWashington International. The relevant market was defined by PHL_s
management. Newark is the largest of the four, and BaltimoreYWashington is the
smallest. The analysis of that survey is reported here.
The 1,100 respondents in the final sample were asked a wide variety of questions
about their travel and airport usage. From the survey data, both univariate and
multivariate probit models of airport choice were estimated. A preference for using a low
fare airport was expressed by survey participants. However, in the sample used here,
actually searching for a low fare was not a significant variable in airport selection.
Apparently, respondents liked the idea of using a low fare airport but did not do much
fare research and did not base their eventual choice on fare differences. As a new entrant
in a multi-airport region, a discount airline should enter at that airport where there is
the greatest opportunity for winning market share from incumbents without relying on
attracting new passengers from other airports.
Income was a significant variable in the use of two of the three distant airports:
Newark and BWI. Higher income increased the likelihood of flying from either Newark
or BWI in the previous year. If distance from the respondent_s residence to the airport
was an important consideration, then it increased their likelihood of using JFK, Newark,
and Philadelphia. Those who used airports other than the target airport were also more
likely to use the target airport. For example, those who had used BWI, JFK, and EWR
were also more likely to have used PHL.
The Model
Passengers must decide on both an airline and an airport for their travel. The phone
survey used to assemble the data asked respondents to think about all of their travel in
the prior year. This precluded directly asking about choice of airline as could be done in
an intercept interview in an airport. Consequently the model used here addresses only
the choice of airport.
The decision about which airport to use can be cast as either an index function model
or a random utility model. In the index function approach, the agent makes a marginal
benefit Y marginal cost calculation based on the utility achieved by choosing to fly a
particular airline between one originYdestination pair instead of another. The difference
between benefit and cost is modeled as an unobservable variable y* such that
The error term is assumed to have a particular known distribution. The net benefit of
the choice is never observed, only the choice itself. Therefore, the observation is

and is known as the index function. The open issue is the parametric specification of
the error term.
The preponderance of airport choice studies rely on intercept interviews in the
airports. Consequently, the respondent has made an airline and airport choice from
among mutually exclusive alternatives. In this context, a multinomial model is
appropriate. The computationally least cumbersome model of
the error process is the multinomial logit. The logit model has the drawback that adding
an additional choice comes at the expense of reducing the probabilities of the original
choices, even when the added choice is no different from those already in the choice set.
This is known as the problem of the independence of irrelevant choices. To escape the
problem of independence of irrelevant alternatives inherent in the multinomial logit,
authors frequently use a nested multinomial logit. A numerically more cumbersome alternative is the multinomial probit model, in which the error term in equation (1) is assumed to have a normal
distribution. The multinomial probit model which overcomes the independence of
irrelevant alternatives problem is used here.
The individual studies and the methodological approach reviewed above all suppose
that the economic agent is choosing from among mutually exclusive alternatives. In the
phone survey conducted for the Philadelphia International Airport, the respondents
were not at a particular airport, having made a travel mode decision. Rather, they were
at home and were asked to reflect on all the choices that they had made in the previous
year. Hence, having flown from Newark Airport does not preclude having flown from
another airport in the region during the prior year, for example. With this in mind the
univariate binomial choice model of equation (1) becomes
for the four airports included in the Philadelphia International Airport study. The error vector
has a joint normal distribution. Since the net benefit of flying from a particular
airport is unobserved, the observed data on the dependent variable are the quadruplet
The open question is the modeling of the index function. The index function that is
used here is a mix of indirect utility arguments and tastes and preferences. The survey
results included data on the respondents_ age, gender, and income. The signs on age and
gender are indeterminate. The marginal effect of an increase in income on the
probability of using a more distant airport could be negative or positive. As an
individual_s income rises, she finds the opportunity cost of increased travel time to a
more distant airport to be a disincentive to using that airport. On the other hand,
service and fare might overcome that incentive.
The indirect utility arguments include whether the respondent_s travel agent tried to
steer them toward a particular airport choice and whether the respondent had obtained
the price of a comparable flight from an airport other than Philadelphia. The agent may
have insider information about fares and service at an airport that is not otherwise
available to the respondent. The advent of the Internet and its lower cost of information
gathering may weaken this argument. Consequently, the respondent values the agent_s
information and is more likely to use the recommended airport. One would expect that a
consumers price research would induce them to use the flight departing from the
cheaper airport.
Tastes and preferences are modeled from a sequence of questions regarding factors that
the traveler finds important in choice of airport. The survey3 asked for an ordered response
to eight such questions. Table 2 provides the questions and corresponding descriptive
statistics. Survey participants could rank an attribute of an airport and its services from 0
to 5; a response of 0 indicated that the factor was not at all important, a response of 5
indicated that the factor was extremely important in the decision making process.
If ability to choose a particular airline or fly an international carrier is important, then
one would expect that the respondent would be more likely to have flown out of JFK, all
other things equal, given its much wider choice of carriers. People for whom
distance to the airport is an important consideration would be less likely to have flown
out of JFK. If finding a non-stop flight is extremely important, then the respondent
should be more likely to have flown out of EWR. The folk wisdom is that because USAir
has dominated PHL for so long, it has the ability to charge higher fares. There is no
similar carrier dominance in the other three airports. Therefore, if price is an extremely
important consideration, then a respondent should be less likely to have flown out of
PHL in the preceding year. Since BWI has the greatest number of parking spaces per
passenger, one would expect that those for whom parking is extremely important would
be more likely to fly from BWI. All three airports are served by public transit. JFK has
the most difficult public transit system. It requires use of the subway and train changes.
On the other hand, there is a direct heavy rail link from Philadelphia_s major train
stations out to the airport, but hardly anyone uses the system. Ticket counter waiting
times at the different airports is not reported on a systematic basis. An alternative would
be ticket counters per passenger, but we were not able to locate that data.
The Data
In May and April 2000, a phone survey was conducted on behalf of the management
of the Philadelphia International Airport. Approximately 5,000 households in a market
region defined by the management of the Philadelphia International Airport were
contacted regarding their participation in the survey about travel outside the region and
modal choice. The phone contacts were selected from one of two sub-populations: those
who had previously expressed an interest in travel and those from the general
population. Those who had flown out of Philadelphia International Airport are overrepresented
in the sample.The resulting final sample had 1,100 usable responses, of
which 827 had traveled out of the region and 627 had flown out of one or more of the
major airports in the region.

</file>

<file= AmE06_J74>

For owners and pilots of V-tail Beechcraft Bonanzas, the fall of 1986 truly was the best of times and the worst of times. On the one hand, they were the proud possessors of arguably the most beautiful, and certainly the most widely recognized, private airplane in the world. On the other hand, the classic V-tails of their Model 35 Bonanzas had an indisputable tendency to come off at inopportune moments, with predictably untidy results. Owners of straight-tail Bonanzas were somewhat better off: neither the Model 33, the sort of frumpy, pillared, sedan version of the sexy Model 35 "hardtop," nor the stretched Model 36, the station-wagon version, had exhibited any problems with in-flight tail failure.1 How the Bonanza community-owners and pilots, their association (the American Bonanza Society [ABS]), the Beech Aircraft Corporation, and responsible regulatory agencies (the National Transportation Safety Board [NTSB] and the Federal Aviation Administration [FAA])-responded to the problem of V-tail Bonanza inflight breakups provides a nice if not terribly exceptional case study of a technological crisis and its resolution. But what makes this case different is the insight it can offer into how ordinary consumers-here, owners and pilots and prospective purchasers of these airplanes-not only participated in and responded to crisis but also thought about technology.

A lot of recent scholarship in the history of technology has moved away from traditional concerns with science, research and development, invention, innovation, engineering, and production to the social, broadly construed to include the roles of workers, users, and consumers in shaping, coconstructing, or negotiating technological artifacts and practices. Results on the whole have been salutary, but despite some marvelously imaginative uses of nonobvious data to make inferences about the thoughts and life-worlds of ordinary people, evidence for what the simple folk do or think remains sparse and to some extent suspect. It's virtually impossible to assess sample bias for populations who ordinarily leave no systematic records.

The V-tail Bonanza controversy provides a natural experiment that goes a long way toward addressing this difficulty.4 Specific variants of Model 35 (V-tail) and Model 33 (straight-tail) Bonanzas are absolutely identical in all respects save for their tail structures, while the Model 36 (also straight-tail) is very nearly identical. Ceteris paribus really holds. This fact of the matter, combined with the particular character of the used airplane market, permits the use of market and price data to make inferences about how the entire relevant community-all of the simple folk-reacted to the emerging crisis and came to credit its resolution. Bonanza owners and pilots represent a class of knowledgeable technological communities, and market data can reveal how such communities collectively attend to expertise, respond to institutionalized authority, and, most important, assess empirical evidence-how they think about technology.
The Bonanza's Tail

The Beechcraft Bonanza has been the paragon of private, single-engined airplanes for well over half a century, and has been in continuous production since 1947, longer than any other airplane, of any type, anywhere in the world, ever. Initial design of the Model 35 Bonanza began late in 1944; its first flight occurred in December 1945 and certification and first deliveries in March 1947. The Bonanza set new standards for private airplane aerodynamic cleanliness, structural integrity and strength, build quality, speed, efficiency, handling, and style, all still largely unsurpassed. When first introduced, the Model 35 Bonanza was significantly faster than any production airplane in the world with equal installed horsepower, and, among all private or commercial airplanes, notably slower only than Beechcraft's own twin-engined D18S Twin Beech, Lockheed's big and expensive L-18 Lodestar, and a handful of much larger and more powerful next-generation airliners, such as the Lockheed Constellation, just then coming into service.

The Bonanza's trademark "V" tail, which made it so distinctive, was adopted both to save weight and to reduce drag (two tail surfaces rather than three). The aircraft's flight qualities-the extraordinary lightness and harmony of its controls-were reported to have deliberately mimicked those of what fighter pilots regarded as the best-handling of the World War II fighter planes, the British Supermarine Spitfire. In building and selling the Bonanza, Beech exploited the organizational capabilities it had developed in producing and marketing the classic Twin Beech 18 (also an all-metal, stressedskin design, and itself in continuous production from 1937 to 1969), as well as capital equipment and expertise acquired during the war. The Bonanza used new, custom-developed engines, and Beech developed new, high-precision Class A production tooling in anticipation of a long production life for the airplane. They guessed right. By 1982, nearly fifteen thousand Bonanzas of all types had been built, including almost 10,400 V-tail Model 35s.

As would be expected over so long a production run, Bonanzas and their derivatives were built in a bewildering array of variants; indeed, all current Beech propeller-driven airplanes trace their lineage by one path or another back to the Bonanza. The Bonanza Model 35 itself went through eighteen major iterations, from the original of 1947 to the V35B of 1970- 82. In addition. Beech introduced the lower-priced, lower-powered, "decontented" Model 33 Debonair in 1959 to compete with the Piper Comanche (1957) and the Cessna 210 (1959), both of which offered comparable (hut slightly inferior) performance to, but were priced substantially below, the Model 35 Bonanza. The Model 33 was structurally nearly identical to the then current V-tail Model K35, except that it used a conventional cruciform tail to even further differentiate it from the more prestigious Bonanza. Similarly, Cessna's introduction of the six-seat Model 206 Stationair in 1963 induced Beech to counter (belatedly, in 1968) with a "utility" version of the Bonanza, the stretched, six-seat Model 36, which also used the Model 33's straight tail.

Initially, neither the Model 33 nor the Model 36 were market successes. People who paid Beechcraft prices didn't want "decontented" airplanes, they wanted Bonanzas. Thus the Debonair quietly became the Model E33 Bonanza in 1968. With introduction of the "long cabin" F33A in 1971, the straight-tail (F33A) and V-tail (V35B) Bonanzas became absolutely identical, except for their tails; engines, propellers, wings, fuselage structure, interiors, landing gear, standard avionics, optional equipment, and gross weights were exactly the same." The two airplanes were even priced identically new. The Model 36 had become the A36 in 1970, and likewise had been upgraded to full Bonanza "plushness." The A36, stretched 10 inches forward of the wing but identical to the F33A aft, was priced above either of the short fuselage airplanes: it offered not only six usable seats and rear cabin access through double doors, but also a higher gross weight and a larger center of gravity envelope, although at a slight performance penalty. In power and equipment, the A36 was identical to the other two airplanes, and quickly became the best-selling Bonanza. For these airplanes, ceteris paribus is the fact of the matter.14 And despite competition from Cessna, Mooney, Piper, and, latterly, France's SOCATA (Aérospatiale), the Beechcraft Bonanza remains the ne plus ultra of private, piston-engined airplanes.

CONTROVERSY AND CRISIS

In the late 1970s and early 1980s, however, it transpired that the V-tail Bonanzas had a higher rate of in-flight breakup than either of the straight-tail variants. The difference was small but statistically significant. The first overt public inkling of trouble came in an April 1979 Aviation Consumer article on private airplane Fatal In-Flight Airframe Failure Accident (FIFAFA) rates. The author, Brent Silver, claimed that National Transportation Safety Board (NTSB) findings of cause for private airplane crashes, which so often came down to pilot error, could be masking structural deficiencies in the airplanes. Silver continued to deconstruct the Bonanza's tail in a February 1980 article that explicitly attacked the structural integrity of all V-tail Bonanzas and reemphasized the statistical difference in FIFAFA rates between the V-tail and straight-tail airplanes. That same spring CBS spread the bad news in a muckraking 60 Minutes segment. Response from Bonanza owners and pilots, and from Beechcraft, was swift and angry, with the president of the American Bonanza Society (ABS) reporting that "The phone at ABS Headquarters has been jumping off the wall."

Beech issued a strongly worded rebuttal on 12 June 1980, which nicely framed most of the issues that would animate the controversy for the next seven years.19 First, the statistical data was suspect. Aviation Consumer had computed FIFAFA rates using FAA estimates of hours flown per year (derived from survey data), which even the magazine acknowledged were likely wrong by at least 10 percent. For this reason, Beech chose to calculate accident rates based on total aircraft population, per aircraft rather than per estimated flight-hour. This choice also biased the numbers, but in the V-tail's favor, since it tacitly assumed that older aircraft were flown as much as newer aircraft.20 Beech looked specifically at "excessive speed" accidents, typically where the pilot had lost control of the airplane, which everyone agreed was the normal precursor to in-flight structure failure. According to Beech, between 1972 and 1979 the rate per year of excessive speed accidents as a function of airplane population was:

V-tail 0.00135

Conventional tail 0.00090

In the same period, annual rates of in-flight airframe separation at excessive speed were:

V-tail 0.00100 (55 over 8 years)

Conventional tail 0.00012 (2 over 8 years)

Annual rates of ground impact at excessive speed were:

V-tail 0.00035 (19 over 8 years)

Conventional tail 0.00078 (13 over 8 years)

With regard to excessive speed accidents overall, Beech argued, "In relation to population, the difference of 0.00045 between the V-tail and conventional tail . . . is, in the opinion of Beech engineers, accounted for by factors which are difficult to assess, such as the fact that the average age of the V-tail fleet is far greater than the average age of the conventional tail fleet, with related differences in IFR instrumentation, airframe, power plant and maintenance."

That the V-tail airplanes suffered in-flight airframe separation an order of magnitude more often than conventional-tailed ones did, but that conventional-tailed airplanes hit the ground intact more than twice as often, was more problematic. Beech offered a bizarre rationale: "In the opinion of Beech engineers, the V-tail aircraft retains full maneuverability deeper into the spectrum of flight at excessive speeds than does the conventional tail. This is not to say that the conventional tail becomes completely rigid or unmaneuverable. Rather, the conventional tail tends not to provide or retain sufficient maneuverability to avoid ground impact at excessive speeds. The V-tail does, but at the cost of imposing loads on the airframe that are beyond its design strength."

On other aspects of the issue, Beech and the critics of the V-tail were in broad agreement. As Beech pointed out, 92 percent of the excessive speed accidents occurred under IFR (instrument flight rules) conditions. Over half (52 percent) occurred after VFR-rated pilots (visual flight rules-rated, that is, not qualified to fly on instruments) flew into IFR conditions. Fully 40 percent resulted from flying into thunderstorms, which no light airplane, and not many heavy ones, could survive.

In addition to weather and pilot error, four other factors were thought to be possible causes of loss-of-control accidents. First, on the V-tail Bonanza ruddervator, balance is especially critical; as little as two ounces out of specification can result in potentially destructive ruddervator flutter. Second, deteriorated or improperly rigged control cables can have the same effect. Balance and maintenance are critical. Third, the 35-series airplanes have tight center-of-gravity limits and are particularly easy to load past their aft limit, which results in a reduction in pitch stability and makes loss of control easier. Finally, the same exquisite control lightness that makes the Bonanza such a delight to fly also makes it easy to lose control of under less than ideal circumstances, while the aerodynamic slipperiness of the airplane permits it to accelerate quickly to unsurvivable speeds. These last two factors, of course, apply equally to the F33A.

Despite these areas of agreement, the statistically robust discrepancy between V-tail and straight-tail FIFAFA rates remained: the V-tail airplanes came apart in midair fully an order of magnitude more often than otherwise identical straight-tail airplanes.

</file>

<file= AmE06_J75>

why cite?
There are three reasons to cite the materials you use:
To give credit to others' work and ideas, whether you agree with
them or not. When you use their words, you must give them credit
by using both quotation marks and citations.
To show readers the materials on which you base your analysis,
your narrative, or your conclusions.
To guide readers to the materials you have used so they can examine
it for themselves. Their interest might be to confirm your work,
to challenge it, or simply to explore it further.
Taken together, these citations fully disclose your sources. That's important
for academic integrity in several ways.
First, good citations parcel out credit. Some belongs to you for the
original work you did; you need to take full responsibility for it. Some
belongs to others for their words, ideas, data, drawings, or other work. You
need to acknowledge it, openly and explicitly.
Second, if you relied on others' work in order to tell your story, explain
your topic, or document your conclusions, you need to say exactly what
you used. Take a simple paper about World War I. No one writing today
learned about it firsthand.Whatwe know,we learned by reading books and
articles, by examining original documents and news reports, by listening
to oral histories, by reviewing data compiled by military historians, and
perhaps by viewing photographs or movies.When we write about the war,
then, we should say how we acquired our information. The only exception
is "commonly known information," something that everyone in the field
clearly understands and that does not require any substantiation.1 There's
no need for a footnote to prove Woodrow Wilson was actually president
of the United States. But if you referred to his speech declaring war, you
would need a proper citation. If you used his words, you'd need quotation
marks, too.
Third, your readers may want to pursue a particular issue you cover.
Citations should lead them to the right sources, whether those are books,
interviews, archival documents,Web sites, poems, or paintings. That guidance
serves several purposes. Skeptical readersmay doubt the basis for your
work or your conclusions. Others may simply want to double-check them
or do more research on the topic. Your citations should point the way.
What citations should not do is prance about, showing off your knowledge
without adding to the reader's. That's just bragging.
Beyond this question of style (and good manners), there is the basic
issue of honesty. Citations should never mislead your readers. There are
lots of ways to mislead or misdirect your readers; accurate citations avoid
them. For example, they should not imply you read books or articles when
you really didn't. They should not imply you spent days in the archives
deciphering original documents when you actually read them in an edited
book or, worse, when you "borrowed" the citation from a scholar who
did study the originals. Of course, it's fine to cite that author or an edited
collection. That's accurate. It's fine to burrow into the archives and read
the original yourself. It's dishonest, though, to write citations that only
pretend you did.
Good citations should reveal your sources, not conceal them. They
should honestly showthe research you conducted. That means they should
give credit where credit is due, disclose the materials on which you base
yourwork, and guide readers to that material so they can explore it further.
Citations like these accurately reflect your work and that of others. They
show the ground on which you stand.

the basics of citation
Acknowledging your sources is crucial to doing honest academic work.
That means citing them properly, using one of several styles. The one you
choose depends on your field, your professor's advice if you are a student,
and your own preferences.
There are three major citation styles:
Chicago (or Turabian), used in many fields
MLA, used in the humanities
APA, used in social sciences, education, engineering, and business
Several sciences have also developed their own distinctive styles:
CSE for the biological sciences
AMA for the biomedical sciences, medicine, and nursing
ACS for chemistry
AIP for physics, plus other styles for astrophysics and astronomy
AMS for mathematics and computer sciences
Legal citations are different from any of these. So are anthropology citations.
I will cover each one, providing clear directions and plenty of examples
so you won't have any trouble writing correct citations. That way, you can
concentrate on your paper, not on the type of citation you're using. I'll
cover each style separately so you can turn directly to the one you need.
Using this information, you'll be able to cite books, articles, Web sites,
films, musical performances, government documents - whatever you use
in your papers.
Why would you ever want to use different citation styles? Why can't
you just pick one and stick with it? Because different fields won't let you.
They have designed citation styles to meet their special needs, whether it's
genetics or German, and you'll just have to use them. In some sciences,
for instance, proper citations list only the author, journal, and pages. They
omit the article's title. If you did that in the humanities or social sciences,
you'd be incorrect because proper citations for those fields require the title.
Go figure.
Compare these bibliographic citations for an article of mine:
Chicago Lipson, Charles. "Why Are Some International Agreements
Informal?'' International Organization 45 (Autumn 1991):
495 - 538.
APA Lipson, C. (1991). Why are some international agreements informal?
International Organization, 45, 495 - 538.
ACS Lipson, C. Int. Org. 1991, 45, 495.
None of these is complicated, but they are different. When you leave
the chemistry lab to take a course on Shakespeare, you'll leave behind your
citation style as well as your beakers. Not to worry. For chemistry papers,
just turn to chapter 9. For Shakespeare, turn to chapter 4, which covers
MLA citations for the humanities. Both chapters include lots of examples,
presented in simple tables, so it won't be "double, double toil and trouble."
Despite their differences, all these citation styles have the same basic goals:
to identify and credit the sources you use; and
to give readers specific information so they can go to these sources
themselves, if they wish.
Fortunately, the different styles include a lot of the same information.
That means you can write down the same things as you take notes, without
worrying about what kind of citations you will ultimately use. You should
writedownthat information as soon as youstart taking notesonanewbook
or article. If you photocopy an article, write all the reference information
on the first page. If you do it first, you won't forget. You'll need it later for
citations.
How these citations will ultimately look depends on which style you
use. Chicago notes are either complete citations or shortened versions
plus a complete description in the bibliography or in a previous note.
Their name comes from their original source, The ChicagoManual of Style,
published by the University of Chicago Press. This format is sometimes
called "Turabian" after a popular book based on that style, Kate Turabian's
A Manual for Writers of Term Papers, Theses, and Dissertations.

If you use complete-citation notes, you might not need a bibliography
at all since the first note for each item includes all the necessary data. If you
use the shortened form, though, you definitely need a bibliography since
the notes skip vital information.
Whether you use complete-citation notes or the shortened version, you
can place them either at the bottom of each page or at the end of the document.
Footnotes and endnotes are identical, except for their placement.
Footnotes appear on the same page as the citation in the text. Endnotes are
bunched together at the end of the paper, article, chapter, or book. Word
processors give you an easy choice between the two.
MLA, APA, and the science citation styles were developed to provide
alternative ways of referencing materials. They use in-text citations such
as (Stewart 154) or (Stewart, 2004) with full information provided only in
a reference list at the end.2 Because these in-text citations are brief, they
require a full bibliography. I'll describe each style in detail and provide lots
of examples, just as I will for Chicago citations.
In case you are wondering about the initials: APA stands for the American
Psychological Association, which uses this style in its professional journals.
MLA stands for the Modern Language Association. Both styles have been
adopted well beyond their original fields. APA is widely used in the social
sciences,MLAin the humanities. Chicago citations arewidely used in both.
I will discuss the science styles (and what their initials mean) a little later.
Your department, school, or publisher may prefer one style or even
require it, or they might leave it up to you. Check on that as soon as you
begin handing in papers with citations. Why not do it consistently from
the beginning?
Tip on selecting a citation style: Check with your teachers in each class to find
out what style citations they prefer. Then use that style consistently.
Speaking of consistency . . . it's an important aspect of footnoting. Stick
with the same abbreviations, capitalizations, and don't mix styles within
a paper. It's easy to write "Volume" in one footnote, "Vol." in another,
and "vol." in a third. We all do it, and then we have to correct it. We all

abbreviate "chapter" as both "chap." and "ch." Just try your best the first
time around and then go back and fix the mistakes when you revise. That's
why they invented the search-and-replace function.
My goal here is to provide a one-stop reference so that you can handle
nearly all citation issues you'll face, regardless of which style you use and
what kinds of items you cite. For each style, I'll showyou howto cite books,
articles, unpublished papers,Web sites, and lots more. For specialized documents,
such asmusical scores or scientific preprints, I show citations only
in the fields that actually use them. Physicists often cite preprints, but they
don't cite Beethoven. The physics chapter reflects those needs. Students
in the humanities not only cite Beethoven; they cite dance performances,
plays, and poems. I have included MLA citations for all of them. In case
you need to cite something well off the beaten path, I'll explain where to
find additional information for each style.
hanging indents
One final point about shared bibliographic style. Most bibliographies - 
Chicago, MLA, APA, and some of the sciences - use a special style known
as "hanging indents." This applies only to the bibliography and not to
footnotes or endnotes. It is the opposite of regular paragraph indention,
where the first line is indented and the rest are regular length. In a hanging
indent, the first line of each citation is regular length and the rest are
indented. For example:
Rothenberg, Gunther E. "Maurice of Nassau, Gustavus Adolphus,
Raimondo Montecuccoli, and the 'Military Revolution' of
the Seventeenth Century." In Makers of Modern Strategy from
Machiavelli to the Nuclear Age, edited by Peter Paret, 32 - 63.
Princeton, NJ: Princeton University Press, 1986.
Spooner, Frank C. Risks at Sea: Amsterdam Insurance and Maritime
Europe, 1766 - 1780. Cambridge: Cambridge University Press, 1983.
There's a good reason for this unusual format. Hanging indents are
designed to make it easy to skim down the list of references and see the
authors' names. To remind you to use this format, I'll use it myself when
I illustrate references in the citation styles that use it. (The only ones that
don't use hanging indents are science styles with numbered citations and
legal citations. It's actually not complicated, and I'll explain it later.)

To make the authors' names stand out further, most bibliographies list
their last names first. If an author's name is repeated, however, the styles
differ.APArepeats the full name for each citation.MLAuses three hyphens,
followed by a period. Chicago uses three em dashes (that is, long dashes),
followed by a period.

Lipson, Charles. Barbecue, Cole Slaw, and Extra Hot Sauce. Midnight,
MS: Hushpuppy, 2006.
More Gumbo, Please. Thibodeaux, LA: Andouille Press, 2005.

</file>

<file= AmE06_J76>

Imperial Aviation, 1909 - 1917
The Dawn of Russian Aviation
Tsars of the air in the land of the Tsars [para. 24-38]  
From the Russian State Duma, 22 February 1910:

At a time when every country has flown by airplane and when private enterprise has taken part in [developing aviation] what have we done in Russia? Not a single one of our nation's people has flown, and yet police laws against the use of airplanes already exist and aviation is already under police surveillance. 

 -  Vasilii Maklakov, Duma Deputy

Duma member Maklakov is indignant that no one in Russia has flown and yet laws governing aviation have already been established. What is so bad about that? We all understand that before we can allow people to fly, we must first teach the police to fly after them. [Applause from the right, laughter from all benches]

 -  Nikolai Mar'kov, Duma Deputy

On the morning of 25 July 1909, Louis Blériot piloted a twenty-four-and-a-half-horsepower Blériot XI monoplane across the English Channel from Baraques, France, toward the Shakespeare Cliff just west of Dover, England. The thirty-six-and-a-half-minute flight was not easy. Blériot's "heavier-than-air" craft was just barely so, and the wooden and canvas structure was continually buffeted by the strong air currents that swirled across the Channel. Wrestling to maintain control of his plane as it shuddered and swayed over the surface of the water, Blériot challenged his aircraft (and the force of gravity) in an attempt to bridge the narrow divide that separated England from the Continental mainland.
 
Blériot was not the first pilot to undertake this feat. Emboldened by a £1,000 prize offered by London's Daily Mail, numerous fliers had made public their intentions to become the first individual to achieve the Channel crossing. The most recent attempt had taken place six days earlier. It ended in near tragedy when an Antoinette IV flown by Englishman Hubert Latham experienced mechanical failure and plunged some 300 feet into the waters below. Miraculously, Latham survived with only minor injuries. Although he vowed to renew his efforts as soon as he could obtain a new aircraft, fate smiled on the Frenchman.
 
After crash-landing in a field not far from Dover Castle, Blériot was treated to a hero's welcome. Hastily assembled and well-attended receptions in London and then Paris celebrated the "miraculous flight" of the "aviator - genius" as both British and French public opinion succumbed to an air-minded delirium. For weeks, French newspapers trumpeted Blériot's exploit as "a great French victory" and compared the "conqueror of the English Channel" to such cultural icons as Lavoisier, Pasteur, and Curie. Paris was "seized by a violent attack of Blériot fever." Captivated by "the most magnificent enterprise a century had ever seen," one daily proclaimed Blériot's flight an expression of the "imperishable genius" of the French race. The Times of London was more reserved. While acknowledging the "merit" of the "plucky" Frenchman's flight, the paper observed that Blériot "had been, so to speak, shown the way by Latham." Without the example set by the English flier, the Times suggested, the French aeronaut would never have met with success. Even so, the glory belonged to Blériot.
The feverish excitement that gripped England and France quickly spread to Russia. For weeks the St. Petersburg and Moscow press had paid only passing attention to the race for the Channel, but with word of Blériot's accomplishment the nation's newspapers rushed to cover the historic event. Reporters sent daily telegrams from London and Paris recounting details of the flight and the countless receptions held in honor of the aviator. As exhilarated newsmen proclaimed the triumph of the "Tsar of the Air," it became increasingly obvious that Blériot's flight had "opened a new chapter in the annals of human history." No less so than the British or French, the Russian press acclaimed Blériot's achievement as an adventure of the ages.
 
The enthusiastic response to Blériot's exploit was only the most recent expression of the nation's long-standing curiosity with technologies of flight. As early as the reign of Catherine II (1762 - 96), Russian observers had looked with considerable interest at Western European aeronautical advances. While the brothers étienne and Joseph Montgolfier entertained French crowds with demonstrations of their machine aérostatique during the summer and fall of 1783, tsarist officials stationed in Paris speculated on the political and commercial revolutions that might result once aerial travel became a practical reality. Inspired by the Montgolfiers' example and informed through lectures organized by visiting French and German scientists, Russian inventors undertook their own experiments, successfully launching a small unmanned hot-air balloon in St. Petersburg in November 1783. Although this first effort to duplicate European successes produced no new scientific advances, it testified to Russians' clear desire to participate in the emerging new field.

Russian interest in things aeronautical was not confined to scientists and statesmen. In the years that followed the Montgolfiers' first ascent, thousands [Page 13] turned out to witness the wonders of ballooning as touring exhibits were undertaken by Frenchmen Jean-Pierre Blanchard and Jacques Garnerin, who performed manned and unmanned launches in St. Petersburg and Moscow. Accompanying the arrival of the foreign aeronauts, publishers edified and entertained Russia's small reading public with treatises, poems, and stories dedicated to aeronautical themes. Although most of these were reproductions of recently published European texts, a few were original Russian compositions. Typical was the 1787 short story "Arrival of an Air-Balloon in Olympus," which recounted the collective effort of a group of aeronauts to ascend into the heavens by means of a balloon. Affronted by the humans' impudence, Jupiter prepares to strike them down with bolts of lighting only to be dissuaded by his daughter, the goddess Minerva. In the end, the aeronauts reach the heights of Mount Olympus, where they are feted by the gods and proclaimed the "new Argonauts." The convergence of public spectacle and popular fiction, together with the production of imported stage pieces like the German opera Die Lufte-Bälle, would lead one contemporary to conclude that, by 1805, aeronautics had become all the rage in the Imperial capital.

As was true in the West, lighter-than-air travel was a recurrent (if not necessarily predominant) preoccupation of Russian scientists and state officials throughout the nineteenth century. Unfortunately, hampered by the nation's nonexistent industrial base and the inherent limitations of the technology itself, early Russian efforts to develop practical devices for commercial and military applications fell far short of their lofty goals. By the 1860s, however, as advances in textile manufacturing and the production of hydrogen made aerostats more reliable and effective, Russian officials began to devote more attention to aeronautics. Intrigued by the appearance of tethered balloons during the American Civil War and the Franco - Prussian War, the Russian War Ministry established a series of commissions to explore the systematic use of lighter-than-air craft in battle. Within a generation these commissions had produced tangible results. In 1890 an Aeronautical Training Park was established in St. Petersburg, and manned balloons were used by the Imperial armed forces to conduct aerial reconnaissance during the Russo - Japanese War (1904 - 5).

The institutionalization of Russian aeronautics at the end of the nineteenth century meant that by the summer of 1909 residents of Russia's urban centers, like their Western European counterparts, were already familiar with the new language of flight. Still, the popular excitement generated by news of Blériot's exploit was unprecedented. As the nation's newspapers raced to keep track of the most recent developments, even casual readers in St. Petersburg or Moscow could not help but be informed of the changes taking place in the "century of speed." From regular features discussing the potential profits (and possible perils) associated with the art of flying to documentary reports on the latest technical innovations and fanciful essays on the future of flight, the nation's press devoted increasing coverage to the battles being waged for the "conquest of the air." In response to the overwhelming demand for more information on the science of aviation, one major daily inaugurated a weekly column titled "Aeronautical Mail-Pouch" in which a resident aviation expert answered questions sent in by curious readers. For months queries poured in from all over the Empire, requesting definitions of general terminology, explanations of specific technical problems, and clarifications concerning current world aeronautical records. The paper even received an appeal from a peasant of Novgorod province, who wrote to request technical assistance for the flying machine that he was constructing. Unfortunately, the imprecise and muddled descriptions sent in by the half-literate peasant prevented the paper's specialist from providing the finishing formulae for what might otherwise have been a native Russian airplane.
The outbreak of Russia's air-minded delirium was not confined to the reading public. Vacationing in southwestern France at the time of the Channel crossing, Grand Duke Aleksandr Mikhailovich hailed the flight as an "epochal event." After telegraphing congratulations to Blériot, he sent a hastily written letter to the editors of leading Russian newspapers requesting their support in establishing a public subscription for the construction of a national air fleet. Convinced that the airplane was the weapon of the future, the grand duke would labor unceasingly for this new-found cause. Ultimately, he became the central patron of Imperial aviation. Similarly, one newly appointed member of the War Ministry, General Bren, proclaimed his own optimistic faith that aviation would quickly develop into a useful  military resource. Interviewed shortly after the flight, Bren announced his certain belief that the airplane represented the future. Much more so than the dirigible, he noted, "the airplane promises possible service to the army as it is less vulnerable, faster, and less expensive to build." As testament to his faith in the latest technology, the minister concluded his commentary with assurances that "once they are produced in sufficient numbers, [our] army will be well equipped with airplanes. I am a man of progress."

The progressive faith in the promise of flight revealed in the responses of the grand duke and the minister was not shared by every member of the Imperial state service. Less than a week after Blériot's flight, the Russian secret police [Okhrana] took decisive steps to forestall the introduction of flying machines into the land of the tsars. "With the goal of hindering criminal elements from accomplishing their projects with the aid of aeronautical devices," the Department of Police instructed the nation's border guards "to maintain strict surveillance in preventing the importation of aeronautical machines and their parts across the frontiers of the Russian Empire." Fearful that airplanes might be employed by revolutionaries to wage a campaign of terror against the state and its representatives, the Okhrana established a "Special Commission on the Means of Battling the Possible Implementation of Criminal Designs With the Assistance of Aeronautical Machines." The Commission convened a series of meetings during the late summer of 1909 to familiarize members of the Interior and the War Ministries with recent advances in aeronautical technology and to design a comprehensive policy for thwarting airborne crimes. Although the ban on the importation of airplanes to Russia was later overturned by the Ministry of Finance, the police did succeed in establishing a series of covert measures designed to restrict and control the Russian public's access to aeronautical technology. Among the more notable policies adopted by the state were instructions requiring all aeronautical clubs to register with police authorities in order that their members and airships might be more easily tracked. In addition, broad directives were issued to local gendarmes instructing them to "strictly monitor all flights as well as aviators and those attempting to learn to fly" within their jurisdictions. Similar commands were sent to the tsar's foreign agents, who were ordered to compile dossiers on the activities of European air clubs, especially those believed to possess ties to revolutionary organizations in Russia. The state's determination to impose control over private aviation was made further evident in late July 1909, when the secret police banned a newspaper advertisement that promised to deliver Blériot monoplanes ("the same plane that flew the Channel") for 5,000 rubles each to Petersburg purchasers.

</file>

<file= AmE06_J77>

Effective sustainable industrial development requires the building, managing, and maintaining of inter-organizational networks. These networks are comprised of the diverse stakeholders
that are necessary for a triple bottom line approach to development. This includes the use of techniques that create economic success for business enterprises by meeting social and environmental objectives. Adopting sustainable business practices generally involve changing institutional visions and operations on many levels. These changes include coordination and formation of networks of organizations (public, corporate, and environmental) to act as strategic stakeholders representing specific issues.
Given the number of institutional and administrative challenges associated with sustainable industrial development and production methods, there is an ongoing need for conceptual models that increase understanding and clarity about the various forms of inter-organizational collaborative partnerships among stakeholders (Hawken [1]). A number of researchers have referred to the network phenomena in public administration since as early as 1978 in terms of: issue networks (e.g., Heclo [2]; implementation structures (e.g., Hjern and Porter [3], Trist [4]; inter-organizational policy systems (e.g., Milward and Wamsley [5]); advocacy coalitions (e.g., Sabatier and Jenkins-Smith [6]); policy formation and implementation
(e.g., Rainey and Milward [7], Mandell [8], Marin and Mayntz [9], Bressers, O'Toole, and Richardson [10], Agranoff [11], O'Toole [12]); and self-governing institutional arrangements (e.g., Ostrom, Gardner, and Walker [13]). There is emerging evidence that the application of network structures and learning organizational models, when placed within the broader context of sustainable development and production, can be very productive (Wheeler [14]). Wheeler noted the case of Shell in Nigeria, where the inability to form productive networks among diverse stakeholders dramatically slowed oil production and cost the lives of many people.
The differences between ''traditional'' and network-based structures can be shown graphically as depicted in Fig. 1. In this figure, ''A'' is a traditional organizational structure. The
''dots'' symbolize departments, units of operation, factories, shareholders or even stakeholders, all within a traditional organization.
Notice that these units are enclosed and aligned by the framework or structure of their organization. They may
be connected but they can connect only within their organizational framework (inside the box), and unless the size of the framework is increased, there is a barrier that must be overcome to include units that are not inside the box. Metaphorically, this framework creates alignment inside, but can also be seen as a barrier to units trying to work outside the framework, or to units outside the framework trying to connect inside.
In this manner, traditional organizational structures inhibit diverse connections.
There are significant costs associated with developing and maintaining organizational structure overhead (it requires energy to build and maintain a wall), as well as the costs for connecting
outside of the box (you have to dig under, climb over, rill through or destroy the wall). These are the challenges of traditional organizational structures in a dynamic global economy
that is driven by the forces of ''creative destruction'' and rapid change (Hart and Milstein [15]).
''B'' in Fig. 1 graphically represents a connected network structure. The links or edges between the units or vertices are forged as needed. There is no expenditure for maintenance
of an organizational structure and thus, no barriers to entry or exit. The only energy required is to find connections to the network.
In the dynamic global economy, removing the barriers to swift change is clearly a major asset.
This paper demonstrates the utility of applying an interorganizational network (ION) model that incorporates dimensions of a network structure with a virtual Web organization.
As the network acquires the properties of a learning organization, by generating multiple feedback loops to the stakeholders, the emerging shared perspective enables increased productivity through the adoption of sustainable development and production methods. The challenges of a networked structure, which are addressed in this paper, include how to forge the connections and achieve alignment among those connections (Dorogovtsev and Mendes [16]).
The model presented here is practically illustrated by an analysis of the process that promoted ''green chemistry,'' to reduce aquatic toxicity and harmful environmental effects, in the North Carolina textile dyeing and finishing (wet processing) industry during the late 1980s. This case illustrates that
an ION structure provided a framework that was flexible enough to involve diverse stakeholders as resources in problem solving, even inside of traditional business enterprises.
This occurred because the traditional business organizations were treated as a single member of the network and the network provided the governance and structure to the dialogue and process, not the businesses. The ION was also effective in enabling stakeholders to resolve environmental regulatory non-compliance issues at minimal economic cost to the stakeholders involved.
2. Background on North Carolina's aquatic toxicity situation2
The wastewaters discharged into the rivers of North Carolina are regulated by federal (USEPA), State (NCDWQ), and local Publicly Owned Treatment Works (POTW) authorities as a tiered regulatory approach under the Federal Clean Water Act (CWA). The treated wastewaters are tested to ensure that the levels of organic and inorganic pollutants fall within a compliance limit that is based on the characteristics and ''best use'' of the water body receiving the treated wastewaters.
Since the advent of the CWA, wastewater treatment facilities have been under increasing pressure by stakeholders of the common resource to lower the concentrations and toxicity of the treated wastewater being discharged.
In 1982, the USEPA and the NCDWQ implemented a change in water quality testing that, for the first time, was based on aquatic bioassays. Bioassays take the treated wastewater and test it for acute toxicity and reproductive inhibition.
During this period of 1985e1995, over 50% of the wastewater being treated by POTW facilities in North Carolina came from textile dyeing and finishing operations. As the NCDWQ began to test POTW discharges that contained large amounts of textile dyeing and finishing process waters, it found a large portion of them were acutely toxic and an even larger portion produced discharges that were not suitable for reproduction of
aquatic life (Ausley [17]).
When the POTWs were found to have failed the toxicity compliance test, engineering firms were called in to give estimates on what changes in treatment plant design would be necessary to achieve a compliant, non-toxic wastewater. Costs to improve treatment plant designs were very large, estimated in the tens of millions of dollars, and it was unclear if the design changes could produce nontoxic treated effluents. Since budgetary constraints dictated that new treatment facilities were too costly, it was obvious that another approach was necessary.
It was also clear that these engineers, working within the mental models dictated by the framework of corporations, did not have the answers to resolving these challenges. Because of the nature of environmental regulations, NCDWQ was required to send notices of violation to the
POTWs, with fines levied. To avoid the fines, the POTW had to produce an action plan, which when accepted by the technical compliance branches of NCDWQ, produced a period of grace for the POTW while it resolved the problems, either by plant construction or by some other method. Since plant construction was not a viable economic option for many communities, ''some other method'' had to be devised. This resulted in opening dialogues with stakeholder groups, biologists and chemists, which were outside of the ''normal'' engineering based waste treatment conversations. Historically, the implementation of the 1974 Clean Water Act resulted in the engineering of additional filters on waste treatment facilities. These filters were costly to develop, produce and maintain and importantly, were not economically productive investments. As integrated approaches to pollution management matured, it became apparent that there was an alternative
to additional treatment for industrial waste streams, ''Pollution Prevention.'' Pollution Prevention (P-2) is the concept and approach that emphasizes rather than use wastewater filters, it is more prudent to redesign industrial processes to prevent the production of pollutants at their sources, in the first place. With the advent of environmental management systems in the mid 1990s, P-2 efforts became formalized and recognized
as additional tools for pollution abatement and were also called ''Cleaner Production.'' An example of the evolution of policy, P-2, and globalization can be found in an associated case of the textile industry in Demark. This case demonstrated the effects of reengineering processes, factories, and the chemicals used in textiles and the invention of new waste treatment methods. The case also demonstrated, sadly, that some of the industry left Denmark and moved into less regulated countries to avoid environmental costs of any kind (Sondergard [18]).
In the parallel North Carolina situation, historically and up to the mid-1990s, the textile industry was very important to the economic well being of the area. The challenge was to design an ad hoc inter-organizational network structure that included all the stakeholders to operate and supervise the P-2 effort and retain the industry as a viable economic force in the state. The network had to be designed to include controls and benchmarks that would allow NCDWQ to convince USEPA that a ''grace period'' could be allowed while the alliance of industry, POTW, NCDWQ, consultants, and stakeholders diagnosed the problem and developed the environmental management system. This involved elevating the level of confidence in a closely monitored dialogue-based process to the same level of respect given to the construction of additional waste treatment facilities.
3. The nature of the inter-organizational network in promoting sustainable development
An inter-organizational network (ION), such as the one required for North Carolina's Toxicity Prevention program, is a strategic partnership or alliance among the stakeholders who come together to collaboratively address and resolve mutual concerns regarding sustainable development and production methods. The ION is both larger and greater than the individual stakeholder organizations that comprise it, and the structure and processes of this superordinate entity directly impact stakeholder collaboration and consensus building efforts. For instance, the network that connected the various institutional stakeholders to NC's P-2 program had no hierarchical power and authority; it was a loosely coupled, dynamic political system, rather than a tightly bonded, homogeneous, hierarchically controlled system, however, critical stakeholders included the USEPA, NCDWQ, the local POTWs, NGOs, the public, the textile industry, and the textile industry supply chain.
Inter-organizational networks such as this one behave more as a group of affiliates than a traditional supply chain network.
They are like an organizational sandbox, where shifting roles and structures can be experimental. Furthermore, effective negotiations within IONs require the relaxing of stakeholder organizational boundaries, as the power structures are more democratic. Of the three types of networks (internal, stable, and dynamic) identified by Miles and Snow [19], the institutional arrangement among the stakeholders for sustainable development is most like a dynamic network, i.e., an ad hoc alliance among the strategic stakeholders. Each stakeholder organization
is independent and collaborates on a specific project or opportunity, based on its skills and expertise (Van Alstyne [20]). While it exists, this network is a ''highly decentralized and densely integrated social system that maximizes mutual influence and communication'' (Bovasso [21]). It may also act like a matrix structure utilized in many research and development settings, where resources are used as needs emerge, toward
the achievement of a common goal (Susskind [22]).
The inter-organizational arrangements required for NC's sustainable textile industry production shared the key principles of networked organizations (Lipnack and Stamps [23], Surowiecki [24]).
3.1. Unifying purpose
The stakeholder organizations became networked when there was a consensus about the value and goal of
collaboration on sustainable industrial development and production practices and pollution prevention.
3.2. Independent members
Each member of the network, whether an individual or a stakeholder organization, could stand on its own while benefiting from being part of the whole.
3.3. Voluntary links
The links that connected sustainable development stakeholders in various combinations were far more profuse and omni-directional than in other types of organizations. As communication pathways increased, the links continued to grow and develop.
3.4. Multiple leaders
The inter-organizational network tended to be leaderful, rather than leaderless. Each person or group (stakeholder) in the network had something unique to contribute. With more than one leader, the network, as a whole, had great resilience.

</file>

<file= AmE06_J78>

Gene function is typically evaluated by sampling the continuum of gene expression at only a few discrete points corresponding to gene knockout or overexpression. We argue that this characterization is incomplete and present a library of engineered promoters of varying strengths obtained through mutagenesis of a constitutive promoter A multifaceted characterization of the library, especially at the single-cell level to ensure homogeneity, permitted quantitative assessment correlating the effect of gene expression levels to improved growth and product formation phenotypes in Escherichia coli. Integration of these promoters into the chromo-some can a flow for a quantitative accurate assessment of genetic control. To this end, we used the characterized library of promoters to assess the impact of phosphoenolpyruvate carboxylase levels on growth yield and deoxyxylulose-Psynthase levels on Iycopene production. The multifaceted characterization of promoter strength enabled identification of optimal expression levels for ppc and dxs, which maximized the desired phenotype. Additionally, in a strain preengineered to produce Iycopene, the response to deoxy-xylulose-Psynthase levels was linear at all levels tested, indicative of a rate-limiting step, unlike the parental strain, which exhibited an optimum expression level, illustrating that optimal gene expression levels are variable and dependent on the genetic background of the strain. This promoter library concept is illustrated as being generalizable to eukaryotic organisms (Saccharo-myces cerevisiae) and thus constitutes an integral platform for functional genomics, synthetic biology, and metabolic engineering endeavours F
Protein engineering via directed evolution and gene shuffling r (1, 2) has been extensively applied for the systematic improvement of protein properties such as antibody-binding affinity (3), enzyme regulation (4), and increased or diverse substrate specificity (5). A similar approach whereby continuously improved mutants are generated along a selection-defined trajectory in the sequence space can also be applied for the systematic improvement or modification of other types of biological sequences, e.g., ribozymes (6 7). We show here that promoters can also be engineered via directed evolution to achieve precise strengths and regulation and, by extension, can constitute libraries exhibiting broad ranges of genetic control. Typically, the deletion( 8) and strong overexpression (9 ) of genes have been the principal strategies for elucidation of gene function. These two methods sample the continuum of gene expression at only a few discrete points, determined by experimental feasibility (10) and not necessarily biological significance. Thus, the fi dependency of phenotype on gene expression may not be accessible due to the limitations inherent in these methods. Gene expression is controlled by a number of factors in the cell, including promoter strength, and transacting factors, cell growths tage the expression level of various RNA polymerase-associated factors, and other gene-level regulation. Of course, gene expression may not always correspond with enzymatic activity given protein level regulation, which may also be present. Nevertheless, several groups have attempted to control gene expression through the creation of promoter libraries ( 11-13). In this work, we present the development of a fully characterized, homogeneous, broad-range, functional promoter library and demonstrate its applicability to the analysis of such a genetic control. By characterizing the strength of these promoters in a quantitative manner with various metrics and subsequently integrating these constructs into the genome, it is possible to deduce the precise impact of the gene dosage on the desired phenotype. An alternative method for controlling gene expression is through the use of a single inducible promoter tested at various levels of inducer. Although inducible promoters allow for a continuous control of expression at the macroscopic level, practical applications of these systems are limited by prohibitive inducer costs, hypersensitivity to inducer concentration, and transcriptional heterogeneity at the single-cell level (14, 15). The latter factor, in particular, can limit the effect of inducers in a culture to a simple increase of the number of cells expressing the gene of interest instead of the overexpression of the gene in all cells. Inducible systems are suitable in certain applications (e .g., recombinant protein overproduction) (16); however, the elucidation of gene function and genetic control on phenotype requires well characterized promoter libraries, which behave in a similar manner at the single-cell level. As a result, the creation of a promoter library based on a constitutive promoter would eliminate the need to regulate inducer concentrations and avoid heterogeneities in cellular response. Methods Strains and Media. In specified strains, lycopene expression was performed by using the p AC-LYC plasmid (17) and assayed as described (18). Assays trains were grown at 37°C with 225 in M9-minimal media (19) containing 5 g/ liter D-glucose. When necessary, the M9 media were supplemented with 0.1% c as amino acids. All other strains and propagations were cultured at 37°C in LB media. Media were supplemented with 68 ,ug/ml chloramphen-icol/20, ug/mlk anamycin/100,u g/mla. Glucose monitoring was conducted by using the r-Biopharm (Swansea, U.K.) kit. Cell density was monitored spectrophotometrically at 6 00 nm. All PCR products and restriction enzymes were purchased from New England Biolabs. M9 minimal salts were purchased from U S Biological (Swampscott, MA), and all remaining chemicals were from Sigma-Aldrich. Primers were purchased from Invitrogen, and sequence information is listed in Supporting List, which is published a s supporting in formation on the PNAS web site. Saccharomyces cerevisiaest rain B Y4741( MAT a; his3A1;le u2A0; met15A0; ura3/v0) used in this study w as obtained from EURO-SCARF (Frankfurt).It was cultivated in yeast extract/peptone/ dextrose medium (10 g of yeast extract per liter/20 g of Bacto Peptone( Becton Dickinson) per liter/20g of glucose per liter). F or yeast transformation, Frozen-EZY east Transformation II (Zymo Research, Orange, CA) was used. To select and grow yeast transformants bearing plasmids with URA3 as selectable marker, a yeast synthetic complete (YSC) medium was used containing 6.7 g of yeast nitrogen base (Difco) per liter, 20 g of glucose/liter, and a mixture of appropriate nucleotides and amino acids (CSM-URA, Qbiogene, Irvine,C A), referred here as to YSCU ra. Medium was supplemented with 1.5% agar for solid media. Yeast cells were routinely cultivated at 30°Ci at 200r pm. For sorting single cells (TEF promoter mutations) by FACS into microtiter plates, each well contained 200 , ulo f YSC Ura- supplemented with 10m g/liter ergosterol and 420 mg/liter Tween 80 (20). Library Construction. Nucleotidea naloguem utagenesis was carried out in the presence of 20 , uM 8-oxo-2'-deoxyguanosine (8 -oxo-dGTP) and 6-(2-deoxy-13-D-ribofuranosyl)-3,4-dihydro-8H-pyrimido-[4,5-c][1,2]oxazin-7-one (dPTP) (TriLink Bio Technologies) (21). By using plasmid p ZE-gfp (ASV) kindly provided by M. Elowitz( California Institute of Technology, Pasadena)a s template (22) along with the primers PL sense Aat II and P L_anti Eco RI, 10 and 30 amplification cycles with the primers mentioned above were performed. The 151-bpP CR products were purified by using the Gene Clean Spin Kit (Qbiogene). After digestion, the product was ligated at 16°C overnight and transformed into library efficiency E . coli DH5a (Invitrogen). Approximately 30,000 colonies were screened by eye from minimal media-casamino acid agar plates, and 200 colonies, spanning a wide range in fluorescent intensity, were picked from the plates. To create the TEF promoter mutation library for S . cerevisiae, the plasmid p 416-TEF-yE Citrine was used as a template for the error-prone PCR of the TEF1 promoter by using the primers TEF Sense and TEF Anti. The mix of purified mutagenized PCR products was transformed into yeast together with p 416-TEF, which was cut with SacI/XbaI before (in vivo cloning). The CEN/ARS plasmid P416- TEF (23), containing the native TEFI promoter from S . cerevisiae, the CYC1 terminator, and the URA3 gene as a selectable marker, was obtained from American Type Culture Collection. The plasmid pKT140 was obtained from EUROSCARF. This plasmid contains the coding sequence of yECitrine, a yeast codon-optimized version of the yellow fluorescent protein( 24), which was used as a reporter protein in this study. To clone the yECitrine gene downstream of the TEF promoter, the coding sequence of  yECitrine was amplified via PCR from the plasmid pKT140 by using the primer yEC3ense and yECAnti. The PCR product was cut with ClaI and XbaI and ligated to ClaI/XbaI restricted vector p416-TEF. The resulting plasmid is referred to as p416-TEF-yECitrine. Library Characterization Initial Characterization. Approximately20 , ulo f overnight cultures of library clones growing LB were used to inoculate 5 ml of M9G mediums upplemented with 0.1% wt/vol casamino acid (M9G/ CAA), and the cultures were grown at 37°C with orbital shaking. After 14 h, a sample of the culture was centrifuged at 18,000x g for 2 min, and the cells were resuspended in ice-cold water. Flow cytometry was performed on a Becton Dickinson FAC Scan, and the geometric mean of the fluorescence distribution of each clonal population was calculated. To ensure that bulk population-averaged measurements could reflect the underlying single-cell behavior, only clones with clean monovariate distributions of fluorescence were retained for further analysis. Twenty-seven clones were isolated in this way. Sequencing revealed that these 27 clones represented 22 unique promoter sequences. Promoter Strength Metric. Shake flasks containing 50 ml of M9G/ CAA medium were inoculated with 1%v ol/vol of an overnight LB culture of a library clone. The culture turbidity( A6001 ,n,) and fluorescence (Packard Fusion microplate fluorescence reader, PerkinElmer) were monitored as a function of time. Fluorescence readings taken during the exponential growth phase were plotted as a function of turbidity. The best-fits lope to this line represent the exponential-phase steady-state concentration of  GFP, fss.  Because fss is affected by the cell growth rate oxygen-dependent maturation constant of GFP and the protease-mediated degradation of GFP as well as the promoter-driven synthesis of new GFP, it is not a suitable metric for promoter strength. Instead, we used a previously published dynamic model (25) that accounts for all of these factors. Under this model and under the assumption that the rate constant of protease-mediated degradation is the same for mature GFP as its precursor polypeptide, P, the rate of promoter-driven production of GFP can be expressed as in Eq. 1.
In Eq. 1, u is the growth rate, m is the maturation constant for the oxygen-dependent fluorophore activation of GFP, and D is the first-order rate constant for protease-mediated degradation. Estimates of m and D of 1.5 h-1 and 0.23 h-1, respectively (26, 27), were obtained from the literature. The parameters fss and u were measured separately for each member of the promoter library. P , in relative fluorescence units per absorbance unit per hour, was calculated from Eq. 1 for each clone. We performed duplicate cultures for each clone. Transcriptional Analysis. Cultures inoculated as previously were grown for 3 h, and the total RNA was extracted from a 1.5-ml sample with a commercial kit (RNEasy, Qiagen,V alencia, C A). All samples were diluted to a final concentration of 20 ,ug/ml and stored at-20°C. A commercial kit for RT-PCR ( iScript O ne-Ste RT-PCR Kit with SYBR green, Bio-Rad) was used with a charge-coupled device-equipped thermal cycler (iCycler,B io-Rad) for RT-PCRo f the<@ transcript. Primers were used at a final concentration of 100 nM, and 20 ng of RNA was used as template in each 50-,ul reaction. We performed duplicate cultures for each clone and duplicate extractions for each culture. The threshold cycles for each sample were calculated from the fluorescence data with proprietary software ( Bio-Rad)
Chloramphenicol Resistance. pZE-promoter plasmids were created by PCR of the chloramphenicolac etyltransferase (C AT) gene from pACYC184 by using primers CAT_Sense_MluI and CATAntiKpnI and ligated into the proper p ZE-promoter construct, which was previously digested by KpnIa nd MluI. Exponential phase cultures grown in LB supplemented with kanamycin were plated onto LB a gar supplemented with kanamycin and various concentrations of chloramphenicol ranging from 0 to 500 ,ug/ml. After overnight incubation at 37°C, the lowest concentration of chloramphenicol that inhibited the growth of a clone was recorded
TEF Promoter Library Characterization. Measuring of specific fluorescence of TEF promoter library in S. cerevisiae was performed by using cells harvested from the logarithmic phased growth in shake flasks. Fluorescence of yECitrine was measured by using a fluorescence spectrometer (H ITACHIF -2500) with an excitation wave length of 502 nm and an emission wave length of 532 nm. The specific fluorescence referred to here is the ratio of fluorescence level measured and the optical density at 600 nm measured in the same cuvette.

</file>

<file= AmE06_J79>

The NS1 protein of influenza A virus is a major virulence factor that is essential for pathogenesis. NS1 functions to impair innate and adaptive immunity by inhibiting host signal transduction and gene expression, but its mechanisms of action remain to be fully elucidated.
We show here that NS1 forms an inhibitory complex with NXF1/TAP, p15/NXT, Rae1/mrnp41, and E1B-AP5, which are key constituents of the mRNA export machinery that interact with both mRNAs and nucleoporins to direct mRNAs through the nuclear pore complex. Increased levels of NXF1, p15, or Rae1 revert the mRNA
export blockage induced by NS1. Furthermore, influenza virus down-regulates Nup98, a nucleoporin that is a docking site for mRNA export factors. Reduced expression of these mRNA export factors renders cells highly permissive to influenza virus replication, demonstrating that proper levels of key constituents of the mRNA export machinery protect against influenza virus replication. Because Nup98 and Rae1 are induced by interferons, downregulation of this pathway is likely a viral strategy to promote viral
replication. These findings demonstrate previously undescribed influenza-mediated viral - host interactions and provide insights
into potential molecular therapies that may interfere with influenza infection. 
Influenza viruses cause _500,000 deaths worldwide per year 1). Strains that are extremely pathogenic pose an even greater health threat. The 1918 influenza pandemic, for example, resulted
in _30 million deaths around the world (2). The non-structural 1 (NS1) protein of influenza A virus is a major irulence factor that inhibits host immune responses by disrupting
host signal transduction (3 - 11) and gene expression (12, 13) pathways. Genetic studies have shown that abrogation of NS1 function by mutation results in highly attenuated viruses, except
in animals lacking innate antiviral mechanisms (12, 14). These observations define NS1 as an essential element of viral virulence and suggest that its function represents a point of vulnerability
for inhibiting influenza viral pathogenesis. NS1 is a multifunctional protein that, in infected cells, is
localized in both the cytoplasm and in the nucleus (15). In the cytoplasm, NS1 inhibits innate immune response by forming a
complex with the pathogen sensor RIG-I (4, 7, 9, 10) and by targeting PKR and the RNase L pathway (6, 8). In addition, NS1 has been shown to activate phosphatidylinositol-3-kinase signaling,
which may be important for promoting viral replication (16). In the nucleus, NS1 inhibits host gene expression. This effect includes mRNA processing, which is mediated by interaction of
NS1 with polyadenylation factors (CPSF and PABII) (12, 13) and a putative splicing factor (NS1-BP) (17). The binding of NS1 to CPSF and PABII inhibits polyadenylation of host mRNAs, contributing to nuclear retention of these messages (12, 13).
However, the potent ability of NS1 to inhibit host mRNA nuclear export led us to explore a potential interaction of NS1 with the mRNA nuclear export machinery that could explain further the mechanism of mRNA export inhibition induced by NS1. The mRNA export receptors NXF1-p15 (TAP-NXT) are key constituents of the mRNA export machinery that is responsible
for nuclear exit of _70% of cellular mRNAs (18). This heterodimer interacts with both messenger ribonucleo protein particles and nuclear pore complex proteins [nucleoporins (Nups)]
to direct mRNAs through the nuclear pore complex (NPC) (18).
Another factor termed E1B-AP5, identified as a cellular protein that interacts with the adenovirus protein E1B-55 and implicated in mRNA export (19), is an hnRNP-like protein that likely mediates the interaction of NXF1 with mRNAs (20). In addition, other RNA-binding proteins such as the REF family (21) and SR splicing factors (22) also are known as adapters for the interaction
between NXF1 and mRNAs. Furthermore, the mRNA export factor Rae1/mrnp41/Gle2 (23 - 25), which shuttles between the nucleus and the cytoplasm, forms a complex with RNPs (24, 26), NXF1 (27), and the nucleoporin Nup98 (26). It has been proposed that Rae1 may recruit TAP to Nup98 to
mediate transport through the NPC (27). Nuclear export of mRNAs has been shown to be targeted by
viruses. For instance, the VSV matrix (M) protein is an inhibitor of bulk mRNA nuclear export (28 - 30), and we showed that it interacts with Rae1 (31), which is in complex with Nup98 (30,
31). However, this mRNA export block can be fully reverted by increasing the intracellular levels of Rae1 or partially reverted by inducing higher levels of Nup98-Nup96 (31, 32). Furthermore, treatment of cells with interferons, which up-regulates Nup98- Nup96 and Rae1, also can revert this mRNA export blockage mediated by VSV M protein (32). Recently, by targeting the
Nup96 gene in mice to produce a strain that expresses low levels
of Nup96 protein, we demonstrated a role for Nup96 in preferentially facilitating expression of IFN-regulated proteins (33). Thus, these findings showed a complex role for the mRNA export machinery in both viral-mediated cytotoxicity and antiviral response.
In this study, we investigated the mechanisms used by influenza A virus to disrupt the mRNA nuclear export machinery, leading to a block in mRNA export. In addition, we showed a
previously undescribed role for the mRNA export machinery in regulating influenza virus replication and cytotoxicity.

Results and Discussion
NS1 Interacts with Key Constituents of the mRNA Export Machinery.
To examine the importance of regulated bulk mRNA export in the context of viral infection, we determined the distribution of host poly(A) RNA in influenza virus-infected cells. We detected
inhibition of bulk host poly(A) RNA nuclear export in MDCK cells infected with the influenza virus strain A/WS/33 at a multiplicity of infection (MOI) of 1 (Fig. 1a). Expression of
influenza proteins clearly was detected by immunoblot analysis, starting at 4 h after infection (Fig. 1b). Inhibition of mRNA export was observed as early as 6 h after infection (Fig. 1a),
which was a time point at which we could easily detect infected cells by using polyclonal antibodies against all influenza proteins.
However, inhibition of mRNA export may begin even earlier, because a key constituent of the mRNA export machinery is degraded at early stages of infection (see below). The inhibition
of mRNA export was enhanced by 24 h of infection. Inhibition of export appeared to be selective for host mRNAs; viral proteins were expressed throughout the course of infection, presumably
because viral RNAs are exported through the Crm1 pathway (34), which does not involve NXF1, p15, or Rae1.
We then investigated whether NS1 interacts with the mRNA nuclear export machinery. Purified GST-NS1 or GST alone was incubated with cell extracts from 293T cells, which expressed
zz-p15. As shown in Fig. 2a, NS1 interacted with endogenous NXF1, p15, Rae1, E1B-AP5, and very weakly with Nup98. In contrast, we did not detect any interaction of NS1 with other
constituents of the nuclear transport machinery including Nup96, Nup62, Nup153, and Nup214 (Fig. 2 b and c). These results indicate that NS1 binds specifically to the mRNA export factors NXF1, p15, Rae1, and E1B-AP5, which are known to form a complex (27).
We then analyzed the domains of NS1 involved in the interactions with these constituents of the mRNA nuclear export machinery. A domain within the amino terminal region of NS1,
from amino acids 19 to 38, is required for NS1-mediated inhibition of mRNA nuclear export (35). Furthermore, a region at the carboxyl terminal domain of NS1, from amino acids 134 to 161, also was shown to be required for the inhibitory effect of NS1 on mRNA export (35). Deletion of the first 73 aa of NS1 inhibited its interaction with NXF1 and Rae1 and decreased considerably its interaction with E1B-AP5 (Fig. 2d). However, lack of the first 48 or 73 residues of NS1 retained significant interaction with endogenous p15 (Fig. 2d) or ectopically expressed
zz-p15 [supporting information (SI) Fig. 6]. On the other hand, NXF1, Rae1, and E1B-AP5 bound poorly to the aminoterminal domain of NS1 (amino acids 1 - 73), and p15 showed no
significant interaction with this domain. These results demonstrate that p15 interacts with the carboxyl-terminal domain of NS1 whereas NXF1, Rae1, and E1B-AP5 binding requires
residues within the amino- and carboxyl-terminal domains of NS1. Interaction of NS1 with NXF1, p15, and Rae1 is not dependent on RNA, because incubation of cell extracts with
RNase A did not affect these interactions (Fig. 2e). However, interaction of E1B-AP5 with NS1 was diminished in the present of RNase A (Fig. 2e), indicating partial dependence on RNA.
These findings demonstrate that NS1 is able to interact with constituents of the mRNA nuclear export machinery and that NS1 may cause a rearrangement of the NXF1/p15/E1B-AP5/ Rae1 complex, resulting in inhibition of mRNA nuclear export.
Alternatively, NS1 may mask binding sites of this mRNA export complex, preventing its proper interaction with other constituents of the mRNA export pathway.
Down-Regulation of Nup98 in Cells Infected with Influenza Virus. To investigate additional effects of influenza virus on the mRNA nuclear export machinery, we determined the levels of constituents of this machinery after infection of 293T and MDCK cells.
We observed that Nup98 levels were depleted markedly at _2 - 4 h after infection of 293T cells and by 24 h in MDCK cells (Fig.2 f and g). We did not detect any major differences in the levels
of Rae1, NXF1, E1B-AP5, Nup153, and Nup62 (Fig. 2 f and g).
We then tested whether the observed changes in Nup98 levels are a consequence of general inhibition of protein synthesis by influenza virus. Fig. 2h shows that Nup98 has a long half-life of
_26 h, which indicates that it is actively degraded during influenza virus infection. This degradation likely contributes to the inhibition of mRNA nuclear export observed upon influenza
infection.
Increased Expression of mRNA Export Factors Maintains Nuclear
Export of mRNA in the Presence of NS1. To determine whether blocking mRNA nuclear export is critical for influenza virus mediated-inhibition of host gene expression, we tested whether  increasing expression of mRNA export factors could prevent this inhibition. As shown in Fig. 3a, NXF1-p15 reversed the dosedependent inhibition of NS1. Inhibition of gene expression also
was reversed when Rae1, Nup98, NXF1, or p15 were overexpressed individually, but it was not altered by overexpression of Nup96 (Fig. 3b). Nup96 does not forma complex with NXF1-p15
(Fig. 2b), but it has a role in mRNA export (33, 36). Thus, reversal of NS1 function is specific for constituents of the NXF1-p15 complex. Then, to demonstrate that both the inhibition of gene expression by NS1 and its reversal by increased levels of mRNA export factors occurred at the mRNA export level, we performed immunofluorescence and oligo(dT) in situ hybridization
in cells expressing NS1 alone or in cells coexpressing NXF1 and p15. As shown in Fig. 3c, expression of NS1 caused nuclear retention of poly(A) RNA, and this effect was significantly
reverted in cells that coexpressed NS1 and NXF1-p15.
These results show that mRNA export factors can antagonize the inhibitory effects of the NS1 protein on mRNA nuclear export.
One possibility is that NS1 may bind to more than one constituent of the NXF1 mRNA export complex by using its different domains, and this would explain the reversal of the NS1 effect
by more than one constituent of this machinery.
Influenza Virus Virulence Correlates with Impaired mRNA Export
Function. To demonstrate the role of the mRNA nuclear export machinery in influenza virus-mediated cytotoxicity, we used cells from mice that express low levels of two key mRNA export
factors to determine their susceptibility to influenza infection. Cells from Rae1_/_ and/or Nup98_/_ mice express low levels of Rae1 or Nup98, respectively, and normal levels of other nuclear
export factors (31, 37). We found that Rae1_/_ or Nup98_/_ cells are more susceptible to influenza virus-mediated cell death than wild-type cells, whereas cells that are heterozygous for both
Rae1 and Nup98 show enhanced susceptibility to cell death induced by influenza infection (Fig. 4 a and b). Mutant cells also produced more viral particles compared with wild-type cells (Fig. 4c). These results demonstrate a key role for the mRNA export machinery components in antiviral response. Interestingly, we observed a higher number of viral particles in the supernatants of Rae1_/_ Nup98_/_ cells as compared with Nup98_/_ Rae1_/_ or Rae1_/_ Nup98_/_ cells, despite the fact that the later cell types showed more cell death than the Rae1_/_ cells (Fig. 4 a and b).

</file>

<file= AmE06_J80>

Invasions by nonindigenous species are a growing global problem, costing U.S. taxpayers hundreds of billions of dollars annually in environmental degradation,
lost agricultural productivity, expensive prevention and eradication efforts, and increased health problems (Vitousek et al. 1996, Mack et al. 2000, Sala et al. 2000,
Mooney et al. 2005). The only study to attempt a nationwide estimate of the economic costs to the United States of nonindigenous species concluded that annual
costs exceed $120 billion (Pimentel et al. 2005) or about $1100 per household annually. While Pimentel et al. (2005) did not account for the economic benefits that
some of the species provide, they also examined only a small subset of harmful species, and did not include many environmental damages caused by the species that were examined. Including these other factors would likely mean that the net costs of invasive species are much higher, and they are clearly growing.
Zebra mussels alone cost each infested large power plant $3 million annually (Leung et al. 2002), and are still spreading throughout the waterways of the United States
(Drake and Bossenbroek 2004). In two Californian lagoons, more than $5 million were spent in the first three years of an on-going eradication program for the seaweed Caulerpa taxifolia. At least $3 million annually are spent
in Florida to control the Australian melaleuca tree (Melaleuca quinquenervia; Pimentel et al. 2005). These and many other expenditures occur because the damages that result from inaction are more costly. Without management, the populations of these species grow and spread so that damages accelerate over time. In contrast to many other forms of pollution, such widespread invasions become irreversible because often the technology does not exist to selectively eradicate species. Relative to the economic and ecological costs of other forms of environmental pollution, the costs of nonindigenous species are therefore of particular concern because they are likely to be borne over very long time frames.
Many long-term changes in ecosystems and the goods and services that they provide to humans are driven by nonindigenous species, including, for example, degradation of U.S. western rangeland and increased fire damage caused by the widespread invasion by Bromus tectorum (cheatgrass; Grace et al. 2001). Some nonindigenous species were introduced intentionally and continue to be highly valued by humans, e.g., agriculture and aquaculture species. Many other species, e.g., West Nile virus, were introduced as by-products of human travel and international commerce, have no utility for humans, and have strong net negative impacts on the environment, industry, and human health.
We highlight in this report the policy and management recommendations that follow logically from recent scientific and technical advances in our understanding of biological invasions (Table 1). These recommendations are especially timely because U.S. state and federal agencies are developing new approaches to reduce the negative environmental, economic, and human-health impacts of nonindigenous species. The National Invasive Species Council (NISC), advised by the Invasive
Species Advisory Committee (ISAC), published the first edition of a National Management Plan (NMP) for invasive species in January 2001 (available online).Our
recommendations are consistent with the NMP, but we emphasize some priorities among its many recommendations.
The science of ecology and the expertise within the Ecological Society of America, in particular, can offer much guidance in the implementation of the NMP's goals at state, federal, and international levels. In this paper we focus on recommendations that require U.S. federal leadership to better coordinate international, federal, state, and local governmental responses.
Definitions 
A potentially confusing set of terms has developed around biological invasions. In this report, nonindigenous means a species that by human influence occurs
outside its native range. Synonyms include ''nonnative,'' ''alien,'' and ''exotic''; alien is the term used in the NMP and in many discussions involving U.S. federal agencies (see footnote 14). Species that spread widely beyond the location of initial establishment, become locally abundant, or spread into natural areas, are
referred to as invasive. Clearly, then, the definition of ''invasive'' depends on time and spatial scale, which must therefore be specified.
In many policy and legal documents in the United States and other countries, another component is added to the definition of invasive: the species causes or is
likely to cause net harm to the economy, environment, or human health. The definition of ''harm'' is a function of human values, which often differ in different regions, and may change temporally. Overall then, scientists
can - with specified temporal and spatial scales - define nonindigenous status and spread, and can describe the loss of native species and other ecological changes
caused by nonindigenous species. However, deciding whether such ecological changes or impacts on industry or human health constitute net harm requires additional input through a broader democratic process that
includes economists, public-health experts, and ecologists (National Research Council 1996, Hayes and Sliwa 2002, Lodge and Shrader-Frechette 2003, Andow 2004, Drake and Keller 2004).
While some species native to a given region are invasive (Van Auken 2000), these species are not the focus of current policy discussions, and not the topic of this report. Thus, we focus in this report on the subset of nonindigenous species that are invasive; that is, we focus on invasive nonindigenous species, which we will
hereafter abbreviate as invasive species. Additional discussions of terminology and related issues are available in Davis and Thompson (2000), Richardson et al. (2000), Lodge and Shrader-Frechette (2003), Colautti and MacIsaac (2004), Donlan and Martin (2004), and Pysek et al. (2004). Process of invasion
At one level, the issue of invasive species is well illustrated by thousands of different examples, replete with idiosyncratic biological details, from brown tree snakes on Guam to snakehead fish in Maryland to monkey pox in the Midwest. At another more basic level, such catalogs of examples obscure the biological processes that are common to all invasions, and that hold the key to scientific analysis and appropriate policy and management responses (Fig. 1). Species are carried
in a pathway, the purpose of which may be to transport species (e.g., the pet and horticultural trades) or in which the transport of species is incidental to the primary
human purpose (e.g., insect pests in lumber shipments, many different kinds of organisms in ballast water of ships, viruses carried by humans themselves). Depending on the conditions and the duration in the pathway, some proportion of the organisms will be alive when they are released or escape at a location outside the geographic area where they previously occurred. Many such nonindigenous species subsequently go extinct in a new location, but a proportion, about 50%
for animal species (Jeschke and Strayer 2005), establishes a self-sustaining population (Mack et al. 2000). At the next stage of invasion, many established species remain localized, and most are probably not even
detected by humans. Yet a proportion of established species, about 50% for animals (Jeschke and Strayer 2005), spread widely and become abundant at many new
locations, sometimes after a lag phase of many years in which populations remained small and localized (O'Dowd et al. 2003). Such species are then classified
as invasive, and because of their abundance, they cause detectable ecological changes, which are often viewed as harmful. Human health is sometimes affected, and economic costs are often incurred (Pimentel et al. 2005).
Policy and management implications become clear when these common processes and probabilistic transitions during invasion are recognized. The possible human management responses narrow as any invasion progresses (Fig. 1). Prevention is possible only early in the process, before a species arrives in a new range or at
the point of entry. Once a species is well established, eradication is costly and sometimes impossible. Eradication therefore depends on the rapid convergence of appropriate technology, political will, and resources.
In the United States, most eradication attempts occur when direct risks to human health loom. The arrival via international travel and trade of viral pathogens of
many organisms, including humans (e.g., West Nile virus, monkey pox, SARS, and HIV) (Breiman et al. 2003, CDC 2003, Chan-Yeung and Yu 2003, Check
2004, Lingappa et al. 2004) and parasite vectors (e.g.,
Asian tiger mosquito, Aedes albopictus, that can carry dengue and yellow fever; Reiter and Sprenger 1987, Moore 1999, Linthicum et al. 2003) have all prompted substantial management and policy responses in the
United States. Nevertheless, only monkey pox and SARS have been eradicated, while West Nile virus, Asian tiger mosquito, and HIV are now widespread. We are not addressing human diseases in this paper, but we do consider management and policy responses to diseases as an instructive example for responses to other invasive species. The activities of the U.S. Centers for Disease Control and Prevention (CDC) are especially relevant. The processes of emergence of human diseases are often the same as those for other invasive species, including wildlife diseases, which we evaluate here.
Indeed, many parasites, including West Nile virus and monkeypox, affect both humans and many other domestic animals and wildlife species. The management and policy responses to disease therefore offer a touchstone for evaluating current societal responses to other invasive species. For both human parasites and
other invasive species, once the opportunity for eradication has passed, few options remain: control of populations in selected locations, slowing the spread of
species, and adaptation by humans.
Even when the technology and political will for control efforts exists, resources must be made available in perpetuity, unlike many other types of pollution abatement. For example, expenditures in response to
West Nile virus in Louisiana alone for just nine months in 2002 - 2003 were $20 million (Zohrabian et al. 2004).
For non-health related species, the costs of control are typically lower, but still substantial. The United States and Canada have spent at least $16 million annually
since 1956 to reduce sea lamprey populations to a level at which losses to the Great Lakes fisheries are acceptable, and Florida spends $14 million annually
for control of nonindigenous aquatic plants (Schmitz et al. 1993). While these and other control programs are successful, similar efforts are too rarely attempted.
Instead, the default response in U.S. policy is adaptation -  passively adjusting to the damages caused by new species  -  even when, as is often the case, eradication or control would be a more cost-effective response.
Overall, only a fraction of introduced nonindigenous species establishes, and only a small proportion of those species pose a direct threat to human health or are otherwise invasive (Williamson 1996; Fig. 1). Nevertheless, the number of invasive species in the United States and elsewhere is large and continuing to grow because of increasing global movements of humans and goods.
For example, the numbers of nonindigenous plant pathogens, insects, and mollusks discovered in the United States since 1920 are strongly correlated with importation of goods over the same time period, and are forecast to increase by 16 - 24% over the next 20 years (Levine and D'Antonio 2003). As the world's largest economy and home to many of the world's richest ecosystems, the United States is particularly vulnerable to additional biological invasions. We therefore emphasize
the urgent need for more effective efforts of prevention, eradication of newly established nonindigenous species, and control of currently invasive species.
We assess general policy approaches in light of recent scientific advances (Table 1), and make six recommendations requiring policy and management action.
PREVENTION
Policy makers should focus on early steps in the invasion process because that is where the most costeffective responses are possible (Fig. 1): preventing organisms from entering a pathway, and preventing organisms that are transported from being released or escaping alive. Thus, prevention efforts must include a focus on pathways (Ruiz and Carlton 2004). Once a highly invasive species arrives, it is difficult to prevent rapid spread. For example, many introduced
plant species disperse freely by wind, water, or animals, and via roads and riparian zones, to many new ecosystems. One purple loosestrife plant (Lythrum salicaria) can produce thousands of seeds that are readily transported downstream by water to new locations along river networks, establishing new populations (Malecki et al. 1993, Galatowitsch et al. 1999), while terrestrial species invade roadways and highway edges (Randall and Marinelli 1996).

</file>

