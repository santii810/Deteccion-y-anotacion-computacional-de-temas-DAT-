<file= AmE06_D01>

The Martyr 

In a first-class stateroom on a cruise ship bound for New York from Alexandria, Egypt, a frail, middle-aged writer and educator named Sayyid Qutb experienced a crisis of faith. "Should I go to America as any normal student on a scholarship, who only eats and sleeps, or should I be special?" he wondered. "Should I hold on to my Islamic beliefs, facing the many sinful temptations, or should I indulge those temptations all around me?" It was November 1948. The new world loomed over the horizon, victorious, rich, and free. Behind him was Egypt, in rags and tears. The traveler had never been out of his native country. Nor had he willingly left now. 

The stern bachelor was slight and dark, with a high, sloping forehead and a paintbrush moustache somewhat narrower than the width of his nose. His eyes betrayed an imperious and easily slighted nature. He always evoked an air of formality, favoring dark three-piece suits despite the searing Egyptian sun. For a man who held his dignity so close, the prospect of returning to the classroom at the age of forty-two may have seemed demeaning. And yet, as a child from a mud-walled village in Upper Egypt, he had already surpassed the modest goal he had set for himself of becoming a respectable member of the civil service. His literary and social criticism had made him one of his country's most popular writers. It had also earned the fury of King Farouk, Egypt's dissolute monarch, who had signed an order for his arrest. Powerful and sympathetic friends hastily arranged his departure.

At the time, Qutb (his name is pronounced kuh-tub) held a comfortable post as a supervisor in the Ministry of Education. Politically, he was a fervent Egyptian nationalist and anti-communist, a stance that placed him in the mainstream of the vast bureaucratic middle class. The ideas that would give birth to what would be called Islamic fundamentalism were not yet completely formed in his mind; indeed, he would later say that he was not even a very religious man before he began this journey, although he had memorized the Quran by the age of ten, and his writing had recently taken a turn toward more conservative themes. Like many of his compatriots, he was radicalized by the British occupation and contemptuous of the jaded King Farouk's complicity. Egypt was racked by anti-British protests and seditious political factions bent on running the foreign troops out of the country - and perhaps the king as well. What made this unimposing, midlevel government clerk particularly dangerous was his blunt and potent commentary. He had never gotten to the front rank of the contemporary Arab literary scene, a fact that galled him throughout his career; and yet from the government's point of view, he was becoming an annoyingly important enemy. 

He was Western in so many ways - his dress, his love of classical music and Hollywood movies. He had read, in translation, the works of Darwin and Einstein, Byron and Shelley, and had immersed himself in French literature, especially Victor Hugo. Even before his journey, however, he worried about the advance of an all-engulfing Western civilization. Despite his erudition, he saw the West as a single cultural entity. The distinctions between capitalism and Marxism, Christianity and Judaism, fascism and democracy were insignificant by comparison with the single great divide in Qutb's mind: Islam and the East on the one side, and the Christian West on the other. 

America, however, stood apart from the colonialist adventures that had characterized Europe's relations with the Arab world. America, at the end of the Second World War, straddled the political chasm between the colonizers and the colonized. Indeed, it was tempting to imagine America as the anticolonial paragon: a subjugated nation that had broken free and triumphantly outstripped its former masters. America's power seemed to lie in its values, not in European notions of cultural superiority or privileged races and classes. And because America advertised itself as an immigrant nation, it had a permeable relationship with the rest of the world. Arabs, like most other peoples, had established their own colonies inside America, and the ropes of kinship drew them closer to the ideals that the country claimed to stand for. 

And so, Qutb, like many Arabs, felt shocked and betrayed by the support that the U.S. government had given to the Zionist cause after the war. Even as Qutb was sailing out of Alexandria's harbor, Egypt, along with five other Arab armies, was in the final stages of losing the war that established Israel as a Jewish state within the Arab world.  The Arabs were stunned, not only by the determination and skill of  the Israeli fighters but by the incompetence of their own troops and the disastrous decisions of their leaders. The shame of that experience would shape the Arab intellectual universe more profoundly than any other event in modern history. "I hate those Westerners and despise them!" Qutb wrote after President Harry Truman endorsed the transfer of a hundred thousand Jewish refugees into Palestine. "All of them, without any exception: the English, the French, the Dutch, and finally the Americans, who have been trusted by many."

The man in the stateroom had known romantic love, but mainly the pain of it. He had written a thinly disguised account of a failed relationship in a novel; after that, he turned his back on marriage. He said that he had been unable to find a suitable bride from the "dishonorable" women who allowed themselves to be seen in public, a stance that left him alone and unconsoled in middle age. He still enjoyed women - he was close to his three sisters - but sexuality threatened him, and he had withdrawn into a shell of disapproval, seeing sex as the main enemy of salvation. 

The dearest relationship he had ever enjoyed was that with his mother, Fatima, an illiterate but pious woman, who had sent her precocious son to Cairo to study. His father died in 1933, when Qutb was twenty-seven. For the next three years he taught in various provincial posts until he was transferred to Helwan, a prosperous suburb of Cairo, and he brought the rest of his family to live with him there. His intensely conservative mother never entirely settled in; she was always on guard against the creeping foreign influences that were far more apparent in Helwan than in the little village she came from. These influences must have been evident in her sophisticated son as well. 

As he prayed in his stateroom, Sayyid Qutb was still uncertain of his own identity. Should he be "normal" or "special"? Should he resist temptations or indulge them? Should he hang on tightly to his Islamic beliefs or cast them aside for the materialism and sinfulness of the West? Like all pilgrims, he was making two journeys: one outward, into the larger world, and another inward, into his own soul. "I have decided to be a true Muslim!" he resolved. But almost immediately he second-guessed himself. "Am I being truthful or was that just a whim?"

His deliberations were interrupted by a knock on the door. Standing outside his stateroom was a young girl, whom he described as thin and tall and "half-naked." She asked him in English, "Is it okay for me to be your guest tonight?" 

Qutb responded that his room was equipped with only one bed. 

"A single bed can hold two people," she said. 

Appalled, he closed the door in her face. "I heard her fall on the wooden floor outside and realized that she was drunk," he recalled. "I instantly thanked God for defeating my temptation and allowing me to stick to my morals." 

This is the man, then - decent, proud, tormented, self-righteous, and resentful - whose lonely genius would unsettle Islam, threaten regimes across the Muslim world, and beckon to a generation of rootless young Arabs who were looking for meaning and purpose in their lives and would find it in jihad. 

Qutb arrived in New York Harbor in the middle of the most prosperous holiday season the country had ever known. In the postwar boom, everybody was making money - Idaho potato farmers, Detroit automakers, Wall Street bankers - and all this wealth spurred confidence in the capitalist model, which had been so brutally tested during the recent Depression. Unemployment seemed practically un-American; officially, the rate of joblessness was under 4 percent, and practically speaking, anyone who wanted a job could get one. Half of the world's total wealth was now in American hands. 

The contrast with Cairo must have been especially bitter as Qutb wandered through the New York City streets, festively lit with holiday lights, the luxurious shop windows laden with appliances that he had only heard about - television sets, washing machines - technological miracles spilling out of every department store in stupefying abundance. Brand-new office towers and apartments were shouldering into the gaps in the Manhattan skyline between the Empire State Building and the Chrysler Building. Downtown and in the outer boroughs, vast projects were under way to house the immigrant masses. 

It was fitting, in such a buoyant and confident environment, unprecedented in its mix of cultures, that the visible symbol of a changed world order was arising: the new United Nations complex overlooking the East River. The United Nations was the most powerful expression of the determined internationalism that was the legacy of the war, and yet the city itself already embodied the dreams of universal harmony far more powerfully than did any single idea or institution. The world was pouring into New York because that was where the power was, and the money, and the transforming cultural energy. Nearly a million Russians were in the city, half a million Irish, and an equal number of Germans - not to mention the Puerto Ricans, the Dominicans, the Poles, and the largely uncounted and often illegal Chinese laborers who had also found refuge in the welcoming city. The black population of the city had grown by 50 percent in only eight years, to 700,000, and they were refugees as well, from the racism of the American South. Fully a fourth of the 8 million New Yorkers were Jewish, many of whom had fled the latest European catastrophe. Hebrew letters covered the signs for the shops and factories on the Lower East Side, and Yiddish was commonly heard on the streets. That would have been a challenge for the middle-aged Egyptian who hated the Jews but, until he left his country, had never met one. For many New Yorkers, perhaps for most of them, political and economic oppression was a part of their heritage, and the city had given them sanctuary, a place to earn a living, to raise a family, to begin again. Because of that, the great emotion that fueled the exuberant city was hopefulness, whereas Cairo was one of the capitals of despair. 

At the same time, New York was miserable - overfull, grouchy, competitive, frivolous, picketed with No Vacancy signs. Snoring alcoholics blocked the doorways. Pimps and pickpockets prowled the midtown squares in the ghoulish neon glow of burlesque houses. In the Bowery, flophouses offered cots for twenty cents a night. The gloomy side streets were crisscrossed with clotheslines. Gangs of snarling delinquents roamed the margins like wild dogs. For a man whose English was rudimentary, the city posed unfamiliar hazards, and Qutb's natural reticence made communication all the more difficult. He was desperately homesick. "Here in this strange place, this huge workshop they call 'the new world,' I feel as though my spirit, thoughts, and body live in loneliness," he wrote to a friend in Cairo. "What I need most here is someone to talk to," he wrote another friend, "to talk about topics other than dollars, movie stars, brands of cars - a real conversation on the issues of man, philosophy, and soul." Two days after Qutb arrived in America, he and an Egyptian acquaintance checked into a hotel. "The black elevator operator liked us because we were closer to his color,"

</file>

<file= AmE06_D02>

TThe philosophy of religion is generally understood to be
the rational study of religious questions, including
questions about the nature of god(s) and the place of
mankind within the universe. Is there a god (or gods)
and, if so, what can we know of the nature of the god(s)
and his/her/its/their interactions in the world of humanity?
Does our understanding of the nature of the god(s)
affect our understanding of ourselves and our relationship
with others? This study is also often called upon to
answer questions of human meaning: Why am I here?
What am I meant to do? Is there an ultimate meaning to
human life in general and my life in particular? These
philosophical and theological questions blend into
questions of morality and ethics: How does one determine
right from wrong action, and how does one decide
on appropriate actions in the face of ambiguity?
Ethics naturally leads to questions of theodicy: Why do
evil people prosper while the good are left to suffer? And what is the appropriate religious response in the face of the evil in our lives and
the lives of others? Underlying these questions of theodicy is a particular understanding
of the nature of the gods. We ask, specifically, if god (or the gods)
is good and just, why is there evil in the world?
These questions can extend beyond the nature of deity to ask not only why
evil exists in general but also why evil strikes when and where it does. Why
does the whirlwind take the one and leave the other? Such questions lead to
more personal questions that can be summarized by the cry, Why me! Why am
I made to suffer, and what can I do to alleviate my own propensity toward
suffering? These questions lead in turn to questions of destiny: Is there a reason
why I am here, at this place and in this time? Does my life have meaning
beyond the needs of day-to-day existence? If there is a meaning to my life, how
do I discover my destiny? And, if destiny can be manipulated, how might I
assure the most positive destiny for myself and those I love?
Although within the Western philosophical tradition these questions have
generally been asked and answered from the dominant male perspective, contemporary
women have entered the fray, asking and answering these questions
from their own perspectives. Thus one must ask one final set of questions:
How do our ideas of god (or the gods), human beings, and their
interactions change in the light of gender awareness? Does the normative male
perspective incorporate the thinking and experiences of both men and
women? If not, how does the experience of women change our understandings
of the great philosophical and theological questions?
The Male as Normative
Much philosophical thinking assumes what is known as the "normative male
perspective." Not only are the major thinkers across traditions men, but the
descriptions of both the gods and human beings imply that maleness is the
base case - that is, the gods and the people incorporated into philosophical
thinking are presumed to be male unless they are specifically identified otherwise.
In her 2003 article in Cross Currents, Rita M. Gross says that this preference
for maleness over femaleness is one of the most deeply rooted problems
in our cultural psyche and that it is "probably due in part to religious symbol
systems that contain deeply misogynist elements and personify the most valued
and ultimate symbols as masculine" (12). This has led, she says, to a belief
that what is male is "normal," that everything deviating from stereotypical
maleness is abnormal, and that men are unencumbered by gender, since gender is one of those characteristics that distinguish some human beings as outside
the (male) norm. This deep-seated preference for characteristics that have
been culturally constructed as male over those that have been constructed as
female as well as over other cultural gender constructions has led to a general
acceptance of women who want to act like men but a continued abhorrence of
men who take on any characteristic that has been culturally defined as "female."
My intention in this book is to explore these issues of gender through the
lens of the philosophy and practices of the Orisha traditions as they have developed
in the Americas. My analysis of the beliefs and practices of the devotees
of the Orisha suggests that, unlike the mainstream religions Gross alludes
to, they exist within a female-normative system in which all practitioners, regardless
of their own understandings of their sex or gender or sexual orientation,
are expected to take up female gender roles in the practice of the religion.
Although there are instances in which male roles are expected of both men and
women in the religion, the more common situation is the adoption of female
roles. How, then, might our perceptions of the philosophical and theological
questions raised above be changed in a system that valorizes the female over
the male, and how might the lives of the individuals caught up in a female normative
system change?
If what Gross says is true - and I believe that it is - this book will be extremely
difficult for both scholars and practitioners of these Orisha traditions,
because it challenges not only Western theological traditions but also misogynistic
attitudes within communities of practitioners. In one way or another
the questions we will be exploring have challenged human beings for millennia.
However, for the past two thousand years, they have been most systematically
analyzed within the confines of the Western Christian milieu. Even when
Europeans began to investigate the ideas of non-Western religious systems,
they generally worked from within a Western Christian foundation, so that,
having developed a highly sophisticated theology based on biblical and Western
philosophical traditions, they viewed the new religious traditions they
found in Asia and later in Africa and the Americas through the lens of their
understandings of Christian theology. Thus the literature of encounters between
Europeans and the Others they found around the world is full of attempts
by those Others and their conversation partners to describe their religious
ideas in Western philosophical and theological terms.
This book is not an attempt to continue that (generally discredited) project.
Rather, it is an effort to formulate the beginnings of a theology of the
contemporary Orisha traditions using the Western philosophical and theological categories while approaching them from a different perspective. Although
I am striving to answer the questions posed above, I will do so from
the point of view of one who has become immersed in the Orisha traditions.
Generally these traditions in the United States are based on the Afro-Cuban
religious complex known as Santería. As we will discuss, Santería is understood
to be a syncretization of the Orisha traditions brought to the Americas
by enslaved Yoruba-speaking people and the colonial Catholic traditions they
found when they arrived in the Americas. Although that is a misrepresentation
of the tradition, it is important to recognize that contemporary practitioners
of the Orisha traditions in the United States, Cuba, and even Nigeria have all
been influenced by the Western colonial project and are familiar with at least
the popular rendition of Western Christian theological and philosophical concepts.
At the same time, it is important for scholars and practitioners to gain an
understanding of this tradition and the ways in which its foundational ideas
are different from those found either in the Western monotheistic traditions of
Christianity, Judaism, and Islam or in the various polytheistic or atheistic
Asian traditions - Hinduism, Buddhism, Shinto, and the like. Perhaps an example
of one of those fundamental differences will be helpful at this point.
Christianity and Islam are generally considered salvational religious traditions.
This means that among the fundamental beliefs is that, when one dies,
one's life is judged worthy of punishment or reward. There are no second
chances in these traditions. Once one dies, judgment is made and one's fate is
determined. Within Hinduism and Buddhism, on the other hand, the idea of
samsara prevails. Within these traditions one is continually born and reborn,
each death leading to a new rebirth. The reward for a good life is rebirth at a
higher level. The ultimate reward is nirvana, a stepping off the wheel of rebirth
and an extinction of the "self." (Nirvana is a Sanskrit word meaning "extinction.")
Continuing to be born and reborn is always a penalty; the goal is to
elude rebirth altogether. Although they are different in many ways, all of these
traditions see human life ("this world") as miserable, and all look forward to a
future that is different from the everyday. Like Buddhism and Hinduism, Yoruba
traditional beliefs include an idea of reincarnation, a belief that those who
have lived before will live again. Significantly, among the Yoruba, reincarnation
is believed to generally follow the family line - that is, the dead are reincarnated
as their own descendants, often their grand- or great-grandchildren.
However, the more important way in which the Yoruba understanding of reincarnation
is different from the Asian traditions is the understanding that rebirth
is a reward for having lived a good life. There is no idea of rising through
levels of social hierarchy or of an ultimate reward of eluding rebirth altogether.
Introduction • 5
Only the most evil and warped human being is denied rebirth within the Yoruba
tradition. This leads to (or comes from) a valorization of this life - the
viewpoint that, in spite of the hardships of day-to-day living, life is good, this
is where we all want to be, and death is only a temporary respite between
lifetimes. This was summed up for me in the proverb "Life in heaven cannot
be pleasant, otherwise people would not live so long and come back so
quickly."
Each of these positions about the afterlife leads to different ways of acting
and thinking. Each provides a different basis on which to answer such great
theological questions as: How am I to live? How am I to react to the inevitable
ups and downs along the way? We will examine the ways contemporary
Orisha worshippers are encouraged to answer these questions and the ways
this idea and others like it inform their idea of the good life. At this point what
is important is a recognition that such differences exist and that they can radically
change one's view of the world.

Santería: A Brief Overview

My curse be on ye for your disloyalty and disobedience, so let your children
disobey you. If you send them on an errand, let them never return to bring
you word again. To all the points I shot my arrows will ye be carried as
slaves. My curse will carry you to the sea and beyond the seas, slaves will
rule over you, and you their masters will become slaves.
Prince Adebo
The Orisha traditions in the United States are known variously as Santería,
Lukumi (sometimes spelled Lucumí), Ifá, Regla de Ocha (Sp., Rule of the
Orisha),Yoruba Traditional Religion, or Orisha Religion.2 Although this
book is written from the viewpoint of traditional Santería (Lukumi) practice,
much of what is said here also applies to these variants of Yoruba-based religion
as well as to the Brazilian tradition of Candomblé. During the eighteenth
and early nineteenth centuries, thousands of Africans were brought from the
west coast of Africa to the Americas to work as slaves on the plantations created
by European colonists.3 After 1762 when the English captured the Spanish
port of Havana, and after the Haitian revolution of 1794, the European
passion for sugared foods was increasingly met by sugar from Cuban plantations.
Hundreds of thousands of Yoruba-speaking people were brought to the
New World to work these plantations. By 1888 nearly 10 million African men
and women had made the Middle Passage to the Americas. It is estimated that
between 500,000 and 700,000 of these Africans were brought to Cuba.

</file>

<file= AmE06_D03>

The closure principle has proved to be a useful hypothesis for the investigation
of a wide range of interactions among physical phenomena;but if
there are any nonphysical influences on physical events, unquestioning acceptance
of this belief will ensure that those influences will not be recognized.
Some scientific materialists have misleadingly argued that the closure
principle must be a universal truth because scientific research has found no
evidence of any nonphysical influences in the natural world. The distinguished
biologist Edward O. Wilson, for instance, declares that the religious
belief in a God who directs organic evolution and intervenes in human
affairs ''is increasingly contravened by biology and the brain sciences.''
Natural philosophy, as it was envisioned by Descartes and other participants
in the Scientific Revolution, had only the physical world as its proper
domain, and this has been largely true of science ever since. Moreover, never
in the history of modern science have instruments or methods been devised
to detect the presence of nonphysical influences of any kind. Research in
modern biology and the brain sciences is conducted with the assumption,
hardly ever questioned, that there are no nonphysical influences in organic
evolution or in human affairs. So the fact that scientists have not discovered
any such influences should hardly come as a surprise. And at this point in
history, it is certainly premature to declare that scientific knowledge of
organic evolution and brain activity is so complete that nonphysical influences
can be absolutely ruled out on purely empirical grounds.
Particularly with regard to the human mind, the closure principle seems
to be incompatible with experience, for our conscious mental processes,
which have not been demonstrated to be composed of configurations of
mass and energy, certainly do appear to influence human behavior. Advocates
of the closure principle assume that the apparent influence of our
desires, beliefs, and intentions on our behavior is actually an illusion - all
behavior is in fact determined solely by the interaction of the nervous system
with the rest of the body and the physical environment. However, contemporary
neuroscience does not even remotely possess sufficient understanding
of the brain to verify this assumption on the grounds of empirical evidence.
If for no other reason, the fact that modern science does not know how or
why consciousness first appeared in terms of the evolution of life on our
planet or in the development of a human embryo should make it abundantly
clear that the closure principle is a metaphysical belief and not a
scientific fact.

Physicalism. With the widespread adoption of reductionism and the closure
principle in the nineteenth century, due in part to the widespread acceptance
of the principle of the conservation of energy, scientific materialism abandoned
its Judeo-Christian origins. No longer could this metaphysical dogma
conform to the Judeo-Christian belief in a nonphysical, personal God who
intervenes in the course of nature and human history and who responds to
the prayers of individuals. By the nineteenth century, the only religion with
which scientific materialism remained compatible was Deism, a religion
contrived in part by the proponents of scientific materialism itself.
Albert Einstein was among the most eminent scientists educated in the
nineteenth century to declare that the concept of a personal God is utterly
incompatible with science and that it is the major source of conflict between
religion and science. This theological stance, however, did not prevent him
from believing in a universal Superior Mind that reveals itself in the world
of experience. This Deist view retains the Christian belief that God possesses
an absolute perspective on reality, but it denies that God influences natural
events in any way. In other words, God is conceived of as an ideal scientist,
a purely objective observer who sees reality as it is without any personal,
subjective intervention.
Twentieth-century scientific materialism abandoned belief in any form of
theism by adopting the principle of physicalism, which states that in reality
only physical objects and processes exist. In other words, only configurations
of space and of mass/energy and its functions, properties, and emergent
epiphenomena are real. A closely related principle maintains that everything
that exists is quantifiable, including the individual elements of physical reality,
as well as the laws that govern their interactions. At this point scientific
materialism becomes compatible only with some of the more primitive
nature religions. The ''God's-eye view'' of reality that was the earlier
ideal of scientific materialism has been replaced by the ideal of the ''view
from nowhere.'' That is, the ideal of pure objectivity has been retained,
but it has been divorced from the theological underpinnings that originally
gave it credibility, meaning, and value. Thus, the quasimystical quest of
earlier scientists to view God's creation from God's own perspective has
been replaced by the ideal of viewing a mindless, meaningless universe from
a nonhuman, purely objective perspective.

There are many scientists and philosophers, of course, who deny that
physicalism is simply a metaphysical principle. Philosopher Patricia Churchland,
for instance, claims that the rejection of consciousness (or any other
''spooky stuff '' such as a soul or spirit) existing apart from the brain ''is a
highly probable hypothesis, based on the evidence currently available from
physics, chemistry, neuroscience and evolutionary biology.'' She declares
that the assertion of physicalism is an empirical matter, not a question of
conceptual analysis, a priori insight, or religious faith. Philosopher Gu¨ ven
Gu¨ zeldere asserts in a similar vein that ''. . . contemporary science tells us
that the world is made up of nothing over and above 'physical' elements,
whatever their nature (waves, particles, etc.).''

Let us assume for the moment that these physicalists are right in asserting
that scientists have empirically demonstrated that only physical things and
events exist. This would imply that this assertion belongs together with a
wide range of other scientific facts - such as the convertibility of mass and
energy, the atomic weight of hydrogen, and the nature of photosynthesis - 
about which there is a very high degree of consensus among the scientific
community. Churchland acknowledges that not all philosophers agree with
her physicalist belief, but it must also be acknowledged that a very sizable
proportion of the scientific community doesn't either. Given that 40 percent
of American scientists today believe in a personal God to whom they can
pray and that this figure has not changed significantly over the past century,
it would seem that if the physicalist hypothesis has been proven empirically
during the twentieth century, virtually half of the scientific community in
the United States still refuses to acknowledge it. If this is the case, are they
prevented from seeing this empirical truth as a result of their commitment
to a theistic ideology? If so, this raises a profound qualm about the reliability
of the scientific community as a whole to distinguish empirical facts
from ideological commitments. One might just as well ask the same question
of those scientists who believe that the empirical evidence does confirm
the hypothesis of physicalism: Are they overinterpreting scientific evidence
to make it conform to their metaphysical beliefs? If we are to trust the
scientific community to give unbiased reports of its research, then physicalism
must be regarded as a matter of conceptual analysis, a priori insight,
or religious faith. For there is clearly no scientific consensus on this matter,
or even a historical convergence toward such a consensus among scientists.

The Marginalization of the Mind

While the nineteenth-century adoption of the closure principle denied
causal efficacy to anything that is nonphysical, the twentieth-century version
of physicalism denies that anything nonphysical even exists in reality. This
shift has major implications for the relation between the mind and the
physical universe. It is noteworthy that, while physical science was well
established by the late seventeenth century, a science of the mind was not
initiated until a full two centuries later. And even then, particularly in the
Anglo-American world, the focus of academic psychology swiftly shifted
away from the mind and toward behavior, and then to neuroscience. Only
in the latter half of the twentieth century did cognitive psychology, for
example, begin to reconsider the functions of the mind as it is experienced
firsthand. In the historical development of modern science, the study of the
mind occurs only as an afterthought, subsequent to the elaborate development
of physics, chemistry, and biology;so it is no coincidence that in the
world as conceived by science, the role of the mind in nature has been
marginalized. According to this view, the universe is conceived as a giant
computer, and the emergence of consciousness during the course of cosmic
evolution is attributed solely to the laws of physics, which over the immensity
of time give rise to a nearly infinite complexity that is purportedly
sufficient to give rise to consciousness. This ''explanation'' places an enormously
heavy explanatory burden on the term ''complexity,'' which in fact
explains nothing.

Since the Scientific Revolution, subjectively experienced mental events
have gradually lost their status as real entities. Advocates of scientific materialism
now variously regard them as mere epiphenomena, as propensities
for behavior, as being equivalent to brain activity, or as bearing no existence
whatsoever. As one indicator of this phenomenon, it is worth noting the
types of discoveries for which Nobel awards have been granted in the fields
of physiology and medicine. While it is well known that many mental
phenomena - including hope and fear, happiness and depression, trust and
suspicion, and belief and disbelief - have profound influences on the human
body and state of health, since Nobel awards were first granted in 1901 for
discoveries in physiology or medicine, not a single one has been given for
discoveries about the nature of the mind. One could rationalize this fact by
claiming that research into the nature of the mind and its possible influences
on the body is not included in the domain of ''hard science'' and is therefore
unworthy of such a distinguished award. But ''hard science'' in this context
means nothing more than science that rigidly conforms to the metaphysical
dictates of scientific materialism, even at the cost of ignoring significant
aspects of health and disease.

How did the mind, which exerts such a powerful influence in our daily
lives and which makes science possible, become so marginalized? In his
classic work The Principles of Psychology, the American psychologist and
philosopher William James (1842 - 1910) presents a thesis that sheds brilliant
light on this issue:

The subjects adhered to become real subjects, attributes adhered to real
attributes, the existence adhered to real existence;whilst the subjects disregarded
become imaginary subjects, the attributes disregarded erroneous
attributes, and the existence disregarded an existence in no man's land, in
the limbo ''where footless fancies dwell.'' . . . Habitually and practically we
do not count these disregarded things as existents at all . . . they are not
even treated as appearances;they are treated as if they were mere waste,
equivalent to nothing at all.

James sums up this idea with the assertion that ''our belief and attention
are the same fact. For the moment, what we attend to is reality ... '' A historical
illustration of this theme is to be found in the history of behaviorism.
In 1913, the American behaviorist John B. Watson wrote that ''the time
has come when psychology must discard all reference to consciousness,''
and he attributed belief in the existence of consciousness to ancient superstitions
and magic. Fifteen years later, he expanded this principle by declaring
that behaviorists must exclude from their scientific vocabulary ''all
subjective terms such as sensation, perception, image, desire, purpose, and
even thinking and emotion as they are subjectively defined.''Behaviorism
duly followed this dictum, with the result that in 1953, B. F. Skinner concluded
that mind and ideas are nonexistent entities ''invented for the sole
purpose of providing spurious explanations. . . . Since mental or psychic
events are asserted to lack the dimensions of physical science, we have an
additional reason for rejecting them.''2Assertions concerning subjective experience
were similarly denied by certain philosophers of the same period
who argued against the very existence of subjective statements.

</file>

<file= AmE06_D04>

According to
the author of the Testament of Moses, Israel is in a vulnerable position just as it was
in the wilderness: the priests have transgressed Moses's commandments to such
an extent that they are no longer qualified to make atonement for the sins of the
people (T. Mos. 5:4). Something drastic must be done to remedy the nation's hopeless
situation in order to bring about the vengeance promised to Joshua (Deut
32:43; T. Mos. 9:7; 10:1). The only means available to accomplish this is through
a strict obedience to God's commandments as taught by Moses (T. Mos. 1:10, 14;
9:4 - 5; 12:10). Such observance of the Torah will enable God's "holy and sacred
spirit" (T. Mos. 11:16) to dwell with the people and save them from their enemies
as it did with Moses's successor, Joshua.

In ch. 9 of the Testament of Moses the author introduces an innocent Levite
named Taxo, who, like Moses, will seek to make atonement for his people's sins.54
Taxo encourages his sons to remain faithful by telling them that their strength
lies in their purity. Unlike the Levitical priests, who have forsaken their roles as intercessors for the people, or the Herodian kings, who the author states are not
from a priestly family, Taxo, lacking sin, is the perfect mediator between God and
the nation. Taxo resolves to fast for a three-day period with his sons and then to
go into a cave exhorting them, "Let us die rather than transgress the commandments
of the Lord of Lords, the God of our fathers. For if we do this, and die, our
blood will be avenged before the Lord" (T. Mos. 9:7). Although the text does not
describe Taxo's fate, the author's mention of "blood" suggests that Taxo expected
that he and his sons would die a violent death.

In contrast to the other members of the priesthood, who have forfeited their
right to be called priests (T. Mos. 5:4), Taxo and his sons constitute the faithful
and sinless remnant (T. Mos. 9:4). Jan Willem van Henten suggests that Taxo's
assertion that his family has never sinned or tempted God is an allusion to Moses's
blessing of Levi in Deut 33:8 - 11. This biblical passage paraphrases the episode
near the waters of Massah and Meriba during Israel's forty-year period in the
wilderness (Exod 17; Num 20). In this biblical blessing, Moses and Levi were
praised because they were tested yet remained faithful to the Lord. Taxo's claim to
hereditary innocence, in light of the writer's assertion that the priests have abused
the temple cult, suggests that only he and his sons have remained faithful to the
covenant. Because the Testament of Moses in ch. 4 apparently denies the cultic
validity of the Second Temple to remove the effects of sin, the author's community
presumably believed that they could not make atonement for their transgressions
within the sanctuary.57 Since Taxo and his sons believe that they are the only
priests who have observed the Law, they alone are qualified to act as intercessors
for the people by seeking atonement apart from the temple cult.

IV. The Role and Function of the Nuntius
Taxo's cry for vengeance is immediately followed by the appearance of the
eschaton and the arrival of God's messenger, the nuntius, who will seek vengeance for the deaths of Taxo and his sons (T. Mos. 10:3). Taxo's relationship to the
mysterious nuntius is the most debated topic in the Testament of Moses. It is not
necessary to assume that the word nuntius must refer to a human messenger simply
because the translator does not use the word angelus. The word nuntius refers
to angels in several fifth- and sixth-century c.e. Latin texts that are contemporary
with the date of the sole surviving Latin manuscript of the Testament of Moses.
The nuntius, like Taxo, is also a priest, since the author writes, "Then the hands of
the nuntius, who will be in heaven, will be filled, thereupon he will avenge them of
their enemies" (Tunc implebuntur manus nuntii qui est in summo constitutus, qui
protinus vindicavit illos ab inimicis eorum [10:2]). The idiom "to fill the hand" in
the Hebrew Bible is a technical term that refers to the consecration of the priests.
Because the nuntius is described as performing a priestly act in heaven, he must
be both an angel and a priest.

The identification of the nuntius as a priestly angel should not be considered
problematic, since many Jewish texts associate divine vengeance with a heavenly
figure who is sometimes portrayed as a priest. During the wilderness experience,
as recounted in Exodus 14 and 23, angels of the Lord fought with the armies of
Israel. In 1QM 10, the chief priest in the context of battle invokes Moses while
reciting a lengthy prayer concerning the halakic purity of the camps. The purpose
of this petition is to affirm that God will go forth with them to fight against
their enemies (1QM 10:8 - 12:9). The angels fight alongside humans in 1QM and
help to inflict great carnage on the unrighteous (1QM 1:10 - 11). Likewise, in
11QMelchizedek a heavenly priest seeks vengeance against Belial. Paul J. Kobelski
notes that in this text Michael plays a special role, similar to his function in Dan
12:1 - 3 and 1QM 17:5 - 8, in the final age by defeating Belial and that he functions
as a heavenly high priest. The nuntius in the Testament of Moses is similar
to these violent angelic figures, all of whom play active roles in eschatological
warfare.

In T. Mos. 10:2 the author describes the violent role of the nuntius and writes,
"then the hands of the messenger, when he will be in heaven, will be filled, and he
will then avenge them against their enemies." Tromp suggests that Taxo and his
sons must be the antecedent of "them," since it is their blood that will be vindicated
according to 9:7, to propose that it is the nuntius who is consecrated as a priest in
heaven. Although Moses is called "the great messenger" (magnus nuntius) in
11:17 and had an intercessory role in the past (T. Mos. 12:6), this does not mean
that he should be equated with the nuntius of 10:2. Although the Hebrew Bible
frequently describes Moses's intercession, it does not state that he will continue
his intercessory role after his death. Rather, Moses's intercession was limited to the
exodus and the wilderness period. During this time, Moses frequently interceded
for the nation to avert God's wrath, such as the time when the people had sinned
by making the golden calf. Moses's intercessory prayers provided food and water
for the people and saved them from their enemies.66 Without Moses's powers of
intercession, Joshua feared that the nation would be overcome by its adversaries
as punishment for its transgressions. For this reason, Moses assured the people
that leadership would continue under Joshua, who would take Moses's place on
earth as intercessor.

In the Testament of Moses, Moses tells Joshua that he had served as intercessor
not because of his own virtue or strength but because of God's mercy (12:7 - 8).
God's "holy and sacred spirit" (11:16), therefore, was not limited to Moses, and
Joshua was assured that he too would defeat Israel's enemies despite the nation's
sin (12:6 - 8). Tromp comments, "Whether Moses or Joshua prays for the people,
it is the Lord who has mercy on them (see again Exod 14:13 and Deut 8:16);
therefore, intercessory prayer will always be possible, with or without Moses (see
esp. As. Mos. 4:1)."  Because the Testament of Moses emphasizes that both Joshua
and Taxo are successors to Moses's intercessory role, intercession is possible apart
from Moses's physical presence. The author of the Testament, by expanding on
the wilderness event, portrays Taxo as an intercessory figure like Moses. Both
Taxo and Moses (Exod 2:1) were of priestly descent and, therefore, qualified to act
as mediators for the people and call for God's vengeance. Taxo, however, differs 
from Moses because he is the nation's final intercessor, whose cries for vengeance
are to be fulfilled through the final destruction of the wicked.

In ch. 10 the author describes the arrival of God's kingdom and the final
defeat of the "devil" (zabulus [10:1]). It is at this moment that the nuntius will
arrive and avenge the death of Taxo and his sons. In contrast to the devil, who
traditionally occupies a role in the heavenly court as a prosecutor, the nuntius
is apparently an advocate for Taxo and his sons.  Because Taxo expects that his
martyrdom will bring God's vengeance upon his enemies, he cites the Song of
Moses (Deut 32:43) to encourage his sons to become martyrs. In this biblical passage,
Moses asks God to "avenge the blood of his children, and take vengeance
upon his adversaries." Taxo also alludes to the Deuteronomic blessing of Levi, in
which God is asked to accept the righteous work of Levi's hands and to crush the
loins of his adversaries and those who hate him (Deut 33:11). In the Testament of
Moses, the nuntius intercedes on Taxo's behalf and presents his prayers for vengeance
(9:7) as an offering before God. God responds to the shedding of Taxo's
innocent blood by sending the nuntius to kill the wicked and avenge Taxo's death.
Because Taxo seeks the death and destruction of the unrighteous, he should not
be viewed as a quietist figure who suffers passive martyrdom. Rather, Taxo is a
militant individual who seeks his own death as a means to exterminate the wicked
for all time.

V. Conclusion

The Testament of Moses, in light of the historical background and texts cited
in this study, shows that some Jews during the first century b.c.e. and the early
first century c.e. believed that God required the shedding of innocent blood by
an intermediary figure to save humanity. In many of these texts, the wilderness
theme is used to emphasize Israel's vulnerability and to stress the importance of
strict adherence to Moses's words. For the author of the Testament of Moses, Israel
now stands somewhere in the wilderness between the period of Moses's past
redemption of the nation from Egypt and the fulfillment of God's eschatological
promises. Like the wilderness generation of Moses's day, the nation is now once
again in danger of committing apostasy and forfeiting their place in God's forthcoming
kingdom.

The author of the Testament of Moses introduces the figure of Taxo, who, like
Moses, is a priestly intercessor. Moreover, Taxo is Moses's ultimate successor, since
the writer believes that he is the final intercessor for Israel. While the Testament of
Moses does not affirm Taxo's divinity, it provides valuable insight into the historical
development of the belief that the martyrdom of a righteous person was necessary
to summon God's violent messenger and bring about the final destruction of
the wicked. The messiah of Psalm of Solomon 17, like Taxo, was also expected to
be "pure from sin" (17:36). Testament of Judah 21:1, which is likely a Christian
passage that may build upon prior Jewish tradition, says of the messiah, "no sin
whatever will be found in him." These parallels suggest that many Jews during
the Second Temple period believed that a righteous figure must be completely
pure and without sin in order to fulfill God's eschatological plan. This theological
doctrine found in the Testament of Moses, along with the belief in a sinless mediator,
the necessity of martyrdom, and the divine vengeance brought about by the
nuntius, would later be used by the nascent Christian community to explain the
necessity of Jesus's death. Both Taxo and Jesus are sinless, and their deaths serve as
a trigger for the eschaton; both are intercessors who offer themselves in sacrificial
deaths, and both associate the temple's destruction with innocent suffering and
divine vengeance. The Testament of Moses, once dated to the Herodian period,
provides a valuable, yet largely neglected, source for understanding these NT doctrines
as well as the roles of other intermediary figures during the Second Temple
period.

</file>

<file= AmE06_D05>

Marriage, he insisted, depended in its essence on the mutual consent of both the man and the woman.
Absent proof of consent by a fit man and a fit woman who had the
freedom and capacity to marry, there could be no valid marriage. Calvin defended this principle repeatedly in his commentaries and sermons.
"While all contracts ought to be voluntary, freedom ought to
prevail especially in marriage, so that no one may pledge his faith
against his will." "God considers that compulsory and forced marriages
never come to a good end. . . . If the husband and the wife are not
in mutual agreement and do not love each other, this is a profanation
of marriage, and not a marriage at all, properly speaking. For the will
is the principal bond." When a woman wishes to marry, she must thus
not "be thrust into it reluctantly or compelled to marry against her
will, but left to her own free choice." "When a man is going to marry
and he takes a wife, let him take her of his own free will, knowing that
where there is not a true and pure love, there is nothing but disorder,
and one can expect no grace from God."

Also like the medieval canonists, Calvin distinguished between contracts
of engagement and contracts of marriage - or betrothals and espousals
as he called them, following the tradition. Engagements were
future promises to be married. Marriages were present promises to be
married. But, unlike the medieval canonists, Calvin removed the need
for the parties to use specific formulaic words: any clear indication of
a future or present intent to marry would do. He softened the distinction
and shortened the duration between engagements and marriages.
He also insisted that the engagement contract be announced through
public banns in the church and community and registered with the
civil authorities and that the marriage contract be celebrated in a mandatory
church wedding.

The 1546 Marriage Ordinance took pains to ensure the free and full
consent of both parties to the engagement and marriage contracts. It
required that both sets of promises be made "simply" and "honorably
in the fear of God."8 Engagements were to be initiated by "a sober
proposal" from the man, accepted by the woman, and witnessed by at
least two persons of "good reputation."9 Engagements made in secret,
qualified with onerous conditions, or procured by coercion were automatically
annulled - and the couple themselves, and any accomplices
in their wrongdoing, could face punishment. Engagements procured
through trickery or "surprise," or made "frivolously, as when merely
touching glasses when drinking together," could be annulled on petition by either party. Engagement promises extracted by or from children
below the age of consent were presumptively invalid, although
children could confirm them upon reaching majority. Engagements
involving a newcomer to the city were not valid until the parties produced
proof of the newcomer's integrity of character and eligibility for
marriage. Absent such proof, the couple had to wait a year before they
could marry.

Normally, a Genevan couple, once properly betrothed, had little
time to waste. Neither their publicly announced engagement nor the
civil registration of their marriage was sufficient to constitute a marriage.
A formal church wedding had to follow - within six weeks of
engagement. If the couple procrastinated in their wedding plans, they
would be reprimanded by the Consistory; if they persisted, they would
be "sent before the Council so that they may be compelled to celebrate
it."

Not only the mutual consent of the parties but also the consent of
their parents was critical to the validity of an engagement and marriage
contract. Calvin devoted no less than eight of the first ten articles of
his 1546 Marriage Ordinance to the doctrine of parental consent. In
the 1545 draft, Calvin had seemed so eager to maximize the rights of
parental consent that he set the age of majority unusually high: boys
had to be twenty-four, girls, twenty, before they could marry without
seeking their parents' consent. The 1546 Marriage Ordinance lowered
these ages of majority to twenty and eighteen for boys and girls,
respectively. This was closer to the Protestant norm but still a bit high,
and Calvin still advised that even fully emancipated children "always
be governed by the advice of their fathers."
The consent of the father was the most critical. The consent of the
mother controlled only when the father was absent and no other relatives
were present. If other relatives were at hand, the mother's views
had to concur with theirs. In his 1545 draft, Calvin had said that, in
the absence of the father, the mother needed to have the concurrence
only of the "closest and most important" relatives to consent to a marriage.
He dropped this qualification in the 1546 version. Now it read
that the mother's consent would count only if and until she had "con sulted one of the relatives if there are any" - without regard for their
"closeness" or "importance" to the family. In the absence of both parents,
guardians would give their consent to a child's engagement and
marriage, again with priority for the male voice.
The 1546 Marriage Ordinance made clear that this parental consent
was only a supplement to, not a substitute for, the consent of the couple
themselves. Parents were prohibited from coercing their children
into unwanted engagements or withholding their consent or payment
of dowry until the child chose a partner whom they favored. Parents
were further prevented from forcing youngsters into marriage before
they were mature enough to consent to and participate safely in the
institution. Children "observing a modest and reverend spirit" could
refuse to follow their parents' insistence on an unwanted fiance´(e) or
a premature engagement.  Other children, confronting a "negligent
or excessively strict" father, could "have him compelled to give a
dowry" in support of a marriage they contracted in spite of him. 
The main goal of these provisions was to stamp out the medieval
Catholic Church's toleration of what the 1546 Marriage Ordinance
called private or "clandestine promises" (promesse clandestine) - that is,
engagements and marriages contracted without parental consent.
The Ordinance made clear that secret engagement promises were
"void" (nulle). This did not necessarily prevent a secretly engaged couple
from going forward with their plans if they received their parents'
consent after the fact. But, absent this parental consent, if anyone challenged
this engagement because it had been secretly contracted, the
engagement would be annulled regardless of what the couple wished.
The Ordinance was not so clear about the legal status of secret marriage
promises - especially if they had been celebrated, consummated,
and yielded children. The crucial statutory language was in item 3 of
the 1546 Marriage Ordinance. There, Calvin provided: "If it happens
that two young people have contracted marriage by their own action,
through folly or recklessness, let them be punished and chastened and
such a marriage be dissolved at the request of those who have charge
of them." It was clear from this language that children who entered
such secret marriages would be punished. What was not clear was
whether their marriage would be annulled if challenged by their parents.
A plain reading of item 3 suggests that parents could seek annulment
of their children's secretly "contracted marriage." On this reading,
while secret engagements were automatically "void," secret marriages
were voidable. They would be voided only if and when the children's
parents or others "who have charge of them" brought an action of
annulment.

But this reading does not pick up the studied ambiguity in the language
of item 3. First, the opening phrase "two young people" might
well have meant only youngsters who were not only below the age of
majority but also below the age of consent. Read as such, item 3 was
only a statement of the familiar medieval impediment of infancy: that
infants and youth may not enter marriage contracts, and when they
do, their parents or guardians need to have those promises dissolved
or at least postponed until the children reached an age where they
could consent or dissent to them. This was, in fact, how that phrase
came to be read by the Genevan Consistory.

Second, the phrase "contracted marriage" (contracte mariage) could
mean either (1) "contracted to get married in the future" or (2) "had
already entered a marriage contract." If the phrase meant the former,
then item 3 would mean simply that parents had standing to bring an
action to annul their child's secret engagement promise. These standing
rights of the parents were not specified elsewhere in the statute. It
made good sense to stipulate them, particularly since in 1546 when
Calvin was drafting the Ordinance, the Consistory had no other rules
of civil procedure to guide them. Even if the phrase "contracted marriage"
meant the latter (that the children "had already entered a marriage
contract") the matter was still not resolved. For final validity of the marriage turned on whether the couple had celebrated their marriage

in a proper wedding liturgy in the church, not whether they had
entered a marriage contract. In Calvin's Geneva, marriage contracts
without church weddings were not valid - and those contracted secretly
or without ceremonies outside of Geneva would have to be recontracted
and celebrated in Geneva in order to be valid.

Third, the two subsequent phrases of item 3 were also ambiguous.
The phrase "by their own action" (de leur propre mouvement) was separated
by a comma from the next phrase, "through folly or recklessness"
(par follie ou legierte´ ). These two phrases could be read separately. This
reading would allow parents to seek annulment of the pending marriage
either (1) if it was contracted secretly by the couple ("by their
own action") or (2) if the marriage, even if done with the parent's
consent, proved to be "foolish" or "reckless." Alternatively, the two
phrases could be read interdependently - with the second phrase understood
as a qualification of the first. This reading would allow parents
to attack a secret marriage only if it could be shown that the
marriage itself was not only secretly contracted ("by their own action")
but also foolish or recklessly entered. This was not so easy a standard
to meet. Children who married secretly sometimes did so not with recklessness
but with elaborate plans to circumvent their parents. And
many times their marriages, while not necessarily well advised, were
hardly "foolish," especially if they were motivated by a desire to get
away from overbearing, abusive, or bickering parents.

All this close exegesis might seem like silly legal hairsplitting. But
Calvin, the jurist, may well have intended the language of the Ordinance
to be a bit open textured. For, the legal status of parental consent
and secret marriages was a divisive question at the time he was
drafting the Ordinance. The first generation of Protestant reformers
had required parental consent in an effort to counter the late medieval
Catholic practice of tolerating secret engagements and marriages. All
the leading Protestant reformers allowed parents to annul their children's
secret engagements. The question that began to divide Protestants
sharply after 1540 was whether parents could annul their children's
secret marriages, too. Some reformers allowed parents to annul their secret marriages under any circumstances. Some allowed the same, unless the wife was pregnant or already had children. Some favored
continuation of the secret marriage. Some insisted on it. The
new Protestant laws of the day reflected these disparate views. This
issue became even more divisive in the 1540s and 1550s, when Catholics
began accusing Protestants of frivolously dissolving marriages and
foolishly catering to the tyranny of parents through their wooden application
of the doctrine of parental consent.
Calvin's 1546 Marriage Ordinance did not clearly answer the question
of whether parents could seek annulment of their children's secret
marriages. In 1549, Calvin seemed inclined to allow such annulments.
The occasion was Calvin's commentary on the Adultero-German Interim
(1548), a new imperial law designed to establish the emerging
Catholic teachings of the Council of Trent.

</file>

<file= AmE06_D06>

Influential Dozen to help Democrats Bridge 'God Gap'
At a meeting of the House Democrats' Faith Working Group, a perplexed member of Congress turned to his colleagues for pastoral guidance. How could he counter a local preacher who argued that all Jesus' moral teachings were about the world to come, not the here and now?
Representative David Price (D., N.C.) stood amid the sympathetic sighs and "you can't convert everyone" comments to offer a new spin on an old parable:
Walking down a road, the Good Samaritan encounters and cares for a stranger who has been beaten and robbed, Price said. The next day, on the same road, another person is beaten and robbed. So it goes for another week--more robberies, more victims.
"How long is it going to take before the Samaritan says, 'Hey, maybe we ought to patrol this road,'" Price said. In other words, he argued, there are some problems that individuals can't solve on their own. They require the resources of a morally responsible government.
As Democrats seek to reframe America's debate over moral values and close their lamented "God gap" with religious communities, conversations such as these are taking place throughout party circles.
Gone are the days when "faith outreach" meant visiting African-American churches two weeks before an election, party leaders say. Instead, Democrats are seeking--and getting--regular meetings with megachurch pastors such as T. D. Jakes, Joel Osteen and Rick Warren.
Rather than cede red states to Republicans, the party is buying airtime on Christian radio stations, with the message that Democrats are indeed a party with deep moral convictions.
No longer leaning on 1960s-era preachers to guide progressive politics, Democrats are also turning to young voices like strategist Mara Vanderslice, 31, and writer Amy Sullivan, 33, who offer new perspectives and fresh ideas.
After interviews with dozens of politicians, strategists, the think-tank set and a Noah's ark--full of faith leaders, Religion News Service has identified the 12 most influential voices in helping Democrats reach people of faith. Those on the list are writing the party's new testament, whispering in the ears of the powerful, and playing matchmaker between religious and political pacesetters.
They are:
--The Theologian: Shaun Casey, an ethicist at Washington's Wesley Theological Seminary, who provides a moral and historical framework for progressive faith-based politics.
--The House Trinity: Representatives James Clyburn, Rosa DeLauro and David Price, who lead the effort on Capitol Hill to frame legislative debates in moral terms.
--The Preacher: Leah Daughtry, chief of staff at the Democratic National Committee, who champions faith outreach at party headquarters.
--The Model: Tim Kaine, governor of Virginia, who showed Democrats how a faith-infused campaign can turn a red state blue.
--The Insider: Mike McCurry, former White House press secretary, who coaxes Washington Democrats into sharing their spiritual sides.
--The "Blessed" One: Barack Obama, the junior senator from Illinois, who challenges his party to make room for religion in the public square.
--The Prophet: Rabbi David Saperstein, head of the Religious Action Center of Reform Judaism, who is Washington's most articulate voice in progressive politics.
--The Matchmaker: Burns Strider, a staffer who shepherds the House Democratic Faith Working Group and corrals Congress into meetings with the religious community.
--The Agitator: Amy Sullivan, an independent but opinionated journalist who pens passionate pleas for Democrats to "get religion."
--The Strategist: Mara Vanderslice of the consulting firm Common Good Strategies, who works the campaign trail, helping candidates to build relationships with diverse religious communities.
In some ways, these 12 "apostles" reflect the diversity on which Democrats pride themselves. They include a former Churches of Christ minister, a Pentecostal preacher, two Catholics, a Methodist Sunday school teacher, an Episcopalian, a progressive evangelical and a rabbi.
Some at Democratic Party headquarters are taking an under-the-radar approach to faith outreach and are reluctant to divulge all the party's plans and advisers.
"Our focus is not in putting someone behind a pulpit," said Leslie Brown, the DNC's "Faith in Action" coordinator. But after conducting polls, meeting with state party chairs and undertaking an "internal education," Democrats are building a "message-driven machine," Brown said.
"We want to talk about things in ways we can relate to the faith community," said Clyburn (D., S.C.), who heads the Faith Working Group. "I don't talk about the environment just as keeping things green, I talk about it in terms of stewardship."
But while recent polls suggest that some evangelicals may be growing disillusioned with Republicans, many are still reluctant to pull the lever for Democratic candidates, said Richard Cizik, vice president of the National Association of Evangelicals.
"Simply using 'faith language' won't redound to the benefit of any candidate, Republican or Democrat, without some authenticity there," Cizik said. "When evangelicals think about the reputation of the Republican Party, which isn't too good right now, at least it does have a record of reaching out to those voters, and it does have a record on Capitol Hill of at least trying to carry water for their issues."
Still, there are issues such as the environment, poverty, war and health care on which Democratic positions dovetail with those of many religious Americans, party leaders say. "It's not always pretty," Casey says of Democrats' fledgling efforts to reach these voters. "But the good news is everybody realizes the party has to do a better job."
Protestant left has 'potential' impact, says scholar.
The Protestant left "has the potential to become a key constituency of the Democratic Party," says a political scientist studying the growing number of faith-related progressive groups in the U.S.
Though socially liberal Protestants have been outpaced by conservative religious movements since the 1980s, Laura R. Olson of Clemson University told an annual conference of sociologists of religion before this month's mid-term elections that "times always change."
Despite their difficulties of coordination and mobilization, which might force "religio-political progressives to remain as they are now--working behind the scenes," Olson said history shows that new opportunities arise with cultural and political shifts.
"I submit that the religious left has more potential energy now than it has since the Vietnam War era," she said October 21 in Portland, Oregon, in a presentation to the Society for the Scientific Study of Religion, which met with the Religious Research Association.
According to Olson, the 2004 re-election of President George W. Bush gave rise to "elite-level progressive Protestant mobilization as measured by the sheer number of new religious left organizations that have been founded and the volume of ink that has been spilled in new books challenging the religious right."
Olson, who is researching a book on the subject, listed nearly 60 organizations--old and new, national and regional--that she said make up the Protestant and ecumenical left.
Among the movement's weaknesses, she said, are its broad political agenda "so that no one priority can ever get the focus it needs," and the knowledge that most mainline congregations are too diverse politically to provide unified support.
"Organizing progressive Protestants online, however, seems to be a particularly clever strategy," especially since many potential recruits are well educated and technologically savvy, Olson said.
Court says gay couples have equal rights.
The New Jersey Supreme Court has ruled that the state's constitution requires that marriage rights be available to same-sex couples on an equal basis with heterosexuals. But the court left it to state legislators to decide whether to refer to the unions by the term marriage or a different one.
The October 25 decision leaves New Jersey in a situation akin to Vermont's, where legislators in 2000 created the nation's first law legalizing civil unions for same-sex couples. Vermont's civil unions, which followed a similar court decision, offer the same rights and benefits as marriage without being called that.
All seven New Jersey justices agreed that the state's constitution requires it to extend to gay couples the same rights as married couples. However, only three justices said those rights include equal use of the term marriage, while the four-justice majority said use of the term is not guaranteed.
"The state has not articulated any legitimate public need for depriving same-sex couples of the host of benefits and privileges" that married couples enjoy, wrote Justice Barry Albin, who authored the majority's opinion. "There is no rational basis for, on the one hand, giving gays and lesbians full civil rights in their status as individuals and, on the other, giving them an incomplete set of rights when they follow the inclination of their sexual orientation and enter into committed same-sex relationships."
Vermont and Connecticut have civil-union laws that provide identical benefits to same-sex couples as married heterosexual couples. Massachusetts, following a decision by its highest court, legalized same-sex marriage in 2004.
Peace Prize winner among pioneers in microfinance loans.
This year's selection of the Nobel Peace Prize winner has thrown the spotlight on Christian-supported microfinance and banking institutions that make small loans to help poor people build a future.
The Norwegian Peace Prize committee announced that this year's award will go jointly to Bangladesh economist Muhammad Yunus and his Grameen Bank as one of the pioneer groups helping to overcome poverty, especially in developing nations.
Opportunity International, a Christian ministry based in Oak Brook, Illinois, congratulated the winners for their role in helping to make microfinance "a proven, long-term solution to poverty." The U.S. group was founded in 1971, five years before Grameen Bank was started by Yunus.
Christopher A. Crane, president and CEO of Opportunity International, said his organization recently announced a $1 billion, seven-year plan to help 100 million people work their way out of poverty by 2015. "The appeal of microfinance is that it gives a hand up, not a handout," Crane said in a statement.
Peace Prize winner Yunus, 66, has been nicknamed a "banker to the poor" for setting up his Grameen Bank to extend small loans, known as microcredit, to the very poorest in Bangladesh, particularly women, enabling them to start up small businesses.
Since then, the bank has been a source of ideas and models for the many institutions in the field of microcredit that have sprung up around the world, the Norwegian Nobel Committee noted in announcing the award October 13. "Lasting peace cannot be achieved unless large population groups find ways in which to break out of poverty. Microcredit is one such means," it stated.
Oikocredit, another church-backed institution, also sent congratulations to Yunus. An international financier based in the Netherlands, Oikocredit said it has approved more than 100 million euro worth of credits in the first 10 months of 2006.
"Muhammad Yunus has given a new perspective on life to the 1.1 billion people who live on less than a dollar a day," said a statement from Oikocredit, which received support in the mid-1970s from the World Council of Churches.
New York court says insurance says cover contraception.
Catholic Charities and other religious organizations must provide insurance that pays for prescription contraception, New York's highest court has ruled.
The decision by the state Court of Appeals upholds a January ruling by the appellate division of the state Supreme Court. The judges rejected religious groups' arguments that the 2002 Women's Health and Wellness Act violates religious freedom.
Ten religious organizations sued the state in December 2002, claiming that the law requiring health insurers to provide coverage for obstetric and gynecologic care should include a religious exemption for organizations that do not support birth control. The ten groups were seeking to broaden a "religious employer" exemption to include church-related schools and social-service organizations.
The New York State Catholic Conference expressed disappointment with the decision. "Any religious organization must have the right in American society to uphold its own teachings," the conference said. It contended that the October 19 ruling opens the way for legislation that would force Catholic entities to pay for employees' abortions.
The ruling reinforces the intention of the women's health legislation, said Betty DeFazio, speaking for Planned Parenthood of the Rochester/Syracuse Region. "Birth control for women is basic health care."

</file>

<file= AmE06_D07>

The Mighty and the Almighty 
I had watched previous inaugural addresses, but the first one I truly took in was John Kennedy's in 1961. My brother John, who was in junior high school, played the trumpet in the Denver police band and had been invited to Washington to march in the inaugural parade. It seems that everyone remembers the snow on the ground and how the glare of sunshine made it impossible for Robert Frost to read the poem he had composed for the occasion. The new president, hatless in the crystal-cold air, his breath visible, asked us to "ask not." It was the speech about "passing the torch" to another generation. I saw it on television -- that is how I experienced all the inaugural addresses until 1993. Then, and again four years later, I watched President Clinton deliver his speeches from the balcony of the U.S. Capitol. The words combined with the crowds and the view of the Washington Monument brought out the sense of history and pride in the United States that has done so much to shape my view of the world. 
• • • 
The inaugural address provides an American president with a matchless opportunity to speak directly to 6 billion fellow human beings, including some 300 million fellow citizens. By defining his country's purpose, a commander in chief can make history and carve out a special place for himself (or perhaps, one day, herself) within it. On January 20, 2005, facing an audience assembled in the shadow of the Capitol, President George W. Bush addressed America and the world. From the first words, it was evident that both he and his speechwriters had aimed high. "It is the policy of the United States," he declared, "to seek and support the growth of democratic movements and institutions in every nation and culture, with the ultimate goal of ending tyranny in our world." He continued, "History has an ebb and flow of justice, but history also has a visible direction, set by liberty and the Author of Liberty." The president concluded that "America, in this young century, proclaims liberty throughout the world and to all the inhabitants thereof." He might have added that, in the Bible, God had assigned that same job, in the same words, to Moses. 
The speech was vintage George W. Bush, one that his admirers would hail as inspirational and his detractors would dismiss as self-exalting. It was of a piece with the president's first term, during which he had responded to history's deadliest strike on U.S. soil, led America into two wars, roused passions among both liberals and conservatives, set America apart from longtime allies, aggravated relations with Arab and Muslim societies, and conveyed a sense of U.S. intentions that millions found exhilarating, many others ill-advised. 
Within the United States, there are those who see the president as a radical presiding over a foreign policy that is, in the words of one commentator, "more than preemptive, it is theologically presumptuous; not only unilateral, but dangerously messianic; not just arrogant, but rather bordering on the idolatrous and blasphemous." The president's supporters suggest the contrary, that his leadership is ideally, even heroically, suited to the perils of this era and in keeping with the best traditions of America. 
My own initial instinct, particularly when the president is trumpeting the merits of freedom, is to applaud. I firmly believe that democracy is one of humankind's best inventions: a form of government superior to any other and a powerful source of hope. I believe just as firmly in the necessity of American leadership. Why wouldn't I? When I was a little girl, U.S. soldiers crossed the ocean to help save Europe from the menace of Adolf Hitler. When I was barely in my teens, the American people welcomed my family after the communists had seized power in my native Czechoslovakia. Unlike most in my generation who were born in Central Europe, I had the chance to grow up in a democracy, a privilege for which I will forever be grateful. I take seriously the welcoming words at the base of the Statue of Liberty; and I love to think of America as an inspiration to people everywhere -- especially to those who have been denied freedom in their own lands. 
As appealing as President Bush's rhetoric may sometimes be, however, I also know that proclaiming liberty is far simpler than building genuine democracy. Political liberty is not a magic pill people can swallow at night and awaken with all problems solved, nor can it be imposed from the outside. According to the president, "Freedom is God's gift to everybody in the world." He told Bob Woodward, "As a matter of fact, I was the person who wrote the line, or said it. I didn't write it, I just said it in a speech. And it became part of the jargon. And I believe that. And I believe we have a duty to free people. I would hope we wouldn't have to do it militarily, but we have a duty." 
These are uplifting sentiments, undoubtedly, but what exactly do they mean? The president says that liberty is a gift to everybody, but is he also implying that God appointed America to deliver that gift? Even to raise that question is to invite others. Does the United States believe it has a special relationship with God? Does it have a divinely inspired mission to promote liberty? What role, if any, should religious convictions play in the decisions of those responsible for U.S. foreign policy? But perhaps we should begin by asking why we are even thinking about these questions, given America's constitutional separation between church and state. And haven't we long since concluded that it is a mistake, in any case, to mix religion and foreign policy? I had certainly thought so. 
Although -- as I learned late in life -- my heritage is Jewish, I was raised a Roman Catholic. As a child, I studied the catechism, prayed regularly to the Virgin Mary, and fantasized about becoming a priest (even a Catholic girl can dream). As I was growing up, my sense of morality was molded by what I learned in church and by the example and instruction of my parents. The message was drilled into me to work hard, do my best at all times, and respect the rights of others. As a sophomore at Wellesley College, I was required to study the Bible as history, learning the saga of ancient Israel in the same way as that of Greece or Rome. 
As an immigrant and the daughter of a former Czechoslovak diplomat, I was primarily interested in world affairs. I did not, however, view the great issues of the day through the prism of religion -- either my own or that of others. Nor did I ever feel secure enough about the depth of my religious knowledge to think I was in a position to lecture acquaintances about what they should believe. I did not consider spiritual faith a subject to talk about in public. For the generation that came of age when and where I did, this was typical. I am sure there were parts of America where attitudes were different, but the scholar Michael Novak got it right when he asserted in the early 1960s, "As matters now stand, the one word [that could not be used] in serious conversation without upsetting someone is 'God.' " 
The star most of us navigated by in those years was modernization, which many took as a synonym for secularization. The wonders we celebrated were less biblical than technological: the space race, breakthroughs in medicine, the birth of nuclear power, the introduction of color television, and the dawn of the computer age. In the United States, the play and movie Inherit the Wind dramatized the triumph of science (the theory of evolution) over creationism (a literal interpretation of Genesis). When we thought of Moses, the image that came to mind was Charlton Heston, in technicolor. Religious values endured, but excitement came from anticipating what our laboratories and researchers might come up with next. We Americans were not alone in our pragmatic preoccupations. Abroad, the rising political tides were socialism and nationalism, as Africans and Asians freed themselves from their colonial overseers and began the task of building countries that could stand on their own. 
In the early 1980s, I became a professor at Georgetown University. My specialty was foreign policy, about which such icons as Hans Morgenthau, George Kennan, and Dean Acheson theorized in almost exclusively secular terms. In their view, individuals and groups could be identified by the nations to which they belonged. Countries had governments. Governments acted to protect their nations' interests. Diplomacy consisted of reconciling different interests, at least to the point where wars did not break out and the world did not blow up. Foreign policy was commonly compared to a game of chess: cerebral, with both sides knowing the rules. This was a contest governed by logic; its players spoke in the manner of lawyers, not preachers. During my adult years, western leaders gained political advantage by deriding "godless communism"; otherwise, I cannot remember any leading American diplomat (even the born-again Christian Jimmy Carter) speaking in depth about the role of religion in shaping the world. Religion was not a respecter of national borders; it was above and beyond reason; it evoked the deepest passions; and historically, it was the cause of much bloodshed. Diplomats in my era were taught not to invite trouble, and no subject seemed more inherently treacherous than religion. 
This was the understanding that guided me while I was serving as President Clinton's ambassador to the United Nations and secretary of state. My colleagues felt the same. When, in 1993, Professor Samuel Huntington of Harvard predicted that the era following the end of the cold war might well witness an interreligious "clash of civilizations," we did all we could to distance ourselves from that theory. We had in mind a future in which nations and regions would draw closer as democratic bonds grew stronger, not a world splitting apart along historic fault lines of culture and creed. When fighting broke out in the Balkans, we urged each side to focus on the rights of the individual, not the competing prerogatives of religious groups. In 1998, after U.S. embassies in Kenya and Tanzania were bombed by terrorists, we published posters seeking information and offering a reward; these posters had the heading, "This is not about religion. This is not about politics. This is about murder, plain and simple." During the administration's marathon effort to find a basis for peace in the Middle East, President Clinton and I were fully aware of the religious significance of Jerusalem's holy places. We hoped, nevertheless, to devise a legal formula clever enough to quiet the emotions generated by the past. We asked and expected both sides to be realistic and settle for the best deal they could get. 
We were living, after all, in modern times. The wars between Catholics and Protestants that had claimed the lives of one-third the population of Christian Europe had been brought to a close in 1648 by the Peace of Westphalia. Large-scale fighting between Christians and Muslims had ceased when, in 1683, the advance of the Ottoman Turks was halted at the gates of Vienna. I found it incredible, as the twenty-first century approached, that Catholics and Protestants were still quarreling in Northern Ireland and that Hindus and Muslims were still squaring off against each other in south Asia; surely, I thought, these rivalries were the echoes of earlier, less enlightened times, not a sign of battles still to come. 
Since the terror attacks of 9/11, I have come to realize that it may have been I who was stuck in an earlier time. Like many other foreign policy professionals, I have had to adjust the lens through which I view the world, comprehending something that seemed to be a new reality but that had actually been evident for some time.

</file>

<file= AmE06_D08>

Introduction 

When scholars apply the term "ecumenical" to marriage, they often seem to use it to mean marriages full of discord and division. Ecumenical marriages, however, should be understood as a potential resource for Christian reunion. This essay, which is a revision of the 2005 Lourdes College Ecumenical Lecture, examines how ecumenical marriages might fulfill this role. It begins by investigating how marriages unite people physically, interpersonally, socially, and theologically, all through love. Then, it indicates how these unifying dimensions of marriage can reconcile differing denominations through ecumenical marriages.

The word "ecumenical" comes from the Greek word "oikoumenikos," which literally translates as "to inhabit the house" and meant "encompassing the whole world." In today's usage, the term's meaning has changed from "including the whole" to "trying to make whole." Churches or people are ecumenical because of their attempts to overcome denominational differences. Yet, when scholars employ the term "ecumenical" in reference to marriage, they seem to use it to refer to discord and division. They do not do this by taking up the negative connotations and rancorous language that for so long characterized approaches to ecumenical marriages. In fact, these authors are in general very supportive and affirming of interdenominational marriages. Instead, the approach these scholars take assumes that ecumenical marriages are troubled. Almost every essay or book on the subject focuses on problems ecumenical marriages face.

Some works start by noting that social-science research reveals several difficulties for interchurch couples. These marriages have lower levels of marital satisfaction and church involvement and higher divorce rates than marriages where the partners are of the same religious tradition. The most frequent causes of conflict in these marriages are "lack of companionship and disagreement over children's religious upbringing." To be fair, most authors nuance this research by noting that other factors, such as church attendance and cultural background, might be the real problems, not denominational differences. Even so, they still indicate that both interdenominational couples and the churches that support them should be attentive to these difficulties.

Other scholarship focuses on practical and pastoral issues facing ecumenical marriages, such as the problems of intercommunion and the celebration of the sacraments. Several authors mention the difficulties of raising children in an interdenominational home. Still others address how interchurch couples need to relate both personally and ecclesiastically. Although overwhelmingly positive, the Interchurch Families Journal and the Association of Interchurch Families' Issues--Reflections--News, the former journal's subsequent manifestation, focus primarily on the problems ecumenical marriages face. Hence, the title of the latter publication lists "issues" first.
This approach to ecumenical marriages assumes that they are fundamentally problematic. By starting with a problem or primarily focusing on a problem, the literature seems to reduce these marriages to their conflicts and difficulties. One of the principal results of this approach is the neglect of one of the key positive aspects of ecumenical marriages. Time and time again, scholars claim that these marriages can contribute to Christian unity. These relationships are "called to exercise a prophetic role for our larger church communities"; "contain numerous elements that ...  [contribute] ...  to the ecumenical movement"; function as "sign," "instrument," and "sacrament" of Christian unity; model "communion" for the churches; and witness to the "unity within our churches." These profound and positive statements about ecumenical marriages suffer from scant attention or development. They seem at times to be an afterthought, a nice sentiment to console the problematic marriage. Whereas the literature is not bad or worthless, the current scholarship on ecumenical marriages is weak--not only for focusing primarily on the problems of these unions but also for neglecting a potentially significant source for Christian reconciliation.
If the positive statements about ecumenical marriages were to be accepted as true and "ecumenical" in the modern sense of "bringing Christians together," the approach to these marriages would change. Although the difficulties that arise in marriages with differing religious traditions and the desperate need for pastoral sensitivity and assistance in these circumstances should not be dismissed, they should not be the fundamental lens through which to understand ecumenical marriages. Ecumenical-marriage rates hover around forty percent for Christians in general, thirty-six percent for Catholics, and twenty-eight percent for active Catholics (those who attend church regularly).To approach these numerous marriages as being primarily problematic misses the true problem: Christians are divided. As Fr. Thomas Ryan, the director of the Paulist North American Office for Ecumenical and Interfaith Relations, has written: "We are quick to forget though that this anomaly [ecumenical marriage] only arises on account of a prior and greater one: the division between churches. These couples have not asked to have the historical divisions of the churches laid on them as they try to live an experience of unity and communion of life in their marriage, but that is what the churches bequeath them."

The reality is that it is not ecumenical marriages that are the origin of division but the churches, and the churches must work to heal the discord. Hence, ecumenical marriages need to be approached differently, both for the sake of the marriages and for the sake of the churches. Ecumenical marriage should be understood as being "witness," "sacrament," and "instrument" of ecclesial reconciliation. The lack of attention to these ideas has both fostered a negative approach to ecumenical marriages and left untapped a great resource for Christian reunion.
This essay examines this potential for ecumenical marriages in two steps. First, it investigates the unifying dimension of marriages in general. Marriages unite people physically, interpersonally, socially, and theologically, all through the impetus of love. Pair these dimensions with the early Christian understanding of marriage as a domestic church --the smallest manifestation of the church, and one has an example of a church uniting and a vision of a united church. Second, this essay then indicates how these unifying dimensions of marriage can reconcile differing denominations through ecumenical marriages. In doing so, a truly positive approach to ecumenical marriages is offered.

Marriage 

What makes marriage work and makes two become one involves a number of factors. First of all, marriage must be rooted in love. This includes the initial falling in love and the romantic moments during the honeymoon year and periodically throughout the rest of the marriage. This love opens up the lover to the world of the beloved and vice versa. The spouses' values, understanding of the world, cares, and concerns are all transformed as their horizons start to intermingle and shape one another. However, this romantic love (what the ancient Greeks called "eros") is not permanent but waxes and wanes. Marriages are hard work. While romance gets a couple started and rejuvenates them from time to time, it cannot sustain a marriage. The instability of romance cannot provide the stable foundation needed for the permanence of marriage.

Marriage needs an additional love, one based on the will, a love that habitually chooses what is good, a love that the early Christians called "agape" and medieval Christians called charity. This kind of love grounds the interactions of the couple in a mutual disposition to do what is truly good for each other. This disposition persists in the midst of disagreements and arguments, when the relationship is going well and when it is struggling, and when one, both, or neither party is in good health. Together, romantic love and agape unify couples by disposing them more and more to do what is right for each other, even when they feel otherwise. The concern of one becomes the concern of both.

This latter love takes the mutuality initiated by romantic love and transforms it by orienting it toward what is good and cementing it in the couple's daily actions. Yet, the habit of perpetually considering the spouse can develop a habit of perpetually considering the thoughts of others. If the spouses can do this well in their marital relationships, they have the basic skills to do it well in their parental and neighborly relationships and with strangers and even enemies. Hence, the unity achieved by a perfect marital love is one that not only unites the couple in common understandings and values but also has the potential to unite the couple with other people.
This love is the foundation. It is the impetus that unites couples. Far from abstractly, however, it unites people concretely through all the various aspects of their relationship. First, it unites them physically. Often, sex as an expression of love is understood as the principle of physical unity. It unites people through their bodies. The Catholic Church even goes as far as maintaining that sex is an inherently unitive act. Sex, though, is not the only means (or perhaps even the primary means)of physical unity. If directed by love, the reality of physically living together is a powerful unifying force. The sharing of a bed, a bathroom, a home, childcare, meals, and all the daily, routine tasks and rituals almost intrinsically unite a couple. Aristotle believed that genuine friends needed to live together in order truly to share the same virtues. Only then would each friend become the "another self" that Aristotle thought friends should be. If this power of domesticity is true for friendship, then it is also true for marriage. The reality and power of actually living together, especially if it is directed by love, is a potent, physically unifying force.

Second, love also unites people through their interpersonal relationships. It moves couples to learn how to communicate with each other. Spouses need to learn how to negotiate the demands of living together in a household, of being parents, grandparents, and aunts and uncles. Each must learn what he or she values and how to discuss the inevitable differences between each other's values. The spouses must learn to address each other's pasts, both the successes and the failures. These interactions must be done in a way that genuinely seeks the good of the other and the good of the marriage. In other words, genuine communication must be directed by love, and, if it is, then the communication leads to union. Hence, it should not be a surprise that the word "communication" comes from the Latin word "communis," which means "mutual participation," and that this word is also the root of "communion," which Christians have used for fellowship among Christians and, by applying the word to the Lord's Supper, fellowship with God.

Third, because love is never restricted to the physical or interpersonal, it also brings a social dimension to marriage. It is almost banal to note the negative effects contemporary society has on marriages; divorce, infidelity, domestic violence, consumerism, and gender inequality are but a few issues. Although not determinative, sociological factors can make good marriages more difficult to maintain. However, the influence can go both ways. Couples can contribute to the well-being of society. Spouses can work at jobs that help improve the quality of society. Teachers do this, as do lawyers, doctors, social workers, businesspeople, nurses, and economists. Moreover, spouses can contribute to society by the care of their children. Establishing a stable and loving environment for children provides the best opportunity for the children to do well in school, sports, and relationships--and thus to do well in the world. These changes not only make the culture better and more peaceful but also make it an environment that is more supportive of marriage and families. The key is to realize that a marriage rooted in genuine love not only transforms the couples but also transforms children, neighbors, strangers, enemies, and societies. In and through these transformations, the couple is not only united but also creates an environment that is more conducive to being united.

Finally, love unites people theologically. The religious dimensions of family can be divisive or unifying. Beliefs about the nature of the world, human beings, and the meaning of life affect decisions, actions, understandings, and the way people can and cannot relate. If a couple is rooted in love, the couple is moved to address these differences and reconcile them.

</file>

<file= AmE06_D09>

A teenage modern dance troupe dressed all in black took their places on the stage of the First Baptist Church of Pleasant Grove, a suburb of Birmingham, Alabama. Two dancers, donning black overcoats, crossed their arms menacingly. As a Christian pop ballad swelled on the speakers, a boy wearing judicial robes walked out. Holding a Ten Commandments tablet that seemed to be made of cardboard, he was playing former Alabama Supreme Court justice Roy Moore. The trench-coated thugs approached him, miming a violent rebuke and forcing him to the other end of the stage, sans Commandments. 
There, a cluster of dancers impersonating liberal activists waved signs with slogans like "No Moore!" and "Keep God Out!! No God in Court." The boy Moore danced a harangue, first lurching toward his tormentors and then cringing back in outrage before breaking through their line to lunge for his monument. But the dancers in trench coats -- agents of atheism -- got hold of it first and took it away, leaving him abject on the floor. As the song's uplifting chorus played -- "After you've done all you can, you just stand" -- a dancer in a white robe, playing either an angel or God himself, came forward and helped the Moore character to his feet. 
The performance ended to enthusiastic applause from a crowd that included many Alabama judges and politicians, as well as Roy Moore himself, a gaunt man with a courtly manner and the wrath of Leviticus in his eyes. Moore has become a hero to those determined to remake the United States into an explicitly Christian nation. That reconstructionist dream lies at the red-hot center of our current culture wars, investing the symbolic fight over the Ten Commandments -- a fight whose outcome seems irrelevant to most peoples' lives -- with an apocalyptic urgency. 
On November 13, 2003, Moore was removed from his position as chief justice of the Alabama Supreme Court after he defied a judge's order to remove the 2.6-ton Ten Commandments monument he'd installed in the Montgomery judicial building. On the coasts, he seemed a ridiculous figure, the latest in a line of grotesque Southern anachronisms. After all, Moore is a man who, in a 2002 court decision awarding custody of three children to their allegedly abusive father over their lesbian mother, called homosexuality "abhorrent, immoral, detestable, a crime against nature, and a violation of the laws of nature and of nature's God upon which this Nation and our laws are predicated," and argued, "The State carries the power of the sword, that is, the power to prohibit conduct with physical penalties, such as confinement and even execution. It must use that power to prevent the subversion of children toward this lifestyle, to not encourage a criminal lifestyle." He's a man who writes rhyming poetry decrying the teaching of evolution and who fought against the Alabama ballot measure to remove segregationist language from the state constitution. 
To the growing Christian nationalist movement, though, Roy Moore is a martyr, cut down by secular tyranny for daring to assert God's truth. 
It's a role he seems to love. The battle that cost Moore his job wasn't his first Ten Commandments fight. In 1995, the ACLU sued Moore, then a county circuit judge, for hanging a Ten Commandments plaque in his courtroom and leading juries in prayer. As Matt Labash recalled in an adulatory Weekly Standard article, "The conflict's natural drama was compounded when the governor, Fob James, announced that he would deploy the National Guard, state troopers, and the Alabama and Auburn football teams to keep Moore's tablets on the wall." 
That case reached an ambiguous conclusion in 1998, when the state supreme court threw out the lawsuit on technical grounds. By then, Moore had become a star of the right. Televangelist D. James Kennedy's Coral Ridge Ministries raised more than $100,000 for his legal defense fund, and Moore spoke at a series of rallies that drew thousands. His right-wing fame helped catapult him to victory in the 2000 race for chief justice of the state supreme court. 
Moore installed his massive Ten Commandments monument on August 1, 2001, and from the beginning, he and his allies used it to stir up the Christian nationalist faithful. He gave videographers from Coral Ridge Ministries exclusive access to the courthouse on the night the monument was mounted, and on October 14, D. James Kennedy started hawking a $19 video about Moore's brave, covert installation on his television show. 
As the controversy over the statue ignited, Moore's fame grew. At rallies across the country, he summoned the faithful to an ideal that sounded very much like theocracy. "For forty years we have wandered like the children of Israel," he told a crowd of three thousand supporters in Tennessee. "In homes and schools across our land, it's time for Christians to take a stand. This is not a nation established on the principles of Buddha or Hinduism. Our faith is not Islam. What we follow is not the Koran but the Bible.This is a Christian nation." 
By the time he was removed as chief justice, Moore had sparked a movement, and his monument was an icon. In the days before officials came to cart the Commandments away, hundreds flocked to Montgomery to rally on the courtroom steps. Some slept there and imagined themselves the nucleus of a new civil rights movement. 
Thomas Bowman, a bearded Christian folk singer from Kentucky who wears a knit Rasta hat, wrote an anthem called "Montgomery Fire" celebrating the demonstrations: "We had love in our hearts that no man could ever remove / but with the whole world we watched as they hauled the Commandments away." When I met him a year later at First Baptist, he referred to the protesters, romantically, as the "ragamuffin warriors" fighting for God against the atheist state. During the controversy, he said, he'd felt the Lord's call, and driven six and a half hours from Louisville. In Montgomery, he met others like him, who'd felt compelled to take a stand against secularism. 
"The opposing side, the anti-God side, the do-whatever-you-want side, the judicial side, just kept pushing and pushing and pushing for the last forty years," Bowman said. "They keep moving that line back." Finally, he said, God called on Christians to defend themselves. 
After the Commandments were removed, a group of retired military men from Texas who called themselves American Veterans in Domestic Defense spent months taking the monument -- now affectionately called "Roys Rock" -- on tour all over the country, holding more than 150 viewings and rallies in churches, at state capitols, even in Wal-Mart parking lots. Moore also found powerful supporters in statehouses and in Congress who proposed laws to radically restrict the power of federal courts to enforce the separation of church and state. In solidarity, another Alabama judge, Ashley McKathan, had the Ten Commandments embroidered onto his robe. Christian homeschool catalogues offered copies of a video titled "Roy Moores Message to America." When Moore suggested he might run for Alabama governor, state polls showed him with a double-digit lead. 
A few days before Bush's second inauguration, The New York Times carried a story headlined "Warning from a Student of Democracy's Collapse" about Fritz Stern, a refugee from Nazi Germany, professor emeritus of history at Columbia, and scholar of fascism. It quoted a speech he had given in Germany that drew parallels between Nazism and the American religious right. "Some people recognized the moral perils of mixing religion and politics," he was quoted saying of prewar Germany, "but many more were seduced by it. It was the pseudo-religious transfiguration of politics that largely ensured [Hitler's] success, notably in Protestant areas." 
It's not surprising that Stern is alarmed. Reading his forty-five-year-old book "The Politics of Cultural Despair: A Study in the Rise of the Germanic Ideology," I shivered at its contemporary resonance. "The ideologists of the conservative revolution superimposed a vision of national redemption upon their dissatisfaction with liberal culture and with the loss of authoritative faith," he wrote in the introduction. "They posed as the true champions of nationalism, and berated the socialists for their internationalism, and the liberals for their pacifism and their indifference to national greatness." 
Fascism isn't imminent in America. But its language and aesthetics are distressingly common among Christian nationalists. History professor Roger Griffin described the "mobilizing vision" of fascist movements as "the national community rising Phoenix-like after a period of encroaching decadence which all but destroyed it" (his italics). The Ten Commandments has become a potent symbol of this dreamed-for resurrection on the American right. 
True, our homegrown quasi-fascists often appear so absurd as to seem harmless. Take, for example, American Veterans in Domestic Defense, the organization that took the Ten Commandments on tour. The group says it exists to "neutralize the destructiveness" of America's "domestic enemies," which include "biased liberal, socialist news media," "the ACLU," and "the conspiracy of an immoral film industry." To do this, it aims to recruit former military men. "AVIDD reminds all American Veterans that you took an oath to defend the United States against all enemies, 'both foreign and domestic,'" its Web site says. "In your military capacity, you were called upon to defend the United States against foreign enemies. AVIDD now calls upon you to continue to fulfill your oath and help us defend this nation on the political front, against equally dangerous domestic enemies." 
According to Jim Cabaniss, the seventy-two-year-old Korean War veteran who founded AVIDD, the group now has thirty-three chapters across the country. It's entirely likely that some of these chapters just represent one or two men, and as of 2005, AVIDD didn't seem large enough to be much of a danger to anyone. 
Still, it's worth noting that thousands of Americans nationwide have flocked to rallies at which military men don uniforms and pledge to seize the reins of power in America on behalf of Christianity. In many places, local religious leaders and politicians lend their support to AVIDD's cause. And at least some of the people at these rallies speak with seething resentment about the tyranny of Jews over America's Christian majority. 
"People who call themselves Jews represent maybe 2 or 3 percent of our people," Cabaniss told me after a January 2005 rally in Austin. "Christians represent a huge percent, and we don't believe that a small percentage should destroy the values of the larger percentage." 
I asked Cabaniss, a thin, white-haired man who wore a suit with a red, white, and blue tie and a U.S. Army baseball cap, whether he was saying that American Jews have too much power. "It appears that way," he replied. "They're a driving force behind trying to take everything to do with Christianity out of our system. That's the part that makes us very upset." 
Ed Hamilton, who'd come to the rally from San Antonio, interjected, "There are very wealthy Jews in high places, and they have significant control over a lot of financial matters and some political matters. They have disproportionate amount of influence in our financial structure." 
We were standing outside the Texas Capitol building on a sunny Saturday morning. A few hundred people from across the state had turned out for the rally, which began at 10 a.m. Three or four men in military uniforms sat with their wives on chairs at the top of the Capitol steps. Next to them sat an old man dressed as Uncle Sam in a tall Stars and Stripes top hat, a red, white, and blue suit, and a pointy white beard. Four other men supported tall, coffin-shaped signs labeled with the names of objectionable Supreme Court rulings. 
The crowd was full of teenagers who'd come on church buses and families with young children. A white-bearded man in a leather biker vest dragged a ten-foot-tall cedar crucifix painted red, white, and blue. One woman wore a T-shirt with a photograph of Moore's monument. Another held a handwritten sign saying: 
Ban Judges 
Not God 
God Rules

</file>

<file= AmE06_D10>

SECRECY AND DISPLAY IN THE QUEST FOR TRANSCENDENCE IN CHINA, CA. 220 BCE - 350 CE

There is something about a secret which makes people believe.
(Graham Greene,Our Man in Havana )
In ancient, medieval, and early modern times, esoteric religious traditions
were developed in several civilizations. They represented paths of salvation,
often including alchemical and alimentary disciplines and some sort
of claim to personal immortality, which were transmitted in secret texts
from masters to initiated disciples and were at least partially defined in
contrast to the sacrificial cults of mainstream society. One thinks of the
mysteries of Eleusis and Dionysus, of the various so-called Gnostic texts
and movements, and of the Hermetic path in the Mediterranean world; of
Manichaeism, which once flourished from western Europe and along the
Silk Road all the way into China; of the "books of secrets" of medieval
and Renaissance Europe and the communities of practitioners who made
and transmitted them; and of the siddha traditions of India.

In addition to these complex regimes of salvation, with their intricate mythologies
and cosmologies and their elaborate biospiritual technologies, bodies of
"practically religious" techniques were developed for the pursuit of more
proximate, if no less urgent, felicities; these esoteric procedures, often
rather grossly and unsatisfactorily labeled "magic" or "the occult" in
modern writings, were also claimed to be derived from hidden sources,
their dissemination - however widespread - at least initially controlled by
initiates. Whatever their stated goals and methods, all of these esoteric
traditions, practices, and texts share two essential features: they were constituted
(by both insiders and outsiders) as being in tension with other religious
and cultural currents often seen as predominant and public, and
access to them was restricted, if only by the crucial claim that they
stemmed from secret sources not publicly accessible. A gulf separates
these cultures of secrecy, wherein knowledge was deemed powerful because
access to it was restricted and access to it was restricted because it
was deemed powerful, from those styles of culture Steven Shapin and
Peter Burke have recently argued arose only in early modern times (in
Europe at least), wherein truth-telling and the art of civil conversation
among gentlemen with no reason to misrepresent things formed the basis
of a new epistemological decorum, and wherein lying and secrecy came
to be seen as violations of a code of honor.

Sporadically for a century now, scholars beginning with Georg Simmel
have attempted a "sociology of secrecy" or "sociology of esoteric culture"
3
 - an enterprise perhaps possible only in a historical period when
secrecy has come to seem unusual. To quote one of the most useful contributions
to this literature, "a crucial aspect of esoteric knowledge is that
it is a secret knowledge of the reality of things, handed down, frequently
orally and not all at once, to a relatively small number of persons who are
typically ritually initiated by those already holding this knowledge." In progressively
realizing this knowledge, "the subject develops internally and
liberates himself from the strictures of everyday life." Because this knowledge
is of "the real but concealed nature of things," its recipients must
prove themselves worthy to receive it, so we often find "a series of trials
and ordeals, in the course of which the adept becomes increasingly socialized
into the esoteric culture and increasingly desocialized from the natural
attitude of the exoteric culture." Since esoteric knowledge must be concealed
from public dissemination, it is often "presented to the adept not
directly but . . . symbolically or metaphorically, so that it has to be deciphered
progressively by the neophyte who uncovers layers of meaning
in stages of initiation." Correspondingly, "the social organization and
handling of esoteric culture tends to take the form of secret societies, societies
whose modes and codes of organization and membership lists are
not publicly disclosed." Finally, "since [esoteric] secrets are those that
reveal the ultimate nature of reality, the concealed forces of the cosmic
order, . . . esoteric knowledge is an ultimate source of power, which must
be shared and utilized by a relatively small group of initiates. Such power
is never justifiable in terms of the enhancement of the material conditions
of the esoteric knowers but rather in terms of broad impersonal ends [or]
humanitarian ideals." At first blush, this list of general features seems to fit the masters of esoterica
(fangshi) and practitioners of longevity and transcendence arts
in late classical and early medieval China, with two adjustments. The first
necessary adjustment is that the most "social organization" such practitioners
ever achieved was the formation of loose (and often only fictively
and retrospectively constructed) lineages of the transmission of texts and
techniques. There were no exclusive brotherhoods of practitioners, no
schools or cross-lineage organizations, and no secret societies, only masters with (usually) small clusters of disciples who were free and often
encouraged by their teachers to seek esoteric knowledge elsewhere as
well.

The second adjustment is that the Chinese practitioners of this period
rarely appealed to "broad impersonal ends" or "humanitarian ideals"
to justify their practices. Accordingly, we should be alert to the possibility
that, in the sorts of cases on which this stipulation of Tiryakian's is based,
such professions may have as much to do with self-justifying ideology as
with "humanitarian" concern.

However, in what follows I want to suggest a more fundamental revision
to this sociology of secrecy. To do so I will draw on recent work
by scholars such as Beryl Bellman, Lamont Lindstrom, and Hugh B. Urban
that approaches secrecy as a cluster of forms of discourse rather than as
a type of content, and that approaches discourses of secrecy as strategies
for accumulating a type of cultural capital, to invoke the language of Pierre
Bourdieu.

More specifically, the following article is, in part, an elaboration
on one particular strategy briefly characterized by Urban as "advertising
the secret" - that is, "the claim to possess very precious, rare, and
valuable knowledge, while simultaneously partially revealing and largely
concealing it. For a secret is only worth anything if someone knows you
have a secret."

I will argue a proposition about the practice and practitioners of esoteric
arts in China up until roughly 350 CE, one that may apply equally to esoteric
traditions in other societies (a possibility I leave for future exploration).
That proposition, which will be filled in more clearly below, may for
now be summarized as follows. In China, at least, there was a basic tension
between hidden arts and observed wonders, between reclusion and recognition,
and these two opposing tendencies implied and reinforced each
other; to describe the tendency to secrecy without describing the other
member of the dyad is to paint an incomplete picture. At least if we understand
power and prestige in social terms, for initiated practitioners to
have accrued any power or prestige from their possession of secretly
transmitted arts, it was necessary that the impressive effects of those arts
be shown to the uninitiated. And, as I will demonstrate, they were indeed
shown, massively, publicly, to people of all social levels - and especially
to the elite. For prestige to accrue, it was necessary also that practitioners of such arts be recognized by others as practitioners, whether or not they flaunted their abilities; as we will see, some are remembered for performing wonders, others for refusing to do so, but to the extent that individual
practitioners are textually remembered as practitioners at all, we can be
sure that they were recognized as practitioners by audiences and that this
recognition was inscribed in works of collective memory. In short, the
detailed methods of the esoteric arts and the texts in which they were
transmitted remained hidden, in principle if not always in practice, from
public view, but the wondrous effects of these arts and the reputations of
those who wielded them were socially displayed, socially sought after and
supported, socially discussed, and socially consumed. I will suggest that
secrecy both implied and stoked outsiders' interest; the existence of esoteric
knowledge was rarely a secret; neither the identities of its possessors
nor the special powers it afforded these persons were typically secret. Only
the contents of the knowledge (or even, in some cases, only a small but
key proportion of the contents) were secret. I will argue that this "simultaneous
proclamation and concealment," to borrow Frank Kermode's fine
characterization of the Gospel of Mark, was fundamental to constituting
the "master of esoterica" or "seeker of transcendence" as social roles, and
that, as in Mark, much depended on the recognition of adepts by others
and on the interpretation of their strange words and behaviors.

I begin, in the first section below, by charting the textual portrayal of
secrecy among practitioners of esoteric arts in China. This charting will
then lead us naturally to notice, in the second section, practitioners' habit
of displaying their secretly transmitted abilities. A concluding section
highlights the tension and co-implicative feedback relationship between
these opposing habits.

The culture of esotericism

A history of secrecy in early Chinese cul ture might begin with chapter 65
of the received Daode jing ("Of old those who excelled in the
pursuit of the way did not use it to enlighten the people but to hoodwink
them"), proceed to early "allegorical" and other understandings of the Book of Odes
as indirect or veiled speech, continue to the notion that Confucius hid his teachings in indirect sayings and historical passages that required decoding by interpreters, treat the role of deception and secrecy
in rhetorical and military strategy, and alight on the strange hermeneutic
applied to what had by then become the canonical works of the imperium
in the Han weft texts (of which we are in desperate need of renewed
study) - a hermeneutic that assumed and implied the classics spoke in
code. Such a history has yet to be written. When it is, it should weigh the implications
of these facts: that books in general during this entire period - 
existing only in laboriously copied manuscripts, of course - were rare,
treasured commodities, in many cases not available at all to private readers
but rather locked up inside royal libraries; that even such (to us at least)
apparently nonesoteric books as the texts of the ancient philosophers, including
the Zhuangzi, were held in secret, a practice motivated or at least
justified by the fear of the texts' dangerous effects on readers; and that
there were no bookshops before the first century CE.

As Donald Harper has observed, "Notions of secrecy fueled Han bibliophilia in general; books were regarded as objects of power. In this environment the concern
that books might be misused by the 'wrong person' and the desire to
possess them were equally matched. Specialists [that is, practitioners of
esoteric arts] and elite collectors [of books] . . . shared this mentality."

Books of esoterica, of course, were hedged about with even more
secrecy, expressed and enacted in several ways. The first of these was
nomenclature. The key generic term used to denote esoteric arts, texts,
and practitioners - fang, literally "recipes" or "methods" - itself already
implies specialization and limited access, heightened by its occasional
pairing with modifiers such as mi, " secret," and jin, "prohibited." More fundamentally, these books' contents and typical rhetorical forms created an aura of remote origins. In other words, many esoteric texts were written in ways that created a palpable sense of their own inaccessibility, rarity, and hence, extreme value and sacred power. self-esotericizing texts

Divine revelation and extreme antiquity are two types of remote origin
commonly claimed by esoteric texts for themselves, often simultaneously.
One of the rhetorically simplest ways in which such origins were claimed
was to frame the text as a dialogue between beings widely recognized
as divine, ancient, or both. Two of the medical manuscripts found at
Mawangdui, for example, share this structure. The text titled "Ten Questions"
(Shiwen) presents itself as the record of dialogues between
various ancient figures, notably including the Yellow Thearch and
Yu, who are cast as disciples, and more authoritative and in some cases
transcendent figures, including the likes of the Celestial Teacher,
Rong Chen, and Ancestor Peng.

</file>

<file= AmE06_D11>

Common Ground

I arrived at Marquette University for graduate studies in theology with high hopes but low expectations. But soon I encountered grace upon grace. I met a kind and brilliant pastor who was willing to talk theology with me until the wee hours. He told me of his upbringing in a Polish-American home where family members customarily greeted one another with phrases from the Scriptures. But, I told myself, he was hardly an ordinary Catholic. He held a doctorate from a Roman university; he had served time as a Vatican official; and everyone whispered (rightly, it turns out) that he was on track to become a bishop.

Then I started to meet other Catholics--one a political philosopher, another a dentist--who showed the same qualities. The thing that most impressed me was that they both carried small Bibles in their pockets. At odd moments during the day, I might catch these men sitting in church reading the Scriptures. If I asked them to help me understand a point of doctrine, they would pull out the little book for backup. I thought to myself: These are men who read the life of Jesus Christ--and read it for all it's worth.

I mentioned to my priest friend that I had met a couple of guys who always carried the New Testament with them and who really seemed to know it.

He replied, "Oh, they must be Opus Dei."

Opus Dei: I knew enough Latin to know that it meant "the Work of God" or "God's Work." Almost immediately, when I heard Father's words, Opus Dei became for me a beacon--a lighthouse that promised the end of my long voyage, a first glimpse of a land I'd encountered only in books. It's not that the land was too small to be seen; nor was Opus Dei the whole of the land, for the Catholic Church is vaster than anything my denominational experience had prepared me for, and there were then (as there are now) so many other great institutions and movements in the Church. But for many reasons, Opus Dei was someplace where I could begin to feel at home.

What were those reasons?

• First and foremost was its members' apparent devotion to the Bible.

• Second was its warm ecumenism. Opus Dei was the first Catholic institution to welcome non-Catholics to cooperate in its apostolic labors.

• Third was how upright the lives of members were.

• Fourth was how ordinary their lives were. They were not theologians--they were dentists, engineers, journalists--but they were talking and living a theology I found attractive.

• Fifth, they espoused a holy ambition--a devout work ethic.

• Sixth, they practiced hospitality and gave their attention generously to my many questions.

• And seventh, they prayed. They made time for intimate prayer every day--true conversation with God. This gave them a serenity I had rarely encountered.

As I grew in my friendships with these men of Opus Dei, I came also to appreciate the rich biblical theology and biblical spirituality at the heart of their vocation. I took these as my own long before God gave me that same vocation--indeed, even before God brought me to the sacraments of the Catholic Church. I recognized immediately that they had tremendous potential for renewing my life, but also the life of Christ's Church, and the life of the world. This book is about Opus Dei's biblical theology and biblical spirituality.
Making Short Work of It

My favorite definition of Opus Dei is the one I found on a prayer card back in the mid-1980s. Opus Dei is "a way of sanctification in daily work and in the fulfillment of the Christian's ordinary duties." It's not just a method of prayer, or an institution in the Church, or a theological school. It's "a way," and that way is wide enough to accommodate everyone whose days are filled with honest work--at home with the kids, in a factory or an office, in the mines, on the farm, or on the battlefield. The way is also wide enough to accommodate free and varied expressions of prayer and theological style and method. God calls some people to commit their lives to this way as the faithful of Opus Dei, but many others take spiritual guidance from Opus Dei and from the books of its founder.

Briefly put: Opus Dei was founded in 1928 by a young Spanish priest, St. Josemaría Escrivá de Balaguer. For years before, he had received presentiments, indications in prayer, that God wanted something from him, but he had no idea what it would be. Then, rather suddenly one October day, as he was sitting down to read over some notes in his journal, he saw it. God showed St. Josemaria'a what He wanted him to do.

The founder rarely spoke about what he "saw" at that moment, but he always used the verb "to see," and he made it clear that he saw Opus Dei in its entirety, as it would unfold through the years. As one Vatican document put it: "It was not a pastoral project which took shape slowly, but rather a call which suddenly burst into the soul of the young priest." What did he see? Perhaps his sketchy private notes give us a glimpse of the vision: "Ordinary Christians. A fermenting mass. Ours is the ordinary, with naturalness. The means: professional work. All saints!" When only three young men showed up for his first formal activities, he gave them benediction with the Blessed Sacrament: "When I blessed those three . . . I saw three hundred, three hundred thousand, thirty million, three billion . . . white, black, yellow, of all the colors, all the combinations that human love can produce."

St. Josemaría saw that Jesus wants everyone to be a saint--everyone, without exception. Our Lord was speaking to the crowd, not to His inner circle, when He said, in the Sermon on the Mount: "Be perfect, just as your heavenly Father is perfect" (Matthew 5:48). That is the uncompromising Gospel, the good news that the apostles preached to the nations. St. Paul announced that God "chose us in Him, before the foundation of the world, to be holy and without blemish" (Ephesians 1:4). Moreover, God has made known His "plan" for us, "the mystery of His will." In the fullness of time--which is right now, today--we are "to restore all things in Christ" (Ephesians 1:10).

St. Josemaría taught that all human activity--political life, family life, social life, labor and leisure--should be restored to Christ, offered to God as a pleasing sacrifice, united with the sacrifice of the cross, united with the sacrifice of the Mass. He longed for a day when "in every place in the world there will be Christians with a dedication that is personal and totally free--Christians who will be other Christs."

St. Josemaría saw creation as a great cosmic liturgy, offered to the Father by those "other Christs" in union with Christ the high priest.
Priestly Soul, Lay Mentality

We can make this offering because we are a "royal priesthood, a holy nation" (1 Peter 2:9). We share in Christ's priesthood and kingship because, through baptism, we share in His nature (see 2 Peter 1:4). St. Josemaria'a urged Christians to have a "truly priestly soul and a fully lay mentality." This is not a contradiction. For, as both priests and kings, we have a vocation that is both sacred and secular. We share in Christ's kingship; we share in His priesthood. So we sanctify the temporal order and offer it to God, restore it "in Christ" because we live in Christ. We restore it, a little bit at a time, beginning with the inch or the yard or the mile over which we've been given dominion. Our workspace, our living space--these are where we exercise our dominion and our priesthood. Our altar is our desktop, our workstation, the row we hoe, the ditch we dig, the diaper we change, the pot we stir, the bed we share with our spouse. All of it is sanctified by our offering hands, which are Christ's own.

This doctrine is a particular emphasis of Opus Dei, but it is the property of the whole Church. The kingship and the priesthood, the rights and the duties, belong not just to a privileged few, not just to the ordained clergy, but to all baptized believers. Our special dignity is that, in baptism, we have become "God's children" (1 John 3:2)--we have joined "the assembly of the firstborn" (Hebrews 12:23). And if we are the firstborn, then we are the heirs (see Galatians 4:7), inheritors of Christ's kingship and the priesthood--the secular (which we are sanctifying) and the sacred. "Everything belongs to you . . .," said St. Paul, "and you to Christ, and Christ to God" (1 Corinthians 3:22-23).

We are God's children. The theological term for this fact is "divine filiation"--and this is the foundation of Opus Dei. It is the source of freedom, confidence, purpose, ardor, and joy for all Christians who live and labor. It is the "open secret" that enables men and women the world over to live out their vocation: to sanctify their work, to sanctify themselves through their work, and to sanctify others through their work.

This is rich fare, I know. Again, we'll spend the rest of the book examining these doctrines in greater detail.
Form Fitting

St. Josemaría spent the rest of his life preaching what God had revealed to him. At first he didn't even give it a name. His spiritual director suggested "Opus Dei," quite by accident, when one day he asked, "How is that work of God coming along?"

Gradually, the organizational details became clear to St. Josemaría, though the Church's canon law could not yet accommodate the institution as God had revealed it. St. Josemaría guided the development of the Work--cautiously, so that it never fell permanently into an inappropriate institutional form, even though it had to pass through a number of temporary, inadequate provisions. In 1965 the Second Vatican Council introduced the idea of a new form, a "personal prelature"--an institution having both lay and clergy members that could carry out specific apostolic tasks. The word personal means that the institution's leader, its prelate, has authority not over a territory (as an ordinary bishop does) but over a certain group or sort of persons, wherever those persons may be. In the case of Opus Dei, they are the "faithful" of the prelature--those who are called to make a permanent, personal dedication to this particular "way of sanctification." Whether married or celibate, they make their definitive commitment (which takes the form of a contract) when they make their "oblation," and they renew this commitment annually. At some point, they may recognize the permanence of their vocation more solemnly, by making "the fidelity"--essentially extending the term of the contract to the course of a lifetime.

St. Josemaría recognized the personal prelature as the perfect form for Opus Dei. He did not, however, live to see his family arrive in its home. He died in 1975. In 1982 Pope John Paul II established Opus Dei as the Church's first personal prelature. As I set pen to paper, there are around 85,000 members--again, "faithful" is the Church's preferred term--in the prelature of Opus Dei. The vast majority of them are ordinary laypeople. A small number are priests.
Extra Ordinary

The stories of Opus Dei's founding could give the wrong impression, and perhaps that's why St. Josemaria'a discussed them so infrequently. The founding of Opus Dei was the occasion of some documented miracles and extraordinary revelations. Yet the emphasis of Opus Dei is decidedly on ordinary life, ordinary work, and ordinary religious experience.

Perhaps the miracles were necessary because of the truly radical nature of God's plan for St. Josemaria'a. It was a plan that seemed out of step with the times in the early twentieth century, a time when Catholic leaders emphasized the dignity of the clergy almost to the exclusion of the ordinary baptized believer

</file>

<file= AmE06_D12>

The point, I should stress, is not to reject historicism but to identify its
limits. To do this, and to consider the consequences of this delimitation for
the study of religion, I attend here to recent developments in psychoanalytic
theory, particularly in the work of Zizek and Eric Santner. Zizek, while
noting the importance of historicist critiques of "'false' eternalization
and/or universalization," faults Marxist historicisms and their correlative
theories of ideology by arguing that the "most cunning ideological procedure
[is the] very opposite of eternalization: an over-rapid historicization"
(1994b: 327). This "precipitate historicization" (Zizek 1999: 72), he
argues, is ideological because it represses the fundamental trauma at the
heart of every human. In other words, for Lacanians such as Zizek and
Copjec historicism masks a groundlessness of society and self that
takes form in each person as "a surplus existence that cannot be caught
up in the positivity of the social" (Copjec 1994: 4). This "excess" of the subject is particularly evident in hysteria, which, as Copjec notes, "is conceived
by psychoanalysis as a challenge to the subject's social identity:
hysteria is the first analyzed instance of the subject's essential division, its
questioning and refusal of social dictates" (1994: 51). This is an essential
division between the socially dictated self and something "in excess" - 
Copjec's "surplus existence" or what Santner calls a "constitutive toomuchness"
of the psyche. These thinkers do not conceive this "too
muchness" as a core self that exists prior to socialization but as something
that comes to be in the production of the social self. Yet they do
conceive it as a key to breaking the grip of ideology, of the way sociolinguistic
structures and webs of meaning (what Lacan calls the "symbolic")
make possible our recognition of ourselves and thus construct us as subjects.
To describe this process, Santner borrows the term interpellation
from Althusser and Lacan. The question, then, is whether we can think
subjectivity beyond "interpellation," a subject that emerges, as Mladan
Dolar puts it, "where ideology fails."

In The Future of Theory, Jean-Michel Rabate suggests that we think
theory and critique through the lens of hysteria. For Lacan, as Rabate notes,
hysteria is not just a clinical condition but also "gives birth to a discourse
and maintains a quest for truth that always aims at pointing out the inadequacies
of official, serious, and 'masterful' knowledge" (Rabate 2002: 9).
For Zizek, ignoring these limits amounts to a form of "perversion" (1999:
247). Contrasting the discourse of the hysteric with the discourse of the
pervert, he notes that where the former obsessively questions, not knowing
what the other wants, the pervert speaks from a position that knows
and refuses to question. To hystericize the subject, then, is to free it from
the "closed loop of perversion" (Zizek 1999: 248). Historicism, I suggest,
as it is currently practiced in the study of religion, is caught in such a
closed loop insofar as it claims to know that "there is no release from this
historical," that our discourses are never interrupted even by traces of
something else, or, at the very least, that even if there is something more,
academic discourse has no business talking about it. This methodological
historicism has dramatic consequences for what it means to study religion
critically, for it produces a view of critique that is self-confirming
and, because it finds its terminus in the explanation of religion as "social
formation," it provides no resources for thinking about what we do with such explanation, with how we might use it. Taking this cue, I view my
work here as an effort to hystericize the study of religion, to traumatize
historicism in a way that, as Tomoko Masuzawa puts it, brings us
face to face with the ultimately hallucinatory status of the conceptual
devices which we use to manage and distance the past/other from
present/us - those devices in [Walter] Benjamin's own words, that
enable 'historicism' to turn the past into an indolent tale told by stringing
a sequence of events 'like the beads of a rosary' ". (Masuzawa 1993:
33)

One alternative to such uncritical historicism takes the form of Eric
Santner's "psychotheology of everyday life." Like Zizek and Copjec, Santner
distinguishes the "subject," as excess, from a socially constructed "identity."
But because he finds resources for this view not only in psychoanalysis
but also in the Jewish thought of Franz Rosenzweig, Santner is particularly
attuned to some of the implications of these views of ideology and
subjectivity for contemporary religious thought and the study of religion.
His "psychotheology" therefore might seem, especially from the perspective
of historicist scholars of religion, to bring us back to a situation where
religion and its study are "confused, conflated, confounded." In a way, it
does. But such a blurring of boundaries can contribute in a positive fashion
to a more self-conscious and theoretically rich vision of the study of religion.

PART II: THE REAL AS THE LIMIT OF HISTORICISM

As a Marxist and a materialist, Zizek believes that historicist analysis
is absolutely central to critical thought and radical politics. But historicism
that fails to recognize its own limits is "false despite the obvious
moment of truth it contains" (Zizek 2001a: 2), because it ignores the
excess of subjectivity by reducing human beings to the flat positivity of
the historical and social. Today, scholars of religion tend to defend such
moves by distinguishing between "metaphysical" and "methodological"
reductionism. The former, which such scholars eschew, takes the position
that human beings are - really, ultimately - only a function of their
social relations. The latter, by contrast, purports to make no final truth  claims about what human beings really are but employs historicist analysis
to examine the social and historical networks out of which human phenomena
emerge. This distinction can be useful in some contexts, but it
has its limits. Zizek marks these limits with his criticisms of Cultural
Studies, which, he says, is characterized by exclusive focus on histories of
discursive formations and a correlative "cognitive suspension," a refusal
to engage in questions about the "inherent 'truth-value' " of theories. The
problem with such historicism, he argues, is "that it continues to rely on
a set of silent (nonthematized) ontological and epistemological presuppositions
on the nature of human knowledge and reality" (Butler et al.
2000: 231). He acknowledges, though, that voicing these presuppositions
puts the theorist in a difficult position. How do we avoid, on the one hand,
the naïve appeal to social and historical "facts" that makes precipitate
historicism basically a variant of positivism or empiricism, and, on the
other hand, a full-blown metaphysics that denies or ignores the historical
overdetermination of all claims about "reality"?

Zizek's answer to this problem - perhaps surprising to those familiar
with his criticisms of Derrida - is to appeal to deconstruction "at its best"
(Butler et al. 2000: 231). For Zizek, deconstruction refuses the option of
naïve empiricism or metaphysical "theories of everything" by admitting
to being caught inbetween them: "true, it is impossible to adopt a philosophical
stance which is free of the constraints of everyday naïve lifeworld
attitudes and notions; however, although it is impossible, this philosophical
stance is simultaneously unavoidable" (Zizek 2000: 232). An "impossible"
discourse, then, deconstruction "oscillates" between historicist-type
analysis of philosophical and literary texts and "quasi-transcendental"
assertions about the role of "difference." In the context of such an oscillation
such assertions are not simply leaps of faith, made in the face of the
reality of historical specificity and location. Rather, they are attempts to
take such reality seriously by ontologizing it - ontologizing, that is, our
inability coherently to ontologize. As Zizek puts it, "the very feature which
forever prevents us from grasping our intended object (the fact that our
grasping is always refracted, 'mediated,' by a decentered otherness) is the
feature which connects us with the basic proto-ontological structure of
the universe" (2000: 232).

Let me explore this "proto-ontology" in a bit more detail. Zizek states
that "the fact that we cannot ever 'fully know' reality is not a sign of the
limitation of our knowledge, but the sign that reality itself is 'incomplete,'
open, an actualization of the underlying virtual process of becoming"
(Schwartz 2004: ix). Hence, his proto-ontology focuses on a process rather
than a structure of being because, he claims, there is no "ontologically fullyconstituted
cosmos" (Zizek 2001b: 174). This is, as Zizek frequently reminds us, a Hegelian approach to ontology, albeit one that radically rethinks received notions of Hegelian teleology usually suggested by the idea of
"absolute knowledge" (1999: 84-6). Like Kant, Zizek will affirm that what
we can know is limited to phenomena constituted in and through our
knowing activity but, against Kant and with Hegel, Zizek argues that this
does not leave us with a distinction between reality as we are able to grasp
it and "external" reality "in itself." The properly Hegelian move, according
to Zizek, is to reject the contrast between our finite conceptual capacities
and the "hard kernel" of reality and to conceive the limits of our
knowledge as inherent to the "symbolic" order (again, the Lacanian term
for the realm of language and discourse by which human beings represent
and grasp "reality"). Hegel's "absolute," from this point of view, is
not something "above and beyond the domain of our finite reflected reasoning"
but is "nothing but the movement of self-sublation of these finite
determinations" (Zizek 1999: 84). In short, Zizek moves from a "reality"
posited as external to and underlying our diverse, historically specific representations
of reality, to the Lacanian "Real" understood as the gap
between different "finite determinations." "Multiple perspectival inconsistencies
between phenomena are not an effect of the impact of the
transcendent [Real] - on the contrary, the [Real] is nothing but the
ontologization of the inconsistency between phenomena" (Zizek 2004: 167).

In sum, the short answer to the question of the limits of historicism
is, for Zizek, the "Real" - the "a priori of historicity itself" (Butler et al.
2000: 310). The longer answer brings us back to deconstruction, specifically
to what I will call Zizek's "deconstructive materialism." Historicist
reductionism, as we generally find it in cultural studies and in the study
of religion today, is basically a form of materialism, holding that human
beings and their practices, ideas, and social relations are caused by historically
specific material forces. But this stance is -  - as Zizek reminds us and
as all good historicists will concur -  - like all human efforts to conceptualize
and articulate the nature of things, thoroughly embedded in language
and history. That is, the conceptualization of the relation between the
material and the ideal is itself ideal: it depends on those processes of identification
and differentiation that constitute the ideal, or, more generally,
the Lacanian "symbolic" in the first place. So all materialisms entail claims made from within the symbolic about that which is external to the symbolic.
And to the extent that any given materialism does not acknowledge
this limit it is, Zizek notes, again in good historicist fashion, "obscurantist."
That is, it mystifies the relation between thought and "reality" by
ignoring the fact that the very distinction between the ideal and the material
(or the symbolic and the real) is, as Zizek put is it, the "symbolic gesture
par excellence" (2003: 69).
Zizek's own materialism acknowledges these limits in deconstructive
fashion because it makes the basic materialist move of reversing the privilege
assigned to the ideal only as it also disrupts the binary opposition
between the ideal and the material.8 Thus he rejects the equivalence
between "materiality" and the so-called external reality understood as an
ontologically complete set of discrete entities - things in themselves - 
that cause our ideal representations of reality. Instead, Zizek conceives
"materiality" as the "Real," as an "ex-timate remainder" within the symbolic
yet always also resisting it. The Real is the internal limit or "failure"
of the symbolic, "an impenetrable/dense stain within the 'ideal' sphere of
psychic life itself" (Butler et al. 2000: 118 - 120). It is, in other words,
within the symbolic but in the modality of being in excess of it. Or, we
might say, Zizek's fundamental ontological category is the Real as excess,
as something neither simply internal nor external to the symbolic, neither,
as I will discuss below, simply in or out of history.

</file>

<file= AmE06_D13>

Sunnis historically have afflicted the weaker Shiites, accusing them of shaping a blasphemous cult around Ali and Hussein. At the Karbalaa Islamic Education Center in Dearborn, Michigan, a large mural depicts mourning women who have encountered the riderless horse of Hussein after his final battle. "You see our history and our situation in this," says Imam Husham al-Husainy, a Shiite Iraqi émigré who leads the center. In Dearborn, Shiite Iraqis initially backed the American invasion to depose Saddam Hussein, who persecuted Iraq's Shiite majority. Most Sunnis in Dearborn condemned the war as an exercise in American imperialism. 
  
            Sufism, another important strain of Islam, is also present in the United States. Sufis follow a spiritual, inward-looking path. Only a tiny percentage of American Muslims would identify themselves primarily as Sufis, in part because some more rigid Muslims condemn Sufism as heretical. But Sufi ideas crop up among the beliefs of many Muslims without being labeled as such. Sufism's emphasis on self-purification appeals to New Age seekers and has made it the most common avenue into Islam for white American converts such as Abdul Kabir Krambo of Yuba City, California. Krambo, an electrician who grew up in a conservative German Catholic family, helped build a mosque amidst the fruit arbors of the Sacramento Valley, only to see it burn down in a mysterious arson. Once rebuilt, the Islamic Center of Yuba City was engulfed again, this time by controversy over whether Krambo and his Sufi friends were trying to impose a "cult" on other worshipers. 
  
            Although there is a broad consensus that Islam is the fastest-growing religion in the country and the world, no one has provable numbers on just how many American Muslims there are. The Census Bureau doesn't count by religion, and private surveys of the Muslim population offer widely disparate conclusions. A study of four hundred mosques nationwide estimated that there are two million people in the United States "associated with" Islamic houses of worship. The authors of the survey, published in 2001 under the auspices of the Council on American-Islamic Relations (CAIR), a Muslim advocacy group, employed a common assumption that only one in three American Muslims associates with a mosque. In CAIR's view, that suggests there are at least six million Muslims in the country. (Perhaps not coincidentally the American Jewish population is estimated to be slightly below six million.) Other Muslim groups put the number higher, seeking to maximize the size and influence of their constituency. 
  
            Surveys conducted by non-Muslims have produced much lower estimates, some in the neighborhood of only two million or three million. These findings elicit anger from Muslim leaders, who claim that many immigrant and poor black Muslims are overlooked. On the basis of all the evidence, a very crude range of three million to six million seems reasonable. Rapid growth of the Muslim population is expected to continue, fueled mainly by immigration and high birthrates and, to a lesser extent, by conversion, overwhelmingly by African-Americans. In the next decade or two there probably will be more Muslims in the United States than Jews. Worldwide, the Muslim head count is estimated at 1.3 billion, second among religions only to the combined membership of Christian denominations. 
  
            American Muslims, like Americans generally, live mostly in cities and suburbs. Large concentrations are found in New York, Detroit, Chicago, and Los Angeles. But they also turn up in the Appalachian foothills and rural Idaho - the sites of two of the stories that follow - among other surprising places. Often the presence of several hundred Muslims in an out-of-the-way town can be explained by proximity to a large state university. Many of these schools have recruited foreign graduate students, including Muslims, since the 1960s. In the 1980s Washington doled out scholarships to Arab students as part of a campaign to counter the influence of the 1979 Iranian Revolution. Some of the Muslim beneficiaries have stayed and raised families. 
  
            In New York, Muslims are typecast as cab drivers; in Detroit, as owners of grocery stores and gas stations. The overall economic reality is very different. Surveys show that the majority of American Muslims are employed in technical, white-collar, and professional fields. These include information technology, corporate management, medicine, and education. An astounding 59 percent of Muslim adults in the United States have college degrees. That compares with only 27 percent of all American adults. Four out of five Muslim workers earn at least twenty-five thousand dollars a year; more than half earn fifty thousand or more. A 2004 survey by a University of Kentucky researcher found that median family income among Muslims is sixty thousand dollars a year; the national median is fifty thousand. Most Muslims own stock or mutual funds, either directly or through retirement plans. Four out of five are registered to vote. 
  
            Relative prosperity, high levels of education, and political participation are indications of a minority population successfully integrating into the larger society. By comparison, immigrant Muslims in countries such as Britain, France, Holland, and Spain have remained poorer, less well educated, and socially marginalized. Western European Muslim populations are much larger in percentage terms. Nearly 10 percent of French residents are Muslim; in the United Kingdom the figure is 3 percent. In the more populous United States the Muslim share is 1 to 2 percent, depending on which Muslim population estimate one assumes. It's unlikely that American cities will see the sort of densely packed, volatile Muslim slums that have cropped up on the outskirts of Paris, for example. 
  
            America's social safety net is stingy compared with those of Western Europe, but there is greater opportunity for new arrivals to get ahead in material terms. This may attract to the United States more ambitious immigrants willing to adjust to the customs of their new home and eager to acquire education that leads to better jobs. More generous welfare benefits in Europe allow Muslims and other immigrants to live indefinitely on the periphery of society, without steady jobs or social interaction with the majority. Europeans, who for decades encouraged Muslim immigration as a source of menial labor, have shown overt hostility toward the outsiders and little inclination to embrace them as full-fledged citizens. Partly as a result, violent Islamic extremism has found fertile ground in Western Europe. 
  
            The July 7, 2005, bus and underground train bombings in London, which roughly coincided with the end of my reporting for this book, were in some ways as unsettling as the 9/11 attacks that sparked my research. The London bombers weren't strange non-Westerners raised in insular Islamic lands. They were Muslims born and bred in Britain yet willing to murder their countrymen. This undeniable confirmation of a threat from within Islam in the West demands reflection on both sides of the Atlantic. 
  
  
There are an estimated thirteen hundred mosques in America and several hundred Islamic religious schools. These institutions vary in religious approach and political ideology. But on the whole, Muslim houses of worship tend to be highly conservative compared with the larger culture. Like Orthodox Jews, almost all Muslim congregations separate the sexes during prayer and generally consign women to subordinate roles in communal rituals and social activity. Most observant Muslims view the Quran as the literal word of God, not a work of divine inspiration composed by humans, as most observant Jews and Christians describe their scriptures. Many Muslim preachers dwell on ideals that would be familiar to members of other faiths, such as treating the neighbors as you would have them treat you. 
  
            Some Muslim preachers, sad to say, give sermons condemning nonbelievers, but these messages are not that different in their essential theme from those delivered in some fundamentalist Christian churches. Similarly, surveys show that most American Muslims hold disapproving opinions about aspects of American society that parallel the views of conservative Christians. Two-thirds of American Muslims consider the United States "immoral" because of permissive attitudes toward sex outside marriage and toward alcohol, both of which Islam bans. Most Muslims also frown on accommodating homosexuality and permitting abortion. They favor outlawing pornography and allowing public funding for religious schools. 
  
            Islamic fundamentalism surged in the Middle East and South Asia in the 1970s, at roughly the same time that Christian fundamentalism became more prevalent in the United States. In Israel the Likud Party rose to power in the 1970s, asserting a biblical mandate to include the West Bank in the Jewish state. A mix of social, political, and economic stimuli caused each of these religious awakenings, but it is worth noting that Islam has not been alone in witnessing a powerful welling up of fundamentalism. 
  
            If there is one source of influence that bears special responsibility for exporting the Muslim world's worst ideas to the West, it is our equivocal ally Saudi Arabia. The kingdom that occupies the birthplace of Islam promotes a puritanical version of the religion called Wahhabism, named for Muhammad ibn Abd al-Wahhab, an eighteenth-century evangelist in Arabia. Wahhab sought to cleanse Islam of corruptions he believed had caused Muslim fortunes to decline. Today an updated version of Wahhabism sometimes goes by other names (the Saudis take offense at the label). But Wahhabi ideas, including hostility to non-Muslims and more moderate Muslims, have persisted and fused with other strains of fundamentalism. One such strain, Salafism, the name of which refers to Muhammad's original righteous companions, seeks to reestablish a dominant Islamic empire based on the pure religion of the Prophet's era. In a misguided attempt to promote their Islamic legitimacy, the Saudi petroleum princes and their charities have financed the propagation of Wahhabi and Salafi radicalism around the world. In the United States, a Saudi-underwritten construction boom has produced scores of mosques and Islamic centers. They can be found in New York, Los Angeles, Washington, Chicago, Houston, Denver, San Francisco, Toledo, Tucson - the list goes on and on. One Saudi charity official based in the United States estimated in October 2001 that fully half the mosques and Islamic schools in the country had received Saudi money. Saudi religious organizations have funded the training of hundreds and maybe thousands of Muslim clerics and teachers sent to America. And Saudi publishers inundate American mosques with books and pamphlets. Inevitably, Wahhabi- and Salafi-influenced fundamentalism* has colored the thinking of some American Muslims. The breadth and degree of this influence are hotly disputed. 
  
            What to do about American Muslims who advocate extreme versions of Islam has become an urgent question of law and social policy in the wake of 9/11. Sami Omar al-Hussayen was a popular graduate student of computer science at the University of Idaho, admired for leading fellow Muslims in mourning the victims of 9/11. But in February 2003 the FBI arrested the Saudi immigrant and charged him with supporting terrorism in connection with his role as webmaster for a Michigan-based group that disseminated extremist views, including murderous anti-Semitism and support for suicide bombing. An Idaho jury had to decide whether al-Hussayen had provided "material support" for terrorism or had been wrongly prosecuted for practicing free speech. 
  
            The alarming opinions that al-Hussayen helped spread - especially theological justifications for violence - have divided many American Muslim communities. The CAIR survey reported that 58 percent of American mosques acknowledged some internal conflict over religious issues, with 18 percent admitting that the turmoil was "moderate or very serious." Khaled Abou El Fadl, a reform-minded scholar of the Quran, has sparked as much of this discord as any other American Muslim. The Egyptian-American law professor at UCLA has become a hero to progressive members of his faith - and a reviled traitor in the eyes of many others - by reinterpreting Islamic scripture to justify equality of the sexes, tolerance of other religions, and opposition to religiously inspired bloodshed. Across the country in Morgantown, West Virginia, Asra Nomani has put Abou El Fadl's principles into action. Nomani, who arrived from India as a small child, has riled the mosque her father helped found by demanding the right to pray in the same space as men. The clash in Morgantown ignited similar debates across the country, and as they spread, far more scandalous questions have arisen: Can women ever lead a mixed congregation in prayer?

</file>

<file= AmE06_D14>

In Wyeth's scheme, Snakes, Diggers, and Pawnacks could at times be
found in the same vicinity, often, indeed, visiting and traveling together.
Moreover, both Snakes and Diggers were fishing people. The Diggers,
however, were found in small, scattered encampments rather than at the
major fisheries. Taken together, early accounts, such as Ross's and
Wyeth's, and modern ethnographic resources suggest the general social
and economic divisions apparent in the Newe world by the fur-trade era.
To borrow Wyeth's nomenclature, the Snakes were the large Shoshonean
populations along the middle and lower Snake. They owned horses and
periodically joined in the buffalo hunt, but mostly they were fishing people
who lived in relatively sedentary villages among the greatest fisheries
in all the Newe country, Sehewooki' and Salmon Falls. The Diggers were
the multitude of small, foot-going Newe groups and family clusters that
continued to pursue a mixed subsistence strategy in more marginal environments
north and south of the Snake River. Finally, the Pawnacks were
most likely the mixed buffalo-hunting bands of the upper Snake River
Plain, who in their annual cycle regularly visited and fished with their
downstream friends and kin.

Despite such visible distinctions, these peoples were not separate
tribes. They maintained close ties with one another, and indeed all were
part of the greater "Snake Nation." That interrelationship was evident
during McKenzie's winter encampment with the Newes near Bear Lake
in 1819. People from all three Newe divisions were in a camp that reputedly
stretched for seven miles. Taking advantage of the opportunity,
McKenzie sought to negotiate peace between the Newes and the Nez
Percés, his two principal trading partners, who were so often in conflict.
The Sherry-dikas dominated both the camp and the council. Along with
the War-are-ree-kas, they spoke in favor of peace and blamed the Ban-attees
for attacks on both the Nez Percés and the whites. The latter people
were eventually brought over to the council and told they must be good,
and the "poor, trembling . . . Ban-at-tees" agreed. None of these groups
represented a separate tribe, but it is also evident that there was an unequal
power relationship at work. This might have been due to the location
of the Bear Lake encampment within the Sherry-dikas' tebíwa, or
homeland. It is also possible that because the Sherry-dikas had developed
a greater band organization and their leaders spoke for more people.
Finally, because they often traveled with the mounted groups and relied
upon them for protection, the trappers generally looked upon the equestrian
people as dominant and ascribed greater power to them.
The principal reason for the greater organization found among the
mounted groups, and for their seemingly dominant status in the Newe
world, was the continuing Blackfoot War. The demands of traveling and
hunting in dangerous territory placed a premium on greater social integration
among the mounted buffalo hunters of the upper Snake River
Plain, who lived closest to the threat. By the 1820s, Blackfoot war parties
regularly ventured deep into the heart of the Snake River country, as far
west as the Great Camas Prairie; and the equestrian Newes, with long
experience in warfare, most often battled the intruders. The men who led
the fur brigades knew the danger well. In 1824, Ross feared the
"Blackfeet and Piegan war roads [that] were everywhere in our way."
But he was lucky and encountered only two sizable Blackfeet camps,
both of which professed peaceful intentions. Ross's successor, John
Work, cautiously tried to keep the Newes between his party and the
Blackfeet. In September 1830, when he learned that the "great Snake
camp" had already left for the buffalo hunt, Work wrote, "This is of
advantage to us as they will be before us and amuse the Blackfeet." Work
spent the harsh winter that followed near the future site of Fort Hall,
constantly fretting over the Blackfeet and noting the passage of Newe
bands downstream, which left his brigade exposed to attack.
The Blackfeet may have been the terrors of the upper Snake River
Plain, but they were not always successful in the brutal fighting. John
Kirk Townsend, a naturalist who accompanied Wyeth's second expedition,
recounted an intense battle on the upper reaches of the Big Lost
River in August 1834. Caught on foot on the open plain, "the Blackfeet
were run down with horses, and, without being able to load their guns,
were trampled to death, or killed with salmon spears and axes." These
unfortunate Blackfeet were probably a horse-raiding party intercepted
short of their goal. Still, the Blackfoot risk had grown so great that formerly
safe havens, such as the Lemhi and Salmon valleys, had become
dangerous "war roads."

One of the consequences of the Blackfoot presence, then, was everlarger
social amalgamations. These were evident as early as 1805, when
the Lemhi Valley Newe joined forces with the Salish to hunt in the Three
Forks country. By the summer of 1819, when McKenzie's Northwest
Company brigade fell in with a "friendly band of Snakes" as they traveled
east along the Snake River, large defensive groups were necessary
even in the heart of Newe country. The fur brigades that worked the
northern Rockies commonly traveled with Nez Percé and Salish allies,
but they also sometimes joined forces with the Newes. The group camping
with McKenzie engaged the Blackfeet in a "severe battle" somewhere
on the middle Snake Plain, and when the victorious Newes returned,
Snakes appeared from every direction to join in a raucous celebration.
Ross wrote: "They came in crowds from their hiding places and joining
the victorious party in their scalp dancing and scalp singing, formed a
host of at least five or six thousand. Their huts, their tents altogether
resembled a city in an uproar, and their scattered fires exhibited rather an
awful spectacle." Although the Newe numbers were probably exaggerated,
the reference to both huts and tents suggests the presence of both
foot-going and mounted Newe, as does the suggestion of more timid
Newes emerging from hiding places. It was late summer, and McKenzie's
party was undoubtedly in the company of mounted Newes returning to
the upper Snake River Plain after their annual trip to the fisheries of the
middle and lower Snake. The foot-going people were at a decided disadvantage in warfare, and the mounted groups exhibited an increasing
Plains influence, including the warrior complex. Indeed, Ross reported
that the more sedentary fishing groups suffered most at the hands of the
Blackfeet, while the mounted buffalo hunters held their own in combat.
Notwithstanding their differences, all of these groups were of the same
"nation," and they came together in large groups first and foremost for
defense
With larger social groups came important shifts in the power of Newe
headmen. Bands formed around successful leaders of important cooperative
endeavors. Liljeblad referred to these leaders as "bosses," although
"talkers" is probably more accurate. For instance, the baingwi-dai'-
gwahni' directed cooperative labor at the fisheries. The dai'gwahni' was
literally "someone who coordinates or conducts a service for the people."
He was an orator and mediator who smoothed over difficulties
among his own people and with other bands. The dai'gwahnee'
increased in importance as the mounted bands traveled ever farther to
access the buffalo. The "band chiefs" whom European Americans
encountered sprang from this political tradition. First and foremost, they
had to be successful leaders of subsistence endeavors. Second, as the
Blackfeet onslaught continued, larger groups coalesced around proven
war leaders. Often these men possessed supernatural sanction for their
abilities. Finally, by the mid - nineteenth century, the ability to act as
effective intermediaries with European Americans became preeminent.
Thus, the dai'gwahnee', men already skilled as intermediaries, most often
emerged as the "chiefs." Among the buffalo-hunting bands, the dai'-
gwahnee' who spoke for the largest groups often fulfilled this role. The
most successful band leaders balanced the needs and desires of their people
against the demands of white society and, most important, negotiated
their people's access to the land and its resources
Still, the power of these leaders should not be overstated. In such a
fluid social world, political leadership developed on grounds that European
Americans could hardly fathom. Newe headmen lacked the coercive
power that white observers were accustomed to in leaders. Individual
choice rather than stable allegiance marked Newe politics, and only
the most capable leaders retained a following. In 1805 Meriwether Lewis
wrote: "Each individual is his own sovereign master . . . the authority of
the Cheif [sic] being nothing more than mere admonition supported by
the influence which the propriety of his own exemplary conduct may
have acquired him in the minds of the individuals who compose the
band. . . . In fact every man is a chief, but all have not an equal influence
on the minds of the other members of the community, and he who happens
to enjoy the greatest share of confidence is the principal Chief."
During McKenzie's peace council in the winter of 1819, the combined
Snakes were led by two brothers, Pee-eye-em and Ama-qui-em, who,
according to Alexander Ross, exercised remarkable control over their
people. Perhaps their size had something to do with their power. Both
men were of great stature, with Pee-eye-em reported to be much larger
than the 312-pound McKenzie! But, clearly, their authority stemmed
from their roles as successful dai'gwahnee'. Ross wrote: "Trade was no
sooner over than Ama-qui-em mounted one of his horses, rode around
and round the camp, and that itself was almost the work of a day, now
and then making a halt to harangue the Indians, remind them of the
peace, their behavior towards the whites, and to prepare them for raising
the camp.
In 1848 Nathaniel Wyeth remarked on the Newes' "almost entire
absence of social organization," except at "salmon time" and during the
buffalo hunt, when "some person called a chief usually opens a trade or
talk, and occasionally gives directions." Wyeth's original journal entries
presented an image of greater respect and influence but still suggested
that ability could only draw followers, not compel them. Writing of the
great fishery at Salmon Falls and the Newe leader in charge, he recorded:
"This chief is a good sized man and very intelligent and the President
would do well if he could preserve the respect of his subjects as well or
maintain as much dignity."

The most famous Newe headman of the fur trade era, the Horn Chief,
embodied all of the talents and characteristics necessary for effective
leadership. The Horn Chief's role as dai'gwahni' was intertwined with
his ability as a war leader. Living close to the buffalo herds on the upper
Snake River Plain meant living in constant conflict with the Blackfeet.
The Horn Chief, whom white fur hunters considered the "principal chief
of the Snakes," was one of the most influential leaders because of his
bold and successful tactics. Once, the story was told, while traveling
alone across the open plain and armed only with a spear, he happened on
a Blackfoot war party. Rather than flee, he charged into the enemy and
killed six in hand-to-hand combat. "He always rushed headlong upon his
enemies without fear of death," wrote the trapper Warren Angus Ferris,
"and rendered himself so terrible . . . that his presence alone was often
sufficient to put them to flight." In addition to his prowess as a warrior,
the Horn Chief also possessed a supernatural sanction for his leadership.
He had survived countless battles, and it was well known among the
Newes that his tutelary spirit had rendered him invulnerable. Whites, of
course, were usually incredulous, branding him superstitious, and
lucky. According to Ferris, the Horn Chief believed that "the moon was
his guardian deity, this extraordinary Indian imagined that she instructed
him in dreams during his sleep; and he taught his followers to believe
that he never acted but in obedience to her directions, and that he could
not be killed by metal." Such beliefs were deeply rooted in the spiritualism
of the Basin and Plateau. Newe peoples believed that tutelary spirits
transmitted sacred knowledge and power, bo'ha, through dreams.

</file>

<file= AmE06_D15>

Even as Dallas evolved into a modern city, the Department of Public Health still saw the human population as parceled into cleanly divisible racial segments, using identification cards in the 1940s to loudly announce that Jews did not belong to the white race. Rabbi David Lefkowitz of Dallas' Temple Emanu-El fought to abolish the department's "Hebrew" racial classification. To Lefkowitz, a Reform rabbi, Jews represented a religious group, not a racial category. 
In 1942 Lefkowitz protested when the health department included Hebrew as a race along with Anglo-Saxon, South European, Mexican, Negro, and Asiatic on its documents. "The use of the word 'Hebrew,' under any circumstances, except as the designation of the original language of the Bible, is incorrect," Lefkowitz wrote. "The designation 'Jewish' is a proper one for religion . . . You are not, of course, seeking to determine the religion of those to whom you distribute the identification cards, otherwise you would put down Episcopalian, Baptist, Catholic, Methodists, etc. In this group, the word Jewish could well be included, but not in the former." 
The unfolding drama of the Nazi Holocaust provided a deadly example to American Jews of the dangers of living in Gentile-majority countries where their neighbors viewed them as not just religious but racial outsiders. If anti-Semitism in early-twentieth-century America rose in part from religious intolerance, the Leo Frank murder case and American immigration policy strongly demonstrated that even in relatively safe America, Jews could be viewed by ruling Anglo-Saxons as racial poison. In the 1920s the U.S. government imposed immigration quotas aimed at limiting the number of Jews and other groups who could enter in a given year, abandoning later would-be Jewish immigrants to the mercies of the Third Reich in the 1930s and 1940s. Lefkowitz and others happily received the news in 1944 that U.S. immigration authorities no longer considered Jews as racially different from northern Europeans, a move that might allow more Jews to enter the United States. As they celebrated their newfound whiteness, however, many Jews still saw blacks, Asians, and other groups as belonging to separate racial categories. "In our country statistics of Jews must not be collected from the point of view that we are a race or ethnic group, in the manner of the American races, Negroes, Chinese, Indians, etc.," H. S. Linfield, director of the Jewish Statistical Bureau, informed Aline Rutland, secretary for Temple Emanu-El, in a letter dated March 7, 1944. Until recently, "the government continued to regard incoming alien Jews as constituting a separate race. The new order has put an end to this practice." 
The black civil rights movement and convulsive demographic changes from the 1940s through the 1960s made the issue of racial identity crucial to Jews and other marginal whites. The black freedom struggle in those three decades and the development of a new class of what historian George Norris Green called "little rich" merchants ignited the most intense period of anti-Semitism in Dallas since the heyday of the Klan. These factors placed additional pressure on Dallas Jews to demonstrate their whiteness, thus troubling the relationship between Jews and African Americans. 
A similar pressure shaped the racial and political attitudes of Mexican Americans. For that community, life in the 1950s and 1960s became a race to escape the bottom of the social ladder. Many Mexican Americans, locked in an uncertain civil rights struggle of their own, felt they had nothing to gain by helping the African American community. These Mexican Americans instead battled for a white identity, but this goal proved elusive. Jews and Latinos remained marginalized even as they strained relations with African Americans, ultimately limiting the gains made by black activists. 
. . . Dallas had come a long way toward accepting Jews as part of the racial ruling class, but many still struggled with the meaning of Jewish identity . . . [P]hilo-Semitism existed side by side with anti-Semitism. Jews like Lefkowitz had changed the Dallas health department's racial classification schemes but not the Gentile perception of the community. Even after the discovery of the Holocaust at the end of World War II, some Gentiles saw Jews as nonwhite and found biblical justification for their exclusion. The Dallas Independent School District created an Old Testament course in 1952. In the course materials, Judaism was depicted as a half-baked religion awaiting Christ's arrival for its completion. "While there are sixty-six books in this one volume [the Bible], they are unified in the person of Christ whose coming was prophesied in the first book of Old Testament," declared a bulletin outlining the course. "As you study the lives of these Hebrew people, you will be conscious of expectancy which existed throughout the Old Testament period and which had its fulfillment in Jesus Christ." 
Jews possessed only half a loaf regarding divine truth, according to the course, and they were not even part of the white race. According to the notes for the lesson titled "The Origin of the Races, the Tower of Babel, and the Confusion of Tongues," all mankind descended from the three sons of Noah following a flood that wiped out the rest of humanity. Japheth, according to the lecture, was the father of the Europeans. The Old Testament course then reinforced the Southern white Protestant rationale for African American subordination by claiming all "colored races" were descended from Ham, whose family line was forever cursed by God after the flood, according to Genesis, to be a "servant of servants." According to the DISD, "The children of Shem, inhabiting the land of Arabia, and southeastern Asia, [included] . . . the Hebrews, Arabians, Assyrians and Persians, all of whom speak the Semitic languages." Jews thus yellowed as Asians in the eyes of the DISD.5 This classification followed soon after an American war with Japan in which U.S. troops at times fought with a genocidal fury unmatched by their peers in Europe. The depiction of Jews as Asians came amid widespread concerns over the "loss" of mainland China to the communists in 1949 and during the 1950-1953 Korean War, whose outcome was uncertain at the time the DISD launched the course. This racial assignment not only symbolically darkened Jews but also threatened to render them part of a dangerous yellow horde in Gentile eyes. Alarmed by recent political convulsions within the city, some Gentiles demanded that Jews come down clearly on one side of the black/white divide. 
The issue of Jewish whiteness came to a head just as the black civil rights movement in Texas picked up new momentum during and after World War II. In the 1940s Juanita Craft won appointment as the Dallas NAACP's membership chair. In 1944 she became the first black woman in Dallas County history to vote, and in 1946 the NAACP named her a field organizer. Due to Craft's tireless proselytizing, the Dallas NAACP branch claimed 7,000 members by 1946 as it became the epicenter of the state's civil rights movement. In 1941 blacks served on juries in Dallas County for the first time since the 1890s. In 1943 the NAACP won a lawsuit on behalf of Thelma Paige and the Negro Teachers Alliance of Dallas, achieving gradual equalization of teacher salaries. A. Maceo Smith's efforts to end the white primary system finally paid off with the NAACP prevailing in the 1944 Smith v. Allwright decision. 
Divisions within the African American community over political priorities, however, sometimes complicated the civil rights struggle. Some African Americans felt ambivalent about a future of integration. A 1947 statewide poll of African Americans showed that a clear majority favored the creation of a separate black university over the integration of the University of Texas. "Some Negroes had, or at least believed that they had, a vested interest in retaining segregation," observed Michael Gillette, an historian of Texas' NAACP. "These were often professionals, such as teachers, who feared that they would lose their jobs to whites if desegregation occurred. Thus . . . there existed, 'many, many Negroes who are deathly afraid of the elimination of segregation.'" 
Other African Americans internalized the lessons of black inferiority pounded home daily by segregation and the mainstream culture. Even the Dallas Express, a vigorous supporter of political activism and pride in black culture, at times conveyed negative messages about blackness. Advertisements for hair straighteners and skin bleach abounded in the pages of the Express, which carried the message that kinky hair and dark skin were unattractive social liabilities. "Enjoy the Light Side of Life with new, improved 'Skin Success' Bleach Cream," one ad beckons. "Now you can enjoy the popularity and admiration that goes with a lighter, fairer complexion." 
Black assertion, however, alarmed some marginal whites, such as Jews and darker-skinned or working-class and middle-class Mexican Americans. Fate granted some men like Pete Garcia both a Spanish surname and a light skin. In 1950s Dallas society, the middle-class Garcia rated as a higher grade of human than did his black neighbors, tantalizingly close to the Caucasian status he so desired. Aware that his Mexican ethnicity marked him as an outsider, Garcia sought ethnic promotion. In 1950 and 1951 Garcia was part of a small army that dynamited a South Dallas neighborhood for about eighteen months to preserve the boundary between black and white. The 1950s bombings echoed the 1940 terrorism against socially mobile blacks. Dallas culture taught Garcia to see the world in terms of a zero-sum game. African American gains could only mean loss for Mexican Americans while oppression of his black neighbors provided a quick route to whiteness. 
The city's World War II population boom aggravated an already disastrous black housing situation. From 1940 to 1950 Dallas' population grew by about a third, from 294,734 to 434,462. Dallas' black population grew by 30,000 in that time, but private builders constructed only 1,000 new dwellings open to African Americans. White residents protested construction of a proposed 2,000-home tract south of Dallas' city limits set aside for blacks. The City Council promptly refused to supply water to the development, killing the project. A 1950 "Report on Negro Housing Market Data" found 21,568 black households occupying 14,850 housing units. This meant that one of three black families shared crowded housing with other families, and even those substandard structures skyrocketed in price. 
By 1948 a nine-square-mile community of 25,000 blacks, Mexican Americans, and poor whites lived on a low flood plain in West Dallas. Created by the earlier construction of levees along the Trinity River, West Dallas consisted of "flimsy shacks, abandoned gravel pits, garbage dumps, open toilets and shallow wells." Fewer than 10 percent of those dwellings had indoor toilets, and only 15 percent had running water. Tenants drank from wells located near human waste disposals. West Dallas accounted for 50 percent of the city's typhus cases, 60 percent of the tuberculosis, and 30 percent of the polio. 
Desperation forced relatively prosperous blacks to again venture in the early 1950s into the Exline Park neighborhood, scene of the 1940\-1941 bombings. Twelve bombings in the next year and a half targeted homes sold to blacks in formerly all-white neighborhoods in a two-square-mile area of South Dallas. Not expecting white protection, African Americans armed themselves. Juanita Craft noted in a letter to Walter White, the executive director of the NAACP, that bombing stopped on Crozier Street when "the widow Sharpe" ran from her home firing a gun at a speeding getaway car after one explosion. Fearful that violence threatened the city's postwar economic boom, elites could not ignore these bombings as they had the 1940 attacks. A special grand jury that included several prominent Dallasites, such as wholesale liquor distributor Julius Schepps and Dallas Morning News managing editor Felix McKnight, investigated the bombings. In an unusual move for 1951 Dallas, the grand jury also numbered three African Americans as members, including NAACP chapter president Bezeleel R. Riley and W. J. Durham, an attorney on the Sweatt v. Painter case that led to desegregation of the University of Texas law school.

</file>

<file= AmE06_D16>

It's the End of the World
(And I Feel Fine)
The second in a series for Advent and Christmas
THE APOCALYPTIC literature
of the Bible, which includes
most notably Daniel and the
Book of Revelation, exists in
the popular consciousness as a sort of
hitchhiker's guide to the end times, chockfull
of predictions of the historical events
that will lead to the end of human history.
Given the highly metaphorical
language the writers use, the
specifics can be difficult to pin
down, but that ambiguity only
feeds speculation. On various
Web sites contributors weigh
whether Revelation 13's "Beast,"
frequently referred to as the
Antichrist, might be Vladmir
Putin (revelation13.net), George
W. Bush (seattleweekly.com) or
Barack Obama (www.
unfogged.com). On raptureready.
com, one can find endtimes
resources like: the
Prophetic Top 10 - current
events that suggest the imminent
end of the world; "Left Behind
Letters" supposedly penned by
those who anticipate being taken
up in the Rapture to help those
who will be left behind; and chat
rooms in which participants are
discussing, among other things,
whether the Gog/Magog war
mentioned in Revelation will
come before the Rapture and
whether the proposed national identification
card might be the prophesied "mark
of the beast."

Such ways of thinking are not as
unusual as one might think. Tim LaHaye
and Jerry B. Jenkins have written more
than a dozen novels imagining the events described in the Book of Revelation,
books that have sold more than 60 million
copies. On Web sites and television programs,
such popular evangelists as Jerry
Falwell and Pat Robertson discuss apocalyptic
topics, including what form the
Antichrist might take and the place of the
United States or Israel in the Last Judgment. And the popular press regularly
churns out "documentaries" and other
programs like the recent NBC mini-series
"Revelations" that present biblical prophesy
as a sort of scriptural Da Vinci Code
indicating the world's future course and
conclusion.

Such interpretations completely misunderstand
both the meaning and the
value of apocalyptic literature. While apocalyptic texts do include end-of-theworld
scenarios, usually in wildly imaginative
terms, their primary concern is not to
predict the future, but to help people survive
in their own troubled times. The texts
are not historical reports, "visions radioed
in by means of a time machine," in the
words of the Jesuit theologian Karl
Rahner, but stories proclaiming
humanity's imminent deliverance.
Evil may rule the day in certain
respects, but we should live with
confidence nonetheless, because
God has already won the war. Both
in the midst of our own world's current
conflicts and anxieties and in
this Advent season, apocalyptic texts
have much to offer us. 

Apocalypse Then and Now

Although today the word "apocalypse"
is synonymous with
Armageddon, an end-times disaster
of human or divine making, that is
far from the biblical sense of the
term. Derived from the Greek word
apokalupsis, "apocalypse" literally
means "revelation," usually "revelation
about the future."
Apocalypticism, broadly defined, is
the belief that God has revealed the
imminent end of the ongoing struggle
between good and evil in history.
The Book of Revelation begins:
"The revelation of Jesus Christ,
which God gave him to show his servants
what must take place; he made it known by
sending his angel to his servant John" (Rev
1:1). The latter half of Daniel, too, is characterized
by visions and divine beings such
as the angel Gabriel, who comes "to give
you wisdom and understanding" (Dan
9:21).

In apocalyptic literature, as the biblical
scholar John J. Collins puts it, an other worldly reality is revealed to a human person
through a divine mediator, often an
angel. The term "otherworldly reality"
has two meanings here. First, it is the
heavenly realm in which the speaker finds
himself. The speaker of Revelation
describes himself in a bejeweled cosmic
throne room "with one seated on the
throne! And the one seated there looks
like jasper and cornelian, and around the
throne is a rainbow that looks like an
emerald. Around the throne are twentyfour
thrones, and seated on the thrones
are twenty-four elders, dressed in white
robes, with golden crowns on their heads.
Coming from the thrones are flashes of
lightning, and rumblings and peals of
thunder" (Rev 4:3-5).

Second, the reality revealed in apocalyptic
literature is what the seer is shown
once he gets into the heavenly realm,
namely our human destiny. Once in heaven,
the seer hears or witnesses a decision
made about earthly matters, generally a
decision in the people's favor. Couched in
highly symbolic language, the decision
often involves a cosmic battle between
God's emissaries and foes that the army of
God wins. In Rev 12:7: "And war broke
out in heaven; Michael and his angels
fought against the dragon. The dragon
and his angels fought back but they were
defeated." Likewise, in Daniel 2 the speaker
describes "a stone ... cut out, not by
human hands" that strikes down a statue
with feet of clay and "became a great
mountain and filled the whole earth."
Whether the vision is a prediction of a
future event or an explanation of a past
series of empires that have fallen (as here
in Daniel), the point remains always the
same - God is vindicated, God is triumphant
over all, God is Lord.

The imagery of violent conflict that
pervades apocalyptic literature is difficult
for us to appreciate today. Taken out of
context, the depictions reinforce a common
understanding of the God of the Old
Testament as a being of wrath and judgment.
But like much of the mythological
language of Revelation and Daniel, this
imagery emerges out of well-defined concepts
of the writers' own times. In the
ancient world, one's status as king was
obtained and confirmed through battle.
Thus many ancient Near Eastern stories
of creation involved a fight between two
different gods. The loser of that battle would often become the substance out of
which the earth and/or human beings were
created; the winner was, by virtue of his victory,
understood to be God the Most High.

In the ancient world, then, being God
meant, by definition, being a victorious
warrior. One of the bold innovations of
Genesis 1, in fact, is that Yahweh rules over
creation without ever engaging in battle.
The sea - a primeval image of chaos and
destruction - God gathers without difficulty;
sea monsters, which in other stories
might have been God's opponents, are in
Genesis 1 God's creation: "God created the
great sea monsters and every living creature
that moves" (Gen 1:21). The underlying
assertion is that unlike the gods of Babylon,
Persia or Assyria, the God of Israel is so
powerful that nothing can legitimately
challenge him. From the beginning everything
is under his control, subject to his
whim.

The writers of Revelation and other
apocalyptic texts return to battle imagery
because invading armies, idolatrous
Israelites or other hardships have challenged
this notion of God's kingship. The
Temple lies defiled (Daniel); Christians
suffer persecution from the Romans (Revelation); and the world no longer
seems under God's control. Drawing such
historical conflicts into the cosmic context
of a war between good and evil from which
God emerges victorious, these stories
acknowledge the evil and suffering in the
world but discount its ultimate significance.
God remains the king who will rescue his
people.

Though seemingly wild-eyed and fanciful,
apocalyptic texts have a concrete pastoral
purpose. They were written to
respond to that gap between what the faithful
believed - that God is a deliverer who
redeems the faithful and punishes the
wicked - and their ongoing experience of
persecution and suffering. Apocalyptic
writings intend to exhort and console.
Believers may feel powerless and beleaguered;
yet viewed from a cosmic perspective,
as Daniel Berrigan, S.J., writes regarding
Daniel, that sense of powerlessness is
transformed from a cause for grief into a
reason for hope:

Let the human know its limits,
God's word seems to say - and
thus its salvation. Neither we nor
any human striving, no matter how virtuous, nor any system nor political
amelioration - no human
effort can usher in the era of justice
and peace known as the realm of
God.... Neither believer nor unbeliever
is able to bring the end to
pass.

But the visions of apocalyptic literature
remind us, God can do this and has
done it. Ultimately we will witness a radically
different and better situation. As the
last chapter of Revelation begins: "I saw a
new heaven and a new earth; for the first
heaven and the first earth had passed away,
and the sea was no more" (Rev 21:1).
Apocalyptic Writings Today: So What?
The topic of apocalyptic lierature is particularly
relevant this December because of
both the Advent season and the state of our
world. Social questions, political divisions
and terrorism challenge our faith in God
and in one another. Yet if Revelation imagines
God's kingdom as sometime in the
future, the readings for Advent and
Christmas bring that expectation to the
present. "The days are coming," says God
to Jeremiah in the readings for the First Sunday of Advent, "when I will fulfill the
promises I made" (Jer 33:14). The Gospel
for the second Sunday suggests that day is
now; it presents John the Baptist as the fulfillment
of Isaiah's prophesy of the one who
will come to prepare the way. On the third
Sunday we hear that "The King of Israel,
the Lord, is in your midst" (Zeph 3:15), and
that we should "Rejoice!... The Lord is
near"(Phil 4:4). The final Sunday presents
signs and wonders - this liturgical year, the
baby leaping in Elizabeth's womb; in other
years, the angel's visitation to Mary or
Joseph's dream - each a further indication
that God's promise is being fulfilled right
now. At midnight Mass on Christmas Eve,
the moment arrives: "The people who
walked in darkness have seen a great light,"
writes Isaiah (Isa 9:1). In the Gospel the
angels announce to neighboring shepherds
that "this day in David's city a savior has
been born to you, the Messiah and Lord"
(Luke 2:11). Using apocalyptic themes and
imagery, then, the Scriptures of Advent,
Christmas and Epiphany point to the birth
of Jesus, as God acting conclusively in history
for our salvation.

They also teach us something about
what our salvation looks like. Living in a
culture saturated with "Left Behind"-style
interpretations of Revelation, one might
expect deliverance to involve wars, meteor
showers and lots of drama. Instead, in
Advent we hear that it begins with small,
largely unseen events - a strange, bug-eating
preacher wandering around in the
desert, a baby moving in its mother's womb
and the birth of a child in some sleepy corner
of the universe. Contrary to our expectations,
the world the day after Jesus' birth
looks pretty much the same as it looked the
day before. Yet we believe it has fundamentally
changed.

In the face of forces that trouble us
today, whether terrorism, natural disaster,
our own mortality or persistent divisions in
our church and government, we are tempted
to doubt or - another form of despair - 
to take matters into our own hands. If God
is not coming to save us from evil, we will
have to save ourselves.

Apocalyptic literature offers a needed
corrective. Reading Revelation or Daniel,
we discover that we are hardly the first to
struggle mightily against forces that we
cannot overcome on our own. These texts
also exhort us to remember what we
believe - that God acts in history, promises to redeem our suffering and is faithful - 
and to proceed accordingly. We do not see
our troubles coming to an end, but the
kingdom of God now grows in our midst.
Though its timeline remains unknown - 
we "know neither the day nor the hour"
(Matt 24:42) - the conclusion to be drawn
is clear: we are being delivered by God into glory, and we are called to proceed in hope
out of that faith.

</file>

<file= AmE06_D17>

There have been quite a number of famous atheists throughout history. These include such luminaries as English poets Percy Shelley and Lord Byron, French philosophers Voltaire and Jean-Paul Sartre, German philosophers Karl Marx and Friedrich Nietzsche, Austrian psychoanalyst Sigmund Freud, and American writers Mark Twain and Upton Sinclair. Many other people, in turn, have become atheists as a result of the influence of such luminaries.
Atheism received a significant shot in the arm during the eighteenth-century Enlightenment. During this time, a thoroughly secularized worldview emerged. Empirical knowledge (knowledge that comes only through the five senses), reason, and the scientific method became the modus operandi of the day. There developed a widespread faith in rationality. Many people came to believe in and trust only that which could be tested and studied. Eventually this methodology was applied to religious matters, including the foundational issue of God's existence. Many concluded that the evidence for a divine Creator-God was insufficient, and hence many became skeptics and atheists. Many of those who did not become skeptics or atheists became deists, embracing the idea that God initially created the world but has since been uninvolved in it.
Atheists have always constituted a relatively small percentage of the population. In 1994, 240 million people worldwide claimed to be atheists. That is roughly four percent of the world's population. A recent Gallup poll indicates that less than two-fifths of one percent of Americans (a little over 900,000) say, or were willing to admit to pollsters, that they are atheists. Agnosticism, the view that one cannot be certain about the existence of God, is more widespread, comprising about 16 percent of the world's population.
As of this writing (2005), a number of respected scholars have suggested that atheism is on the decline. In March 2005, noted theologian Wolfhart Pannenberg told United Press International that "atheism as a theoretical position is in decline worldwide." Oxford scholar Alister McGrath agrees, suggesting that atheism's "future seems increasingly to lie in the private beliefs of individuals rather than in the great public domain it once regarded as its habitat." The Reverend Paul M. Zulehner, dean of Vienna University's divinity school, commented that "true atheists in Europe have become an infinitesimally small group. There are not enough of them to be used for sociological research."
One reason suggested for the decline of atheism is that modern science seems to be pointing away from atheism. Professor Antony Flew is cited as one example of a long-time atheist who has now come to believe in the existence of a deistic God as a result of evidence from the intelligent design movement. 
For now, my goal is simply to define atheism and differentiate it from agnosticism and skepticism. I think you will come to agree with me that the three are quite closely related.
The word atheism comes from two Greek words: the prefix a, meaning "no" or "without", and theos, meaning "God" or "deity." An atheist is a person who does not believe in God or any deity.
I find it interesting that the term atheism was once used among the Greeks and Romans in regard to Christians, who denied the gods of pagan religions. It was only much later that the term atheist came to be used in regard to a denial of the personal Creator-God of the Christian Bible. As scholar Michael Martin said, "In Western society the term atheism has been used more narrowly to refer to the denial of theism, in particular Judeo-Christian theism, which asserts the existence of an all-powerful, all-knowing, all-good personal being."
There have been various kinds of atheists throughout history. Some have argued that the idea of God is mythological, and there is no need for such mythology in modern times. Others say there once was a God but He died. Others have argued that because of the finitude and limitations of "God-talk" (that is, language about God), we really cannot know anything about such a being. Still others come right out and dogmatically assert that there never has been and never will be a God.
Some atheists argue that atheism is the default position of all human beings. David Eller, in his book Natural Atheism, wrote:
I was born an atheist. All humans are born atheists. No baby born into the world arrives with specific religious beliefs or knowledge. Such beliefs and knowledge must be acquired, which means that they must first exist before and apart from the new life and that they must be presented to and impressed on the new suggestible mind - one that has no critical apparatus and no alternative views of its own. Human infants are like sponges, soaking up (not completely uncritically, but eagerly and effectively) whatever is there to be soaked up from their social environment. Small children in particular instinctively imitate the models that they observe in their childhood, but I was not compelled to attend or practice any particular religion, and as I grew I never saw any reason to "convert" to any particular religion. I have thus been an atheist all my life. I am a natural atheist.
Eller is aware that Christian critics challenge the idea that one can be a "natural atheist." He says his critics
claim that atheism requires an active rejection of religious belief, which cannot occur without prior exposure or even commitment to religion. So, a newborn is not yet an "atheist" but something other than atheist or theist, they maintain - a "pre-theist" maybe. Atheism must be a choice. I see this argument as spurious and actually negatively motivated. Theists do not want to admit that they were once atheists too and that they gave it up not by any choice they made but by the forces imposed on them by a religious world.
Reacting against the claim by Christian apologists that atheism itself constitutes a religious belief, prominent atheist George Smith argues that atheism, in its most basic form, is not a belief, but rather is the absence of a belief. Atheism is said to be "no more a religion than bald is a hair color or health is a disease." "The definition for atheism that we use, put simply, says that atheism is the lack of a god-belief, the absence of theism, to whatever degree and for whatever reason."
The problem with this line of argumentation, Christian theists point out, is that once you say "I lack a belief in God," you have in fact affirmed a religious "belief." Further, this line of argumentation fails to recognize that atheism is in fact a faith system. After all, atheists cannot offer definitive proofs that God does not exist, and hence there must be an element of faith in their viewpoint. The late science fiction writer Isaac Asimov, one of the more prominent signers of the Humanist Manifesto II, was being intellectually honest when he stated, "Emotionally I am an atheist. I don't have the evidence to prove that God doesn't exist, but I so strongly suspect he doesn't that I don't want to waste my time." The lack of evidence to prove God does not exist - to merely suspect that God does not exist - is a position that quite obviously involves a level of faith. Atheism is a faith system.ypical Atheistic Beliefs
Atheism Is Man-centered. The American Atheists creed affirms:
An atheist loves himself and his fellow man instead of a god. An atheist accepts that heaven is something for which we should work now - here on earth - for all men together to enjoy. An atheist accepts that he can get no help through prayer, but that he must find in himself the inner conviction and strength to meet life, to grapple with it, to subdue it and to enjoy it. An atheist accepts that only in a knowledge of himself and a knowledge of his fellow man can he find the understanding that will help to a life of fulfillment.
Atheism Denies the Existence of God. God did not create human beings. Rather, human beings created God. God is a myth. While Christians argue that all things need a cause, and therefore the universe must have been caused by God, one must logically proceed to ask, "Who caused God?" While Christians point to the entire Bible as a revelation from God, atheists claim that the God of Old Testament "revelation" is vicious and cruel, ordering His people to murder women and children. While Christians argue for the love of God, the Bible portrays God as sending people to hell to suffer for all eternity. In view of such problems, it is more reasonable, atheists say, to believe that God does not exist.
Atheism Affirms that the Universe Is Eternal. God is not the creator of the universe. The universe can be explained in terms of the philosophy of naturalism. The universe is eternal. The late famous scientist Carl Sagan, in his popular PBS television show Cosmos, said that "the cosmos is all that is or ever was or ever will be." More than one scholar has noted that Sagan's comment seems to be a purposeful substitution for the Gloria Patri: "Glory be to the Father and to the Son and to the Holy Ghost. As it was in the beginning, is now, and ever shall be, world without end." Of course, Sagan did not believe in the existence of God or a Creator. To him, the universe is infinitely old and self-existing. The universe alone gave birth to life on this planet. We are literally children of the cosmos.
Atheism Espouses Human Evolution. We need not appeal to the existence of a Creator-God to account for human beings. Rather, human beings evolved via Darwinistic natural selection. 'Once we accept the theory of evolution by natural selection, the traditional idea of God really does go out of the window." We are told that "living creatures on earth are a direct product of the earth. There is every reason to believe that living things owe their origin entirely to certain physical and chemical properties of the ancient earth." Indeed, "nothing supernatural appeared to be involved - only time and natural physical and chemical laws operating within the peculiarly suitable environment."
Atheism Affirms the Reality of Evil. Evil truly exists, and it constitutes a powerful argument against the existence of God. As scholar Alvin Plantinga stated, "Many believe that the existence of evil (or at least the amount and kinds of evil we actually find) makes belief in God unreasonable or rationally unacceptable." Theologians William Hamilton and Thomas Altizer have flat-out concluded that God is dead. British empiricist David Hume asked in regard to God, "Is he willing to prevent evil, but not able? Then he is impotent. Is he able, but not willing? Then he is malevolent. Is he both able and willing: whence then is evil?" If there is a God - and He is all-good and all-powerful - then, atheists argue, such atrocities as Hitler's murder of six million Jews should never have happened. It is more reasonable to conclude that such evil disproves the existence of God.
Atheism Denies an Afterlife. Human beings do not have an immortal soul. Most atheists consider humans to be strictly material beings. "There is no mind apart from brain. Nor is there a soul independent of body." Hence, when a human dies, that is the end of him.
Atheism Denies Moral Absolutes. Ethical guidelines emerge in human societies by trial and error - much in the way traffic laws emerged after the invention of the car. "Right" actions are those that bring the greatest good in the long run. Changing situations bring about the need for new or adjusted ethical guidelines.
Atheism Is Anti-religion. Atheists can be quite vitriolic in their condemnation of organized religion. Scholar Alister McGrath comments, "What propels people toward atheism is above all a sense of revulsion against the excesses and failures of organized religion. Atheism is ultimately a worldview of fear - a fear, often merited, of what might happen if religious maniacs were to take over the world."

</file>

